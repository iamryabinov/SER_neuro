{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.models_one_task import AlexNet, vgg\n",
    "from datasets.ramas import RamasDataset\n",
    "from constants import *\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import skorch\n",
    "from skorch.helper import predefined_split\n",
    "from skorch.dataset import Dataset\n",
    "from skorch.classifier import NeuralNetClassifier\n",
    "import skorch.callbacks as callbacks\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= INITIALIZING DATASET Ramas224Descrete_224_train ===============\n",
      "============================ SUCCESS! =========================\n",
      "============= INITIALIZING DATASET Ramas224Descrete_224_test ===============\n",
      "============================ SUCCESS! =========================\n"
     ]
    }
   ],
   "source": [
    "ramas_224_train = RamasDataset(RAMAS_PATH_TO_WAVS, 'Ramas224Descrete',\n",
    "                 spectrogram_shape=224,\n",
    "                 augmentation=True, padding='repeat', mode='train',  tasks='emotion', type='descrete')\n",
    "ramas_224_test = RamasDataset(RAMAS_PATH_TO_WAVS, 'Ramas224Descrete',\n",
    "                 spectrogram_shape=224,\n",
    "                 augmentation=False, padding='repeat', mode='test',  tasks='emotion', type='descrete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ramas_224_train\n",
    "test_dataset = ramas_224_test\n",
    "filename = 'AlexNet--{}_augmentation-{}.md'.format(train_dataset.name, str(train_dataset.augmentation).lower())\n",
    "best_model_file_path = os.path.join(RESULTS_FOLDER, filename)\n",
    "callback_train_acc = callbacks.EpochScoring(scoring=\"accuracy\", \n",
    "                                            lower_is_better=False, \n",
    "                                            on_train=True, \n",
    "                                            name='train_acc')\n",
    "callback_save_best = callbacks.Checkpoint(monitor='valid_loss_best', \n",
    "                                          f_params=None, \n",
    "                                          f_optimizer=None, \n",
    "                                          f_criterion=None, \n",
    "                                          f_history=None, \n",
    "                                          f_pickle=best_model_file_path,  \n",
    "                                          event_name='event_cp')\n",
    "callback_early_stop = callbacks.EarlyStopping(monitor='valid_loss', patience=30, \n",
    "                                              threshold_mode='rel', lower_is_better=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlexNet(num_classes=8)\n",
    "net = skorch.classifier.NeuralNetClassifier(\n",
    "    model, criterion=nn.CrossEntropyLoss, optimizer=torch.optim.Adam,\n",
    "    lr=3e-4, max_epochs=300, batch_size=32, train_split=predefined_split(test_dataset), \n",
    "    device=device, iterator_train__shuffle=True, \n",
    "    callbacks=[\n",
    "        callback_train_acc,\n",
    "        callback_save_best,\n",
    "        callback_early_stop\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_acc    train_loss    valid_acc    valid_loss    cp      dur\n",
      "-------  -----------  ------------  -----------  ------------  ----  -------\n",
      "      1       \u001b[36m0.2073\u001b[0m        \u001b[32m1.9601\u001b[0m       \u001b[35m0.2264\u001b[0m        \u001b[31m1.9107\u001b[0m     +  46.6378\n",
      "      2       \u001b[36m0.2225\u001b[0m        \u001b[32m1.8998\u001b[0m       \u001b[35m0.2869\u001b[0m        \u001b[31m1.7789\u001b[0m     +  46.1199\n",
      "      3       \u001b[36m0.2647\u001b[0m        \u001b[32m1.8206\u001b[0m       0.2623        1.8543        46.0725\n",
      "      4       \u001b[36m0.2673\u001b[0m        \u001b[32m1.7855\u001b[0m       \u001b[35m0.2992\u001b[0m        \u001b[31m1.7316\u001b[0m     +  46.0463\n",
      "      5       \u001b[36m0.2855\u001b[0m        \u001b[32m1.7609\u001b[0m       \u001b[35m0.3012\u001b[0m        \u001b[31m1.7265\u001b[0m     +  46.1071\n",
      "      6       \u001b[36m0.2924\u001b[0m        \u001b[32m1.7486\u001b[0m       \u001b[35m0.3053\u001b[0m        \u001b[31m1.7133\u001b[0m     +  46.1171\n",
      "      7       0.2916        \u001b[32m1.7321\u001b[0m       \u001b[35m0.3094\u001b[0m        \u001b[31m1.6991\u001b[0m     +  46.0627\n",
      "      8       \u001b[36m0.3006\u001b[0m        \u001b[32m1.7136\u001b[0m       0.3012        \u001b[31m1.6932\u001b[0m     +  46.0935\n",
      "      9       \u001b[36m0.3057\u001b[0m        1.7159       \u001b[35m0.3197\u001b[0m        \u001b[31m1.6835\u001b[0m     +  46.0566\n",
      "     10       \u001b[36m0.3096\u001b[0m        \u001b[32m1.6948\u001b[0m       \u001b[35m0.3238\u001b[0m        1.6952        46.0964\n",
      "     11       0.3055        \u001b[32m1.6936\u001b[0m       0.2869        1.7072        46.0293\n",
      "     12       \u001b[36m0.3127\u001b[0m        1.7069       0.3176        \u001b[31m1.6722\u001b[0m     +  46.1451\n",
      "     13       \u001b[36m0.3147\u001b[0m        \u001b[32m1.6844\u001b[0m       \u001b[35m0.3381\u001b[0m        \u001b[31m1.6562\u001b[0m     +  46.2036\n",
      "     14       \u001b[36m0.3265\u001b[0m        \u001b[32m1.6667\u001b[0m       0.3135        1.6616        46.3170\n",
      "     15       0.3201        1.6690       \u001b[35m0.3432\u001b[0m        \u001b[31m1.6428\u001b[0m     +  47.2820\n",
      "     16       \u001b[36m0.3365\u001b[0m        \u001b[32m1.6550\u001b[0m       0.3197        1.6555        46.2031\n",
      "     17       \u001b[36m0.3444\u001b[0m        \u001b[32m1.6428\u001b[0m       0.3371        1.6560        47.9756\n",
      "     18       \u001b[36m0.3508\u001b[0m        1.6481       \u001b[35m0.3535\u001b[0m        1.6496        46.5605\n",
      "     19       0.3506        \u001b[32m1.6375\u001b[0m       \u001b[35m0.3555\u001b[0m        1.6460        46.3218\n",
      "     20       \u001b[36m0.3578\u001b[0m        \u001b[32m1.6323\u001b[0m       0.3340        \u001b[31m1.6424\u001b[0m     +  46.3105\n",
      "     21       \u001b[36m0.3626\u001b[0m        \u001b[32m1.6113\u001b[0m       0.3484        \u001b[31m1.6420\u001b[0m     +  46.3165\n",
      "     22       \u001b[36m0.3662\u001b[0m        \u001b[32m1.6031\u001b[0m       0.3412        \u001b[31m1.6402\u001b[0m     +  47.1592\n",
      "     23       \u001b[36m0.3872\u001b[0m        \u001b[32m1.5771\u001b[0m       \u001b[35m0.3607\u001b[0m        \u001b[31m1.6379\u001b[0m     +  46.1561\n",
      "     24       0.3819        1.5782       0.3443        1.6496        46.0864\n",
      "     25       \u001b[36m0.3913\u001b[0m        \u001b[32m1.5728\u001b[0m       \u001b[35m0.3863\u001b[0m        \u001b[31m1.6109\u001b[0m     +  45.8253\n",
      "     26       \u001b[36m0.4006\u001b[0m        \u001b[32m1.5537\u001b[0m       0.3627        1.6229        46.1617\n",
      "     27       \u001b[36m0.4044\u001b[0m        \u001b[32m1.5328\u001b[0m       \u001b[35m0.3873\u001b[0m        1.6136        46.2380\n",
      "     28       \u001b[36m0.4083\u001b[0m        \u001b[32m1.5194\u001b[0m       \u001b[35m0.3883\u001b[0m        \u001b[31m1.6043\u001b[0m     +  46.2257\n",
      "     29       \u001b[36m0.4249\u001b[0m        \u001b[32m1.5190\u001b[0m       \u001b[35m0.3893\u001b[0m        1.6183        46.0951\n",
      "     30       \u001b[36m0.4264\u001b[0m        \u001b[32m1.4959\u001b[0m       \u001b[35m0.3945\u001b[0m        \u001b[31m1.6021\u001b[0m     +  46.3011\n",
      "     31       \u001b[36m0.4390\u001b[0m        \u001b[32m1.4865\u001b[0m       0.3842        \u001b[31m1.5899\u001b[0m     +  46.2426\n",
      "     32       \u001b[36m0.4416\u001b[0m        \u001b[32m1.4770\u001b[0m       0.3934        \u001b[31m1.5875\u001b[0m     +  46.0926\n",
      "     33       \u001b[36m0.4536\u001b[0m        \u001b[32m1.4493\u001b[0m       0.3914        \u001b[31m1.5644\u001b[0m     +  46.1605\n",
      "     34       \u001b[36m0.4667\u001b[0m        \u001b[32m1.4250\u001b[0m       \u001b[35m0.4109\u001b[0m        1.5877        46.1513\n",
      "     35       \u001b[36m0.4728\u001b[0m        \u001b[32m1.4193\u001b[0m       0.3965        1.6145        46.1586\n",
      "     36       \u001b[36m0.4736\u001b[0m        \u001b[32m1.3866\u001b[0m       \u001b[35m0.4180\u001b[0m        1.6249        46.1452\n",
      "     37       \u001b[36m0.4787\u001b[0m        \u001b[32m1.3729\u001b[0m       0.4160        \u001b[31m1.5618\u001b[0m     +  46.2289\n",
      "     38       \u001b[36m0.4969\u001b[0m        \u001b[32m1.3380\u001b[0m       \u001b[35m0.4334\u001b[0m        1.6023        46.1316\n",
      "     39       \u001b[36m0.5028\u001b[0m        \u001b[32m1.3252\u001b[0m       0.4262        1.5886        46.1875\n",
      "     40       \u001b[36m0.5179\u001b[0m        \u001b[32m1.2975\u001b[0m       0.4273        1.6207        46.0928\n",
      "     41       \u001b[36m0.5182\u001b[0m        \u001b[32m1.2787\u001b[0m       0.4170        1.6664        46.0612\n",
      "     42       \u001b[36m0.5415\u001b[0m        \u001b[32m1.2302\u001b[0m       0.4211        1.7445        46.1318\n",
      "     43       \u001b[36m0.5451\u001b[0m        \u001b[32m1.2284\u001b[0m       0.4160        1.6328        46.1792\n",
      "     44       \u001b[36m0.5730\u001b[0m        \u001b[32m1.1935\u001b[0m       0.4232        1.6450        45.9860\n",
      "     45       0.5605        1.1986       0.4170        1.6750        46.0181\n",
      "     46       \u001b[36m0.5828\u001b[0m        \u001b[32m1.1464\u001b[0m       0.4334        1.7433        46.0507\n",
      "     47       \u001b[36m0.6040\u001b[0m        \u001b[32m1.1090\u001b[0m       0.4232        1.8501        46.1645\n",
      "     48       \u001b[36m0.6087\u001b[0m        \u001b[32m1.0938\u001b[0m       \u001b[35m0.4365\u001b[0m        1.7342        46.0370\n",
      "     49       \u001b[36m0.6253\u001b[0m        \u001b[32m1.0381\u001b[0m       0.4232        1.7190        46.1535\n",
      "     50       \u001b[36m0.6461\u001b[0m        \u001b[32m1.0040\u001b[0m       0.4129        1.8501        46.1383\n",
      "     51       \u001b[36m0.6530\u001b[0m        \u001b[32m0.9821\u001b[0m       0.4252        1.8768        46.1492\n",
      "     52       \u001b[36m0.6602\u001b[0m        \u001b[32m0.9492\u001b[0m       0.4129        1.9188        46.0664\n",
      "     53       \u001b[36m0.6789\u001b[0m        \u001b[32m0.8907\u001b[0m       0.4180        1.8290        46.2647\n",
      "     54       \u001b[36m0.6930\u001b[0m        \u001b[32m0.8661\u001b[0m       0.4170        1.8850        46.1108\n",
      "     55       \u001b[36m0.7025\u001b[0m        \u001b[32m0.8512\u001b[0m       0.4293        1.8806        46.1219\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=AlexNet(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): ReLU(inplace=True)\n",
       "      (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): ReLU(inplace=True)\n",
       "      (10): Conv2d(256, 256, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(5, 5))\n",
       "    (classifier): Sequential(\n",
       "      (0): Dropout(p=0.5, inplace=False)\n",
       "      (1): Linear(in_features=6400, out_features=2048, bias=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (4): Dropout(p=0.75, inplace=False)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Linear(in_features=512, out_features=8, bias=True)\n",
       "    )\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(train_dataset, y=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ramas_224_train\n",
    "test_dataset = ramas_224_test\n",
    "filename = 'VGGNet--{}_augmentation-{}.md'.format(train_dataset.name, str(train_dataset.augmentation).lower())\n",
    "best_model_file_path = os.path.join(RESULTS_FOLDER, filename)\n",
    "callback_train_acc = callbacks.EpochScoring(scoring=\"accuracy\", \n",
    "                                            lower_is_better=False, \n",
    "                                            on_train=True, \n",
    "                                            name='train_acc')\n",
    "callback_save_best = callbacks.Checkpoint(monitor='valid_loss_best', \n",
    "                                          f_params=None, \n",
    "                                          f_optimizer=None, \n",
    "                                          f_criterion=None, \n",
    "                                          f_history=None, \n",
    "                                          f_pickle=best_model_file_path,  \n",
    "                                          event_name='event_cp')\n",
    "callback_early_stop = callbacks.EarlyStopping(monitor='valid_loss', patience=30, \n",
    "                                              threshold_mode='rel', lower_is_better=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgg(type=11, bn=True, num_classes=8)\n",
    "net = skorch.classifier.NeuralNetClassifier(\n",
    "    model, criterion=nn.CrossEntropyLoss, optimizer=torch.optim.Adam,\n",
    "    lr=3e-4, max_epochs=300, batch_size=32, train_split=predefined_split(test_dataset), \n",
    "    device=device, iterator_train__shuffle=True, \n",
    "    callbacks=[\n",
    "        callback_train_acc,\n",
    "        callback_save_best,\n",
    "        callback_early_stop\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_acc    train_loss    valid_acc    valid_loss    cp      dur\n",
      "-------  -----------  ------------  -----------  ------------  ----  -------\n",
      "      1       \u001b[36m0.2365\u001b[0m        \u001b[32m1.9116\u001b[0m       \u001b[35m0.1742\u001b[0m        \u001b[31m3.1456\u001b[0m     +  67.4569\n",
      "      2       \u001b[36m0.2704\u001b[0m        \u001b[32m1.7873\u001b[0m       \u001b[35m0.3166\u001b[0m        \u001b[31m1.7154\u001b[0m     +  67.3093\n",
      "      3       \u001b[36m0.2940\u001b[0m        \u001b[32m1.7394\u001b[0m       0.2859        \u001b[31m1.6974\u001b[0m     +  67.3468\n",
      "      4       \u001b[36m0.3091\u001b[0m        \u001b[32m1.7186\u001b[0m       0.2213        2.3580        67.5020\n",
      "      5       \u001b[36m0.3209\u001b[0m        \u001b[32m1.7093\u001b[0m       0.2982        \u001b[31m1.6944\u001b[0m     +  67.5118\n",
      "      6       \u001b[36m0.3278\u001b[0m        \u001b[32m1.6698\u001b[0m       0.2736        2.4418        67.2240\n",
      "      7       \u001b[36m0.3547\u001b[0m        \u001b[32m1.6653\u001b[0m       \u001b[35m0.3463\u001b[0m        \u001b[31m1.6413\u001b[0m     +  67.3927\n",
      "      8       \u001b[36m0.3598\u001b[0m        \u001b[32m1.6426\u001b[0m       0.3443        1.6632        67.3277\n",
      "      9       \u001b[36m0.3865\u001b[0m        \u001b[32m1.6186\u001b[0m       \u001b[35m0.3811\u001b[0m        \u001b[31m1.6149\u001b[0m     +  67.1798\n",
      "     10       \u001b[36m0.3975\u001b[0m        \u001b[32m1.5942\u001b[0m       0.3166        1.9065        67.3908\n",
      "     11       \u001b[36m0.4044\u001b[0m        \u001b[32m1.5757\u001b[0m       \u001b[35m0.3822\u001b[0m        1.7001        67.1412\n",
      "     12       \u001b[36m0.4177\u001b[0m        \u001b[32m1.5478\u001b[0m       0.3197        1.9166        67.6629\n",
      "     13       \u001b[36m0.4295\u001b[0m        \u001b[32m1.5132\u001b[0m       \u001b[35m0.4139\u001b[0m        \u001b[31m1.5567\u001b[0m     +  67.5292\n",
      "     14       \u001b[36m0.4400\u001b[0m        \u001b[32m1.4995\u001b[0m       0.3934        1.5592        67.4332\n",
      "     15       \u001b[36m0.4554\u001b[0m        \u001b[32m1.4857\u001b[0m       0.3801        1.7411        67.3082\n",
      "     16       \u001b[36m0.4680\u001b[0m        \u001b[32m1.4408\u001b[0m       \u001b[35m0.4436\u001b[0m        \u001b[31m1.5382\u001b[0m     +  67.4914\n",
      "     17       0.4659        1.4422       0.3883        1.6183        67.5221\n",
      "     18       \u001b[36m0.4974\u001b[0m        \u001b[32m1.3827\u001b[0m       \u001b[35m0.4590\u001b[0m        \u001b[31m1.4661\u001b[0m     +  67.4962\n",
      "     19       \u001b[36m0.5187\u001b[0m        \u001b[32m1.3317\u001b[0m       0.4395        1.4888        67.2203\n",
      "     20       \u001b[36m0.5246\u001b[0m        \u001b[32m1.3210\u001b[0m       0.4457        1.5114        67.7123\n",
      "     21       \u001b[36m0.5310\u001b[0m        \u001b[32m1.3075\u001b[0m       0.4477        1.4887        67.3147\n",
      "     22       \u001b[36m0.5407\u001b[0m        \u001b[32m1.2673\u001b[0m       \u001b[35m0.4621\u001b[0m        1.4882        67.3252\n",
      "     23       \u001b[36m0.5602\u001b[0m        \u001b[32m1.2107\u001b[0m       0.4436        1.5589        67.7016\n",
      "     24       \u001b[36m0.5815\u001b[0m        \u001b[32m1.1741\u001b[0m       \u001b[35m0.4959\u001b[0m        1.5036        67.5873\n",
      "     25       \u001b[36m0.5823\u001b[0m        \u001b[32m1.1504\u001b[0m       0.4775        \u001b[31m1.4514\u001b[0m     +  67.2344\n",
      "     26       \u001b[36m0.6081\u001b[0m        \u001b[32m1.0688\u001b[0m       0.4508        1.7072        67.3240\n",
      "     27       \u001b[36m0.6240\u001b[0m        \u001b[32m1.0345\u001b[0m       0.4775        1.6588        67.0308\n",
      "     28       \u001b[36m0.6381\u001b[0m        \u001b[32m1.0050\u001b[0m       0.4816        1.6103        67.3748\n",
      "     29       \u001b[36m0.6489\u001b[0m        \u001b[32m0.9771\u001b[0m       0.4918        1.5871        67.0829\n",
      "     30       \u001b[36m0.6758\u001b[0m        \u001b[32m0.9235\u001b[0m       \u001b[35m0.5164\u001b[0m        1.4758        67.2612\n",
      "     31       \u001b[36m0.6925\u001b[0m        \u001b[32m0.8507\u001b[0m       0.4641        1.6333        67.7178\n",
      "     32       \u001b[36m0.7117\u001b[0m        \u001b[32m0.7933\u001b[0m       0.5041        1.7357        67.4125\n",
      "     33       \u001b[36m0.7209\u001b[0m        \u001b[32m0.7817\u001b[0m       0.4816        1.8131        67.3969\n",
      "     34       \u001b[36m0.7232\u001b[0m        \u001b[32m0.7741\u001b[0m       0.4918        1.8536        67.2959\n",
      "     35       \u001b[36m0.7673\u001b[0m        \u001b[32m0.6706\u001b[0m       0.4662        1.7615        67.5811\n",
      "     36       \u001b[36m0.7824\u001b[0m        \u001b[32m0.6255\u001b[0m       0.4570        1.9135        67.2649\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=VGG(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ReLU(inplace=True)\n",
       "      (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (17): ReLU(inplace=True)\n",
       "      (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (24): ReLU(inplace=True)\n",
       "      (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (27): ReLU(inplace=True)\n",
       "      (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=25088, out_features=2048, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Linear(in_features=512, out_features=8, bias=True)\n",
       "    )\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(train_dataset, y=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= INITIALIZING DATASET Ramas224Binary_224_train ===============\n",
      "============================ SUCCESS! =========================\n",
      "============= INITIALIZING DATASET Ramas224Binary_224_test ===============\n",
      "============================ SUCCESS! =========================\n"
     ]
    }
   ],
   "source": [
    "ramas_224_train = RamasDataset(RAMAS_PATH_TO_WAVS_BINARY, 'Ramas224Binary',\n",
    "                 spectrogram_shape=224,\n",
    "                 augmentation=True, padding='repeat', mode='train',  tasks='emotion', type='binary')\n",
    "ramas_224_test = RamasDataset(RAMAS_PATH_TO_WAVS_BINARY, 'Ramas224Binary',\n",
    "                 spectrogram_shape=224,\n",
    "                 augmentation=False, padding='repeat', mode='test',  tasks='emotion', type='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ramas_224_train\n",
    "test_dataset = ramas_224_test\n",
    "filename = 'AlexNet--{}_augmentation-{}.md'.format(train_dataset.name, str(train_dataset.augmentation).lower())\n",
    "best_model_file_path = os.path.join(RESULTS_FOLDER, filename)\n",
    "callback_train_acc = callbacks.EpochScoring(scoring=\"accuracy\", \n",
    "                                            lower_is_better=False, \n",
    "                                            on_train=True, \n",
    "                                            name='train_acc')\n",
    "callback_save_best = callbacks.Checkpoint(monitor='valid_loss_best', \n",
    "                                          f_params=None, \n",
    "                                          f_optimizer=None, \n",
    "                                          f_criterion=None, \n",
    "                                          f_history=None, \n",
    "                                          f_pickle=best_model_file_path,  \n",
    "                                          event_name='event_cp')\n",
    "callback_early_stop = callbacks.EarlyStopping(monitor='valid_loss', patience=40, \n",
    "                                              threshold_mode='rel', lower_is_better=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlexNet(num_classes=2)\n",
    "net = skorch.classifier.NeuralNetClassifier(\n",
    "    model, criterion=nn.CrossEntropyLoss, optimizer=torch.optim.Adam,\n",
    "    lr=1e-5, max_epochs=300, batch_size=32, train_split=predefined_split(test_dataset), \n",
    "    device=device, iterator_train__shuffle=True, \n",
    "    callbacks=[\n",
    "        callback_train_acc,\n",
    "        callback_save_best,\n",
    "#          callback_early_stop\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_acc    train_loss    valid_acc    valid_loss    cp      dur\n",
      "-------  -----------  ------------  -----------  ------------  ----  -------\n",
      "      1       \u001b[36m0.5766\u001b[0m        \u001b[32m0.6871\u001b[0m       \u001b[35m0.6130\u001b[0m        \u001b[31m0.6682\u001b[0m     +  13.6861\n",
      "      2       0.5766        \u001b[32m0.6795\u001b[0m       0.6130        \u001b[31m0.6627\u001b[0m     +  13.4678\n",
      "      3       0.5766        \u001b[32m0.6771\u001b[0m       0.6130        0.6640        13.4681\n",
      "      4       0.5766        \u001b[32m0.6755\u001b[0m       0.6130        \u001b[31m0.6609\u001b[0m     +  13.3835\n",
      "      5       0.5766        \u001b[32m0.6737\u001b[0m       0.6130        \u001b[31m0.6596\u001b[0m     +  13.4660\n",
      "      6       0.5766        \u001b[32m0.6668\u001b[0m       0.6130        0.6597        13.3721\n",
      "      7       \u001b[36m0.5864\u001b[0m        \u001b[32m0.6618\u001b[0m       0.6027        \u001b[31m0.6525\u001b[0m     +  13.7686\n",
      "      8       \u001b[36m0.6014\u001b[0m        \u001b[32m0.6597\u001b[0m       0.6027        \u001b[31m0.6507\u001b[0m     +  13.3899\n",
      "      9       \u001b[36m0.6351\u001b[0m        \u001b[32m0.6428\u001b[0m       0.6130        \u001b[31m0.6426\u001b[0m     +  13.3924\n",
      "     10       \u001b[36m0.6377\u001b[0m        0.6487       0.5685        0.6723        13.4020\n",
      "     11       0.6218        0.6458       \u001b[35m0.6267\u001b[0m        \u001b[31m0.6401\u001b[0m     +  13.3596\n",
      "     12       0.6360        \u001b[32m0.6370\u001b[0m       0.6233        0.6403        13.3879\n",
      "     13       0.6200        0.6426       0.6130        0.6478        13.3576\n",
      "     14       \u001b[36m0.6608\u001b[0m        \u001b[32m0.6286\u001b[0m       \u001b[35m0.6336\u001b[0m        \u001b[31m0.6336\u001b[0m     +  13.3586\n",
      "     15       \u001b[36m0.6687\u001b[0m        \u001b[32m0.6183\u001b[0m       0.6301        0.6459        13.3917\n",
      "     16       0.6608        0.6284       0.6267        \u001b[31m0.6332\u001b[0m     +  13.3486\n",
      "     17       \u001b[36m0.6749\u001b[0m        \u001b[32m0.6132\u001b[0m       \u001b[35m0.6404\u001b[0m        \u001b[31m0.6314\u001b[0m     +  13.3957\n",
      "     18       0.6749        0.6161       \u001b[35m0.6473\u001b[0m        \u001b[31m0.6240\u001b[0m     +  13.3215\n",
      "     19       0.6740        0.6152       \u001b[35m0.6644\u001b[0m        \u001b[31m0.6221\u001b[0m     +  13.4017\n",
      "     20       0.6661        0.6178       0.6301        0.6323        13.3815\n",
      "     21       0.6687        0.6178       \u001b[35m0.6781\u001b[0m        0.6241        13.3888\n",
      "     22       0.6625        0.6175       0.6541        \u001b[31m0.6158\u001b[0m     +  13.3812\n",
      "     23       \u001b[36m0.6811\u001b[0m        \u001b[32m0.6032\u001b[0m       0.6541        \u001b[31m0.6127\u001b[0m     +  13.4216\n",
      "     24       \u001b[36m0.6864\u001b[0m        \u001b[32m0.5909\u001b[0m       0.6781        0.6222        13.4217\n",
      "     25       0.6856        0.5971       \u001b[35m0.6815\u001b[0m        \u001b[31m0.6080\u001b[0m     +  13.3638\n",
      "     26       \u001b[36m0.6900\u001b[0m        \u001b[32m0.5847\u001b[0m       0.6644        \u001b[31m0.6075\u001b[0m     +  13.4170\n",
      "     27       \u001b[36m0.6997\u001b[0m        0.5881       \u001b[35m0.6849\u001b[0m        \u001b[31m0.5975\u001b[0m     +  13.3497\n",
      "     28       \u001b[36m0.7033\u001b[0m        \u001b[32m0.5805\u001b[0m       0.6849        0.6046        13.3841\n",
      "     29       0.7024        \u001b[32m0.5729\u001b[0m       \u001b[35m0.6918\u001b[0m        \u001b[31m0.5951\u001b[0m     +  13.3686\n",
      "     30       \u001b[36m0.7139\u001b[0m        \u001b[32m0.5698\u001b[0m       0.6678        0.6107        13.3548\n",
      "     31       0.7130        \u001b[32m0.5690\u001b[0m       \u001b[35m0.7158\u001b[0m        \u001b[31m0.5895\u001b[0m     +  13.3729\n",
      "     32       \u001b[36m0.7254\u001b[0m        \u001b[32m0.5610\u001b[0m       0.6815        0.6041        13.3989\n",
      "     33       0.7192        \u001b[32m0.5516\u001b[0m       0.6918        0.5905        13.3361\n",
      "     34       \u001b[36m0.7343\u001b[0m        \u001b[32m0.5444\u001b[0m       0.6815        0.6148        13.3672\n",
      "     35       0.7210        0.5490       \u001b[35m0.7226\u001b[0m        0.6003        13.3688\n",
      "     36       \u001b[36m0.7369\u001b[0m        \u001b[32m0.5322\u001b[0m       0.6712        0.6376        13.3363\n",
      "     37       0.7201        0.5487       0.6678        0.6170        13.3955\n",
      "     38       \u001b[36m0.7405\u001b[0m        0.5364       0.6918        0.6029        13.3510\n",
      "     39       0.7228        0.5338       0.7089        0.5944        13.3866\n",
      "     40       0.7334        0.5459       0.7192        0.5977        13.3814\n",
      "     41       \u001b[36m0.7493\u001b[0m        \u001b[32m0.5214\u001b[0m       0.7021        0.5954        13.4270\n",
      "     42       0.7440        0.5325       0.7055        0.6048        13.4160\n",
      "     43       \u001b[36m0.7511\u001b[0m        \u001b[32m0.5213\u001b[0m       0.6849        0.5924        13.4530\n",
      "     44       0.7484        \u001b[32m0.5102\u001b[0m       0.7089        0.5935        13.4088\n",
      "     45       0.7493        0.5147       0.6918        0.6002        13.3255\n",
      "     46       0.7396        0.5252       0.7226        \u001b[31m0.5839\u001b[0m     +  13.3611\n",
      "     47       \u001b[36m0.7547\u001b[0m        \u001b[32m0.5076\u001b[0m       0.6986        0.5961        13.3419\n",
      "     48       0.7547        0.5107       0.6952        0.5904        13.4326\n",
      "     49       \u001b[36m0.7706\u001b[0m        \u001b[32m0.5007\u001b[0m       0.6781        0.6186        13.3536\n",
      "     50       0.7458        0.5144       0.7021        0.5896        13.3698\n",
      "     51       0.7511        0.5160       0.6918        \u001b[31m0.5807\u001b[0m     +  13.3511\n",
      "     52       0.7573        \u001b[32m0.4954\u001b[0m       0.7089        0.5848        13.3889\n",
      "     53       0.7591        0.5075       0.6986        \u001b[31m0.5779\u001b[0m     +  13.3669\n",
      "     54       0.7644        0.5024       0.6986        0.6025        13.3522\n",
      "     55       \u001b[36m0.7750\u001b[0m        0.4966       0.7158        \u001b[31m0.5707\u001b[0m     +  13.3223\n",
      "     56       0.7626        \u001b[32m0.4953\u001b[0m       0.7226        0.5747        13.4135\n",
      "     57       0.7635        0.5017       0.7089        \u001b[31m0.5622\u001b[0m     +  13.3363\n",
      "     58       0.7715        \u001b[32m0.4810\u001b[0m       0.7055        0.5774        13.4181\n",
      "     59       0.7724        0.4865       0.7192        \u001b[31m0.5601\u001b[0m     +  13.3541\n",
      "     60       0.7635        \u001b[32m0.4810\u001b[0m       0.7123        0.5792        13.4019\n",
      "     61       0.7706        \u001b[32m0.4757\u001b[0m       0.7123        0.5760        13.3414\n",
      "     62       \u001b[36m0.7839\u001b[0m        \u001b[32m0.4651\u001b[0m       0.6781        0.6315        13.3780\n",
      "     63       0.7653        0.4793       0.7055        0.5919        13.3515\n",
      "     64       0.7795        0.4683       0.7089        0.6086        13.3657\n",
      "     65       \u001b[36m0.7901\u001b[0m        \u001b[32m0.4590\u001b[0m       0.6918        0.6210        13.3484\n",
      "     66       0.7812        0.4671       0.7123        \u001b[31m0.5532\u001b[0m     +  13.3554\n",
      "     67       \u001b[36m0.7910\u001b[0m        0.4638       0.7123        0.5591        13.3565\n",
      "     68       \u001b[36m0.7989\u001b[0m        \u001b[32m0.4426\u001b[0m       0.7192        0.5658        13.3902\n",
      "     69       \u001b[36m0.8051\u001b[0m        \u001b[32m0.4404\u001b[0m       \u001b[35m0.7329\u001b[0m        0.5701        13.4361\n",
      "     70       \u001b[36m0.8087\u001b[0m        \u001b[32m0.4233\u001b[0m       0.7123        0.5891        13.4295\n",
      "     71       0.8007        0.4418       0.7260        0.5748        13.4350\n",
      "     72       \u001b[36m0.8149\u001b[0m        \u001b[32m0.4221\u001b[0m       0.7021        0.5890        13.4059\n",
      "     73       0.8113        \u001b[32m0.4203\u001b[0m       0.7055        0.5728        13.3918\n",
      "     74       0.8043        0.4388       \u001b[35m0.7363\u001b[0m        0.5723        13.4112\n",
      "     75       0.8105        \u001b[32m0.4175\u001b[0m       0.7226        0.5592        13.4103\n",
      "     76       \u001b[36m0.8282\u001b[0m        \u001b[32m0.4031\u001b[0m       \u001b[35m0.7397\u001b[0m        0.5577        13.4355\n",
      "     77       0.8175        \u001b[32m0.3984\u001b[0m       0.7089        0.5846        13.4321\n",
      "     78       0.8229        0.4109       0.7260        0.5603        13.3751\n",
      "     79       \u001b[36m0.8291\u001b[0m        \u001b[32m0.3836\u001b[0m       \u001b[35m0.7603\u001b[0m        0.5802        13.4028\n",
      "     80       0.8202        0.4026       0.7021        0.5861        13.3986\n",
      "     81       \u001b[36m0.8459\u001b[0m        \u001b[32m0.3655\u001b[0m       0.7363        0.5774        13.3929\n",
      "     82       0.8326        0.4055       0.7158        0.5886        13.4166\n",
      "     83       0.8299        0.3983       0.7192        0.5555        13.4500\n",
      "     84       0.8388        0.3802       0.7192        0.5839        13.4137\n",
      "     85       0.8299        0.3734       0.7192        0.5859        13.4302\n",
      "     86       \u001b[36m0.8574\u001b[0m        \u001b[32m0.3560\u001b[0m       0.7295        0.6115        13.4430\n",
      "     87       0.8521        \u001b[32m0.3373\u001b[0m       0.7329        0.6293        13.3863\n",
      "     88       0.8477        0.3477       0.7329        0.5952        13.3596\n",
      "     89       0.8521        \u001b[32m0.3322\u001b[0m       0.7432        0.5809        13.4007\n",
      "     90       0.8565        \u001b[32m0.3284\u001b[0m       0.7295        0.5999        13.3796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     91       \u001b[36m0.8636\u001b[0m        0.3298       0.7260        0.6000        13.3703\n",
      "     92       \u001b[36m0.8663\u001b[0m        0.3303       0.6644        0.7520        13.3655\n",
      "     93       \u001b[36m0.8707\u001b[0m        \u001b[32m0.3267\u001b[0m       0.7500        0.5865        13.5485\n",
      "     94       \u001b[36m0.8751\u001b[0m        \u001b[32m0.3125\u001b[0m       0.7089        0.6190        13.4140\n",
      "     95       \u001b[36m0.8778\u001b[0m        \u001b[32m0.2919\u001b[0m       0.6747        0.7017        13.4709\n",
      "     96       \u001b[36m0.8822\u001b[0m        \u001b[32m0.2844\u001b[0m       0.7432        0.6369        13.4202\n",
      "     97       0.8787        0.3006       0.7260        0.6394        13.3891\n",
      "     98       \u001b[36m0.8902\u001b[0m        \u001b[32m0.2834\u001b[0m       0.7192        0.6470        13.3785\n",
      "     99       \u001b[36m0.8937\u001b[0m        \u001b[32m0.2764\u001b[0m       0.7158        0.6588        13.4687\n",
      "    100       \u001b[36m0.9070\u001b[0m        \u001b[32m0.2546\u001b[0m       0.7192        0.7333        13.4412\n",
      "    101       0.8795        0.2832       0.6712        0.7498        13.4216\n",
      "    102       0.9026        \u001b[32m0.2501\u001b[0m       0.6986        0.6988        13.4113\n",
      "    103       0.9035        \u001b[32m0.2453\u001b[0m       0.7021        0.7202        13.3974\n",
      "    104       0.8813        0.2774       0.7260        0.6953        13.4161\n",
      "    105       \u001b[36m0.9097\u001b[0m        \u001b[32m0.2308\u001b[0m       0.7123        0.7219        13.3624\n",
      "    106       0.8866        0.2926       0.7192        0.6832        13.3775\n",
      "    107       0.9017        0.2491       0.7397        0.7237        13.3539\n",
      "    108       \u001b[36m0.9114\u001b[0m        \u001b[32m0.2229\u001b[0m       0.6986        0.7696        13.3140\n",
      "    109       0.9088        \u001b[32m0.2160\u001b[0m       0.7432        0.7334        14.2632\n",
      "    110       \u001b[36m0.9150\u001b[0m        \u001b[32m0.2157\u001b[0m       0.7021        0.7783        13.4099\n",
      "    111       \u001b[36m0.9291\u001b[0m        \u001b[32m0.2044\u001b[0m       0.7021        0.7647        13.3787\n",
      "    112       0.9229        0.2059       0.6678        0.8010        13.3986\n",
      "    113       0.9291        \u001b[32m0.1821\u001b[0m       0.7260        0.8509        13.3330\n",
      "    114       0.9291        \u001b[32m0.1760\u001b[0m       0.7192        0.8215        13.3878\n",
      "    115       0.9167        0.2328       0.7158        0.7797        13.3324\n",
      "    116       \u001b[36m0.9442\u001b[0m        \u001b[32m0.1589\u001b[0m       0.7226        0.8699        13.3352\n",
      "    117       0.9114        0.2319       0.7329        0.8009        13.3415\n",
      "    118       0.9256        0.1824       0.6781        0.8808        13.3653\n",
      "    119       0.9362        0.1678       0.7055        0.8992        13.3161\n",
      "    120       0.9256        0.1866       0.6815        0.9149        13.3294\n",
      "    121       0.9176        0.2010       0.7123        0.8412        13.3379\n",
      "    122       0.9380        0.1652       0.7123        0.9002        13.3216\n",
      "    123       0.9300        0.1879       0.6678        0.9945        13.4194\n",
      "    124       0.9247        0.1872       0.6952        0.8371        13.4101\n",
      "    125       \u001b[36m0.9460\u001b[0m        \u001b[32m0.1489\u001b[0m       0.6986        0.9287        13.4168\n",
      "    126       0.9407        0.1555       0.6986        0.9511        13.3732\n",
      "    127       \u001b[36m0.9495\u001b[0m        0.1505       0.7123        0.9115        13.4197\n",
      "    128       \u001b[36m0.9575\u001b[0m        \u001b[32m0.1440\u001b[0m       0.7089        0.9174        13.3899\n",
      "    129       0.9451        0.1572       0.6986        0.9381        13.3514\n",
      "    130       0.9353        0.1721       0.6815        0.9936        13.4028\n",
      "    131       0.9442        0.1509       0.6884        0.9384        13.4177\n",
      "    132       0.9424        0.1610       0.7021        1.0059        13.3904\n",
      "    133       0.9495        \u001b[32m0.1395\u001b[0m       0.6952        0.9763        13.3895\n",
      "    134       0.9451        \u001b[32m0.1347\u001b[0m       0.7055        0.9976        13.4386\n",
      "    135       0.9433        \u001b[32m0.1339\u001b[0m       0.7055        1.0199        13.3654\n",
      "    136       0.9548        \u001b[32m0.1268\u001b[0m       0.7089        1.0075        13.3504\n",
      "    137       0.9477        0.1326       0.6815        1.0511        13.4738\n",
      "    138       \u001b[36m0.9619\u001b[0m        \u001b[32m0.1162\u001b[0m       0.6918        1.0406        13.4146\n",
      "    139       0.9504        0.1272       0.6952        1.0298        13.4332\n",
      "    140       0.9557        0.1172       0.7055        1.0693        13.4114\n",
      "    141       \u001b[36m0.9646\u001b[0m        \u001b[32m0.1106\u001b[0m       0.7055        1.0279        13.4068\n",
      "    142       0.9486        0.1310       0.7055        1.0742        13.3573\n",
      "    143       0.9539        0.1273       0.6815        1.0865        13.3312\n",
      "    144       0.9539        0.1290       0.7192        1.0848        13.3531\n",
      "    145       0.9495        0.1229       0.6952        1.0498        13.3260\n",
      "    146       0.9274        0.1785       0.6849        1.0065        13.3616\n",
      "    147       0.9584        \u001b[32m0.1090\u001b[0m       0.6952        0.9988        13.3673\n",
      "    148       \u001b[36m0.9699\u001b[0m        \u001b[32m0.0897\u001b[0m       0.7055        1.0680        13.3981\n",
      "    149       0.9672        0.0954       0.7158        1.1097        13.4033\n",
      "    150       0.9486        0.1246       0.7158        1.0667        13.3461\n",
      "    151       0.9690        \u001b[32m0.0856\u001b[0m       0.6849        1.1279        13.3572\n",
      "    152       0.9619        0.0896       0.7226        1.1327        13.3833\n",
      "    153       \u001b[36m0.9717\u001b[0m        \u001b[32m0.0817\u001b[0m       0.6678        1.2372        13.3587\n",
      "    154       0.9699        \u001b[32m0.0802\u001b[0m       0.7123        1.0763        13.3651\n",
      "    155       0.9672        0.0888       0.6918        1.1602        13.3759\n",
      "    156       0.9672        0.0898       0.7123        1.1940        13.3913\n",
      "    157       0.9655        0.0885       0.6952        1.2088        13.3789\n",
      "    158       0.9663        0.0887       0.6644        1.3355        13.4116\n",
      "    159       0.9717        0.0891       0.7089        1.1496        13.3438\n",
      "    160       0.9628        0.0951       0.6952        1.1455        13.3403\n",
      "    161       0.9690        0.0950       0.6781        1.2806        13.4456\n",
      "    162       0.9531        0.1179       0.7226        1.1138        13.4055\n",
      "    163       0.9584        0.1007       0.7158        1.1912        13.4208\n",
      "    164       0.9699        0.0818       0.6815        1.1951        13.4459\n",
      "    165       \u001b[36m0.9725\u001b[0m        \u001b[32m0.0738\u001b[0m       0.7055        1.1801        13.4614\n",
      "    166       0.9681        0.0910       0.7089        1.2758        13.4384\n",
      "    167       0.9725        0.0755       0.7158        1.1178        13.4507\n",
      "    168       0.9699        0.0877       0.7021        1.1532        13.4264\n",
      "    169       \u001b[36m0.9796\u001b[0m        \u001b[32m0.0585\u001b[0m       0.6884        1.1863        13.4558\n",
      "    170       0.9787        0.0761       0.6952        1.1932        13.4387\n",
      "    171       0.9699        0.0902       0.6884        1.1985        13.4706\n",
      "    172       0.9761        0.0681       0.6815        1.2380        13.4272\n",
      "    173       0.9725        0.0704       0.7158        1.1862        13.4114\n",
      "    174       \u001b[36m0.9849\u001b[0m        \u001b[32m0.0472\u001b[0m       0.7158        1.2764        13.4125\n",
      "    175       0.9734        0.0780       0.7260        1.3354        13.4377\n",
      "    176       0.9699        0.0663       0.6815        1.2972        13.3968\n",
      "    177       0.9690        0.0829       0.7158        1.2136        13.3944\n",
      "    178       0.9787        0.0594       0.7192        1.2343        13.4262\n",
      "    179       0.9681        0.0922       0.6918        1.1884        13.3989\n",
      "    180       0.9743        0.0723       0.7123        1.2027        13.3929\n",
      "    181       0.9805        0.0618       0.7158        1.2916        13.3762\n",
      "    182       0.9655        0.0824       0.7158        1.1927        13.4804\n",
      "    183       0.9814        0.0510       0.6918        1.3405        13.3666\n",
      "    184       0.9699        0.0785       0.6781        1.3276        13.3624\n",
      "    185       0.9752        0.0708       0.7021        1.3415        13.3473\n",
      "    186       0.9796        0.0586       0.6952        1.3859        13.3567\n",
      "    187       0.9841        \u001b[32m0.0450\u001b[0m       0.6918        1.3994        13.4308\n",
      "    188       0.9779        0.0688       0.7158        1.3216        13.3745\n",
      "    189       0.9725        0.0720       0.6712        1.2997        13.3910\n",
      "    190       0.9752        0.0678       0.6815        1.3390        13.3833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    191       0.9832        0.0614       0.6986        1.3055        13.4106\n",
      "    192       0.9832        0.0536       0.6986        1.5063        13.3790\n",
      "    193       0.9805        0.0659       0.6986        1.3428        13.3629\n",
      "    194       0.9779        0.0673       0.6918        1.4073        13.3983\n",
      "    195       \u001b[36m0.9894\u001b[0m        \u001b[32m0.0393\u001b[0m       0.6747        1.4952        13.4044\n",
      "    196       0.9796        0.0588       0.7055        1.4508        13.4033\n",
      "    197       0.9796        0.0569       0.7089        1.3465        13.4064\n",
      "    198       0.9646        0.0902       0.7089        1.2962        13.3859\n",
      "    199       0.9805        0.0635       0.7089        1.3729        13.4198\n",
      "    200       0.9734        0.0595       0.7055        1.3521        13.4626\n",
      "    201       0.9858        \u001b[32m0.0388\u001b[0m       0.7089        1.3633        13.4098\n",
      "    202       0.9814        0.0500       0.7123        1.4189        13.3851\n",
      "    203       0.9894        \u001b[32m0.0351\u001b[0m       0.7021        1.4477        13.4148\n",
      "    204       0.9841        0.0473       0.7055        1.4739        13.4036\n",
      "    205       0.9858        0.0403       0.7021        1.4847        13.3876\n",
      "    206       0.9805        0.0538       0.6986        1.4961        13.3720\n",
      "    207       0.9841        0.0451       0.7089        1.4510        13.3997\n",
      "    208       0.9734        0.0575       0.6747        1.5035        13.4128\n",
      "    209       0.9415        0.1669       0.7192        1.2403        13.4221\n",
      "    210       0.9805        0.0578       0.6952        1.3272        13.3958\n",
      "    211       0.9841        0.0547       0.6849        1.3864        13.4143\n",
      "    212       0.9876        0.0441       0.7055        1.4161        13.4246\n",
      "    213       0.9885        0.0373       0.6884        1.4778        13.4440\n",
      "    214       0.9867        0.0417       0.6438        1.6458        13.4056\n",
      "    215       0.9858        0.0443       0.6781        1.5656        13.4126\n",
      "    216       0.9867        0.0431       0.7021        1.6702        13.4602\n",
      "    217       0.9849        0.0438       0.7089        1.5048        13.4382\n",
      "    218       0.9805        0.0508       0.6678        1.6086        13.4296\n",
      "    219       0.9832        0.0467       0.7123        1.4655        13.4404\n",
      "    220       \u001b[36m0.9903\u001b[0m        \u001b[32m0.0328\u001b[0m       0.6986        1.5285        13.4164\n",
      "    221       0.9894        0.0348       0.7055        1.5431        13.3233\n",
      "    222       0.9876        0.0349       0.7226        1.5193        13.4194\n",
      "    223       0.9849        0.0371       0.6986        1.5397        13.4236\n",
      "    224       0.9885        0.0346       0.7089        1.5772        13.3921\n",
      "    225       0.9849        0.0429       0.6712        1.5952        13.4576\n",
      "    226       0.9814        0.0518       0.6849        1.5617        13.4488\n",
      "    227       0.9876        0.0436       0.6986        1.5218        13.5357\n",
      "    228       \u001b[36m0.9920\u001b[0m        \u001b[32m0.0273\u001b[0m       0.7021        1.5820        13.4399\n",
      "    229       0.9867        0.0472       0.7021        1.5030        13.4212\n",
      "    230       \u001b[36m0.9947\u001b[0m        \u001b[32m0.0268\u001b[0m       0.6884        1.5540        13.4269\n",
      "    231       0.9885        0.0336       0.7123        1.5604        13.4041\n",
      "    232       0.9876        0.0394       0.7295        1.5708        13.4064\n",
      "    233       0.9867        0.0301       0.6678        1.8601        13.3992\n",
      "    234       0.9805        0.0460       0.7123        1.6297        13.4194\n",
      "    235       0.9849        0.0462       0.7021        1.5727        13.4398\n",
      "    236       0.9885        0.0340       0.7055        1.5333        13.3913\n",
      "    237       0.9911        0.0281       0.6781        1.7971        13.4270\n",
      "    238       0.9823        0.0440       0.6918        1.6548        13.4182\n",
      "    239       0.9903        0.0322       0.6986        1.6385        13.4234\n",
      "    240       0.9867        0.0389       0.7055        1.6213        13.4510\n",
      "    241       0.9770        0.0593       0.7089        1.5549        13.4120\n",
      "    242       0.9876        0.0359       0.7055        1.5928        13.4065\n",
      "    243       0.9885        0.0377       0.6952        1.6392        13.4170\n",
      "    244       0.9787        0.0560       0.7089        1.5506        13.4158\n",
      "    245       0.9858        0.0448       0.7123        1.5909        13.4393\n",
      "    246       0.9920        0.0312       0.7123        1.5486        13.4194\n",
      "    247       0.9894        0.0316       0.7226        1.6839        13.4208\n",
      "    248       0.9832        0.0370       0.7158        1.5913        13.4201\n",
      "    249       0.9929        0.0273       0.7021        1.7237        13.4364\n",
      "    250       0.9876        0.0292       0.7123        1.6543        13.4046\n",
      "    251       0.9876        0.0329       0.6781        1.8131        13.4040\n",
      "    252       0.9770        0.0549       0.7329        1.5387        13.3968\n",
      "    253       0.9885        0.0393       0.7192        1.5338        13.4335\n",
      "    254       0.9743        0.0794       0.6884        1.5545        13.3995\n",
      "    255       0.9849        0.0397       0.6952        1.4902        13.4404\n",
      "    256       0.9920        0.0289       0.7260        1.5249        13.4167\n",
      "    257       0.9938        \u001b[32m0.0179\u001b[0m       0.7089        1.5938        13.4133\n",
      "    258       0.9823        0.0446       0.7226        1.6846        13.4161\n",
      "    259       0.9849        0.0431       0.7226        1.5910        13.3920\n",
      "    260       0.9903        0.0297       0.7158        1.5492        13.3647\n",
      "    261       0.9885        0.0325       0.6986        1.6325        13.4162\n",
      "    262       0.9938        0.0187       0.7123        1.6349        13.4283\n",
      "    263       0.9920        0.0224       0.7192        1.8212        13.3782\n",
      "    264       0.9911        0.0233       0.7089        1.7319        13.4135\n",
      "    265       0.9938        0.0218       0.7158        1.7144        13.3921\n",
      "    266       0.9876        0.0399       0.7397        1.6569        13.3546\n",
      "    267       0.9920        0.0191       0.7089        1.6886        13.4198\n",
      "    268       0.9947        0.0185       0.7021        1.7155        13.4429\n",
      "    269       0.9885        0.0386       0.7123        1.6899        13.4851\n",
      "    270       0.9858        0.0425       0.6884        1.6605        13.4133\n",
      "    271       \u001b[36m0.9956\u001b[0m        0.0227       0.6747        1.9621        13.4895\n",
      "    272       0.9956        \u001b[32m0.0167\u001b[0m       0.6918        1.9920        13.3924\n",
      "    273       0.9903        0.0321       0.6884        1.7592        13.4011\n",
      "    274       0.9920        0.0224       0.6884        1.9220        13.3842\n",
      "    275       0.9761        0.0530       0.7192        1.7013        13.3675\n",
      "    276       0.9849        0.0361       0.7055        1.8577        13.4639\n",
      "    277       0.9929        0.0273       0.7123        1.7676        13.4678\n",
      "    278       0.9770        0.0552       0.7055        1.6730        13.4551\n",
      "    279       0.9894        0.0288       0.7158        1.7565        13.4300\n",
      "    280       0.9903        0.0260       0.7123        1.7559        13.4510\n",
      "    281       0.9885        0.0379       0.7226        1.6694        13.4284\n",
      "    282       \u001b[36m0.9973\u001b[0m        \u001b[32m0.0153\u001b[0m       0.6986        1.7727        13.4603\n",
      "    283       0.9894        0.0308       0.7329        1.7051        13.3877\n",
      "    284       0.9920        0.0332       0.6473        1.9649        13.4125\n",
      "    285       0.9903        0.0290       0.7329        1.6324        13.3982\n",
      "    286       0.9885        0.0318       0.7192        1.8483        13.4042\n",
      "    287       0.9947        \u001b[32m0.0150\u001b[0m       0.7260        1.7203        13.4134\n",
      "    288       0.9885        0.0393       0.7123        1.7472        13.3882\n",
      "    289       0.9867        0.0309       0.7158        1.6033        13.3952\n",
      "    290       0.9956        0.0167       0.7295        1.7465        13.4360\n",
      "    291       0.9956        \u001b[32m0.0135\u001b[0m       0.7055        1.8285        13.4056\n",
      "    292       0.9973        \u001b[32m0.0102\u001b[0m       0.6952        1.8415        13.4368\n",
      "    293       0.9929        0.0235       0.7158        1.6696        13.3978\n",
      "    294       0.9911        0.0204       0.7192        1.7828        13.4110\n",
      "    295       0.9929        0.0254       0.7021        1.8295        13.3898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    296       0.9965        0.0144       0.7089        1.9500        13.4127\n",
      "    297       0.9876        0.0314       0.6849        1.7543        13.4423\n",
      "    298       0.9911        0.0208       0.7192        1.8661        13.4264\n",
      "    299       0.9911        0.0166       0.7055        1.8925        13.4844\n",
      "    300       0.9876        0.0251       0.7363        1.6800        13.4490\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=AlexNet(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): ReLU(inplace=True)\n",
       "      (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): ReLU(inplace=True)\n",
       "      (10): Conv2d(256, 256, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(5, 5))\n",
       "    (classifier): Sequential(\n",
       "      (0): Dropout(p=0.5, inplace=False)\n",
       "      (1): Linear(in_features=6400, out_features=2048, bias=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(train_dataset, y=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ramas_224_train\n",
    "test_dataset = ramas_224_test\n",
    "filename = 'VGGNet--{}_augmentation-{}.md'.format(train_dataset.name, str(train_dataset.augmentation).lower())\n",
    "best_model_file_path = os.path.join(RESULTS_FOLDER, filename)\n",
    "callback_train_acc = callbacks.EpochScoring(scoring=\"accuracy\", \n",
    "                                            lower_is_better=False, \n",
    "                                            on_train=True, \n",
    "                                            name='train_acc')\n",
    "callback_save_best = callbacks.Checkpoint(monitor='valid_loss_best', \n",
    "                                          f_params=None, \n",
    "                                          f_optimizer=None, \n",
    "                                          f_criterion=None, \n",
    "                                          f_history=None, \n",
    "                                          f_pickle=best_model_file_path,  \n",
    "                                          event_name='event_cp')\n",
    "callback_early_stop = callbacks.EarlyStopping(monitor='valid_loss', patience=40, \n",
    "                                              threshold_mode='rel', lower_is_better=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgg(type=11, bn=True, num_classes=2)\n",
    "net = skorch.classifier.NeuralNetClassifier(\n",
    "    model, criterion=nn.CrossEntropyLoss, optimizer=torch.optim.Adam,\n",
    "    lr=1e-5, max_epochs=300, batch_size=32, train_split=predefined_split(test_dataset), \n",
    "    device=device, iterator_train__shuffle=True, \n",
    "    callbacks=[\n",
    "        callback_train_acc,\n",
    "        callback_save_best,\n",
    "#         callback_early_stop\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_acc    train_loss    valid_acc    valid_loss    cp      dur\n",
      "-------  -----------  ------------  -----------  ------------  ----  -------\n",
      "      1       \u001b[36m0.5934\u001b[0m        \u001b[32m0.6657\u001b[0m       \u001b[35m0.3870\u001b[0m        \u001b[31m0.7708\u001b[0m     +  19.7789\n",
      "      2       \u001b[36m0.6289\u001b[0m        \u001b[32m0.6371\u001b[0m       \u001b[35m0.4932\u001b[0m        \u001b[31m0.7340\u001b[0m     +  19.7696\n",
      "      3       \u001b[36m0.6351\u001b[0m        0.6420       \u001b[35m0.6918\u001b[0m        \u001b[31m0.6220\u001b[0m     +  19.8091\n",
      "      4       \u001b[36m0.6643\u001b[0m        \u001b[32m0.6235\u001b[0m       0.6267        0.6467        19.7680\n",
      "      5       \u001b[36m0.6838\u001b[0m        \u001b[32m0.6094\u001b[0m       0.6370        0.6381        19.7855\n",
      "      6       0.6723        \u001b[32m0.6084\u001b[0m       0.6849        \u001b[31m0.6212\u001b[0m     +  19.7667\n",
      "      7       0.6838        \u001b[32m0.5877\u001b[0m       0.6541        0.6371        19.7666\n",
      "      8       \u001b[36m0.6944\u001b[0m        \u001b[32m0.5855\u001b[0m       0.6747        0.6322        19.7452\n",
      "      9       0.6856        0.6001       0.5959        0.6920        19.7843\n",
      "     10       \u001b[36m0.6953\u001b[0m        \u001b[32m0.5684\u001b[0m       0.6849        0.6261        19.8093\n",
      "     11       \u001b[36m0.7068\u001b[0m        \u001b[32m0.5656\u001b[0m       \u001b[35m0.7192\u001b[0m        \u001b[31m0.6133\u001b[0m     +  19.7572\n",
      "     12       \u001b[36m0.7245\u001b[0m        \u001b[32m0.5492\u001b[0m       0.6644        0.6621        19.8117\n",
      "     13       0.7192        \u001b[32m0.5400\u001b[0m       0.7192        \u001b[31m0.6081\u001b[0m     +  19.7900\n",
      "     14       \u001b[36m0.7307\u001b[0m        \u001b[32m0.5196\u001b[0m       0.6747        0.6458        19.7283\n",
      "     15       \u001b[36m0.7378\u001b[0m        0.5196       0.5582        0.8100        19.7881\n",
      "     16       0.7334        0.5432       0.6473        0.6760        19.7834\n",
      "     17       \u001b[36m0.7422\u001b[0m        0.5196       0.6781        0.6394        19.9281\n",
      "     18       \u001b[36m0.7564\u001b[0m        \u001b[32m0.4901\u001b[0m       0.6096        0.7174        19.7978\n",
      "     19       0.7520        0.5151       0.6952        0.6108        19.8342\n",
      "     20       \u001b[36m0.7697\u001b[0m        \u001b[32m0.4860\u001b[0m       0.6918        0.7248        19.7881\n",
      "     21       0.7679        \u001b[32m0.4815\u001b[0m       \u001b[35m0.7295\u001b[0m        \u001b[31m0.6024\u001b[0m     +  19.7781\n",
      "     22       0.7617        0.4893       0.6781        0.6796        19.8086\n",
      "     23       0.7688        0.4903       0.7021        0.6194        19.7572\n",
      "     24       \u001b[36m0.7803\u001b[0m        \u001b[32m0.4568\u001b[0m       0.6712        0.7122        19.8026\n",
      "     25       \u001b[36m0.7910\u001b[0m        \u001b[32m0.4512\u001b[0m       0.7055        0.7273        19.7816\n",
      "     26       0.7892        0.4615       0.7158        0.7026        19.7524\n",
      "     27       0.7759        0.4789       0.7295        0.6351        19.7079\n",
      "     28       \u001b[36m0.7989\u001b[0m        \u001b[32m0.4364\u001b[0m       0.7295        \u001b[31m0.5773\u001b[0m     +  19.7603\n",
      "     29       \u001b[36m0.8105\u001b[0m        \u001b[32m0.4217\u001b[0m       0.6849        0.7905        19.7504\n",
      "     30       0.8087        \u001b[32m0.4132\u001b[0m       0.7192        0.6026        19.7582\n",
      "     31       \u001b[36m0.8255\u001b[0m        \u001b[32m0.3969\u001b[0m       0.7192        0.6249        19.7715\n",
      "     32       0.8237        \u001b[32m0.3905\u001b[0m       \u001b[35m0.7466\u001b[0m        0.5865        19.7464\n",
      "     33       0.8175        \u001b[32m0.3859\u001b[0m       \u001b[35m0.7534\u001b[0m        0.6086        19.7469\n",
      "     34       \u001b[36m0.8291\u001b[0m        \u001b[32m0.3676\u001b[0m       0.6164        0.8787        19.7276\n",
      "     35       \u001b[36m0.8432\u001b[0m        0.3805       0.6610        0.9881        19.7759\n",
      "     36       \u001b[36m0.8459\u001b[0m        \u001b[32m0.3523\u001b[0m       0.6884        0.6801        19.8274\n",
      "     37       \u001b[36m0.8485\u001b[0m        \u001b[32m0.3455\u001b[0m       0.6986        0.8877        19.7516\n",
      "     38       \u001b[36m0.8698\u001b[0m        \u001b[32m0.3061\u001b[0m       0.7226        0.6681        19.7525\n",
      "     39       \u001b[36m0.8725\u001b[0m        \u001b[32m0.3058\u001b[0m       0.6233        1.7145        19.7509\n",
      "     40       0.8663        \u001b[32m0.3026\u001b[0m       0.5822        1.2391        19.8414\n",
      "     41       \u001b[36m0.8849\u001b[0m        \u001b[32m0.2802\u001b[0m       0.5445        1.4581        19.7541\n",
      "     42       0.8849        \u001b[32m0.2716\u001b[0m       0.7295        0.7400        19.8329\n",
      "     43       \u001b[36m0.9265\u001b[0m        \u001b[32m0.2093\u001b[0m       0.6610        1.5566        19.7418\n",
      "     44       0.9088        0.2387       0.5822        1.4824        19.7965\n",
      "     45       0.9061        0.2353       0.7021        1.1265        19.8559\n",
      "     46       \u001b[36m0.9327\u001b[0m        \u001b[32m0.2016\u001b[0m       0.7500        0.6473        19.8118\n",
      "     47       0.9238        \u001b[32m0.1864\u001b[0m       0.7534        0.6912        19.8184\n",
      "     48       0.9097        0.2172       0.5719        1.4006        19.7863\n",
      "     49       \u001b[36m0.9522\u001b[0m        \u001b[32m0.1503\u001b[0m       0.6644        1.6749        19.8163\n",
      "     50       0.9389        0.1577       0.7397        0.7217        19.7656\n",
      "     51       0.9415        0.1546       \u001b[35m0.7740\u001b[0m        0.6370        19.8334\n",
      "     52       0.9504        \u001b[32m0.1421\u001b[0m       0.7432        0.8610        19.8399\n",
      "     53       \u001b[36m0.9628\u001b[0m        \u001b[32m0.1072\u001b[0m       0.7740        0.8034        19.7928\n",
      "     54       \u001b[36m0.9637\u001b[0m        0.1171       0.7637        0.8612        19.8580\n",
      "     55       0.9469        0.1274       0.7671        0.7434        19.7790\n",
      "     56       0.9637        0.1075       0.7603        0.7790        19.8346\n",
      "     57       \u001b[36m0.9725\u001b[0m        \u001b[32m0.0921\u001b[0m       0.7397        0.7561        19.7804\n",
      "     58       0.9708        0.0961       0.7055        0.9720        19.8117\n",
      "     59       0.9327        0.1679       0.6884        1.6422        19.8084\n",
      "     60       0.9566        0.1238       0.7568        0.8004        19.7762\n",
      "     61       0.9646        0.1077       0.7158        1.4519        19.7720\n",
      "     62       \u001b[36m0.9832\u001b[0m        \u001b[32m0.0655\u001b[0m       0.7329        1.2976        19.8312\n",
      "     63       0.9796        0.0678       0.7671        0.9030        19.7321\n",
      "     64       0.9832        \u001b[32m0.0564\u001b[0m       0.7295        1.3869        19.8127\n",
      "     65       \u001b[36m0.9858\u001b[0m        0.0608       0.6884        1.3499        19.7978\n",
      "     66       0.9708        0.0724       0.6507        2.9737        19.7703\n",
      "     67       0.9841        \u001b[32m0.0502\u001b[0m       0.6849        2.1513        19.7789\n",
      "     68       0.9663        0.0973       0.7432        0.9685        19.7938\n",
      "     69       0.9823        0.0583       0.7466        0.9862        19.7559\n",
      "     70       0.9743        0.0715       0.7740        0.8932        19.8343\n",
      "     71       0.9814        0.0633       0.7432        1.0442        19.7956\n",
      "     72       0.9681        0.0949       0.7295        1.1862        19.8286\n",
      "     73       0.9752        0.0685       0.7158        1.0280        19.7570\n",
      "     74       \u001b[36m0.9894\u001b[0m        \u001b[32m0.0471\u001b[0m       0.7534        0.8930        19.7210\n",
      "     75       \u001b[36m0.9938\u001b[0m        \u001b[32m0.0304\u001b[0m       \u001b[35m0.7945\u001b[0m        1.0289        19.8299\n",
      "     76       0.9885        0.0411       0.6610        1.6266        19.7341\n",
      "     77       0.9885        0.0371       0.7021        1.4126        19.7858\n",
      "     78       0.9911        0.0386       0.7466        1.0003        19.7585\n",
      "     79       0.9894        0.0356       0.7568        1.0729        19.8075\n",
      "     80       0.9814        0.0572       0.7432        1.0321        19.7560\n",
      "     81       0.9805        0.0577       0.5034        3.4756        19.8083\n",
      "     82       0.9805        0.0541       0.7774        1.0475        19.8506\n",
      "     83       0.9903        0.0316       0.7329        1.0687        19.7933\n",
      "     84       0.9858        0.0395       0.7021        1.2602        19.7901\n",
      "     85       \u001b[36m0.9947\u001b[0m        \u001b[32m0.0232\u001b[0m       0.7705        0.9420        19.8006\n",
      "     86       0.9885        0.0312       0.7705        1.0652        19.7993\n",
      "     87       0.9894        0.0354       0.7260        1.1766        19.7706\n",
      "     88       \u001b[36m0.9991\u001b[0m        \u001b[32m0.0107\u001b[0m       0.7808        1.1398        19.7979\n",
      "     89       0.9841        0.0426       0.7705        1.0300        19.7742\n",
      "     90       0.9858        0.0378       0.7877        0.9831        19.7608\n",
      "     91       0.9929        0.0264       0.7432        1.3905        19.7415\n",
      "     92       0.9770        0.0699       0.6986        2.4593        19.7789\n",
      "     93       0.9690        0.0917       0.7705        1.1489        19.7518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     94       0.9894        0.0291       \u001b[35m0.8014\u001b[0m        0.9330        19.7543\n",
      "     95       0.9938        0.0216       0.7534        1.2130        19.7307\n",
      "     96       0.9973        0.0115       0.7740        1.0885        19.8439\n",
      "     97       0.9991        0.0121       0.7603        1.3643        19.7212\n",
      "     98       0.9982        0.0109       0.7808        1.1702        19.7795\n",
      "     99       0.9965        0.0147       0.7500        1.2275        19.7323\n",
      "    100       0.9655        0.0892       0.7295        1.5141        19.8608\n",
      "    101       0.9876        0.0328       0.7432        1.4437        19.7255\n",
      "    102       0.9911        0.0277       0.7603        1.3453        19.8034\n",
      "    103       0.9973        0.0157       0.7705        1.1028        19.7756\n",
      "    104       0.9947        0.0171       0.7671        1.2047        19.7832\n",
      "    105       0.9947        0.0193       0.7226        1.3711        19.8732\n",
      "    106       0.9991        \u001b[32m0.0098\u001b[0m       0.7534        1.2527        19.8366\n",
      "    107       0.9947        0.0144       0.7637        1.2820        19.8900\n",
      "    108       0.9956        0.0181       0.7534        1.4093        19.8248\n",
      "    109       0.9770        0.0743       0.7500        1.2355        19.8149\n",
      "    110       0.9752        0.0674       0.7740        1.4043        19.8281\n",
      "    111       0.9876        0.0335       0.7295        1.3496        19.8733\n",
      "    112       0.9973        0.0147       0.7432        1.6335        19.7632\n",
      "    113       0.9876        0.0353       0.7055        2.2717        19.8332\n",
      "    114       0.9876        0.0337       0.7568        1.2781        19.7517\n",
      "    115       0.9973        0.0111       0.7705        1.2187        19.7286\n",
      "    116       0.9876        0.0318       0.7877        1.2384        19.8289\n",
      "    117       0.9796        0.0456       0.4247        6.7395        19.8085\n",
      "    118       0.9823        0.0521       0.5788        3.7334        19.7805\n",
      "    119       0.9743        0.0947       0.7192        1.3503        19.8315\n",
      "    120       0.9885        0.0351       0.7466        1.3862        19.7802\n",
      "    121       0.9965        0.0164       0.7329        1.2717        19.8327\n",
      "    122       \u001b[36m1.0000\u001b[0m        \u001b[32m0.0074\u001b[0m       0.7671        1.1167        19.8045\n",
      "    123       0.9991        0.0075       0.7705        1.2103        19.8072\n",
      "    124       1.0000        \u001b[32m0.0052\u001b[0m       0.7603        1.2043        19.8094\n",
      "    125       1.0000        \u001b[32m0.0036\u001b[0m       0.7808        1.1603        19.7804\n",
      "    126       0.9947        0.0138       0.4589        5.3992        19.8042\n",
      "    127       0.9894        0.0331       0.6781        3.3799        19.7925\n",
      "    128       0.9956        0.0097       0.7705        1.3034        19.7979\n",
      "    129       0.9938        0.0214       0.7295        2.3111        19.8033\n",
      "    130       0.9956        0.0138       0.6678        2.3109        19.8350\n",
      "    131       0.9965        0.0163       0.7603        1.3170        19.8578\n",
      "    132       0.9991        0.0041       0.7568        1.3042        19.8214\n",
      "    133       1.0000        0.0045       0.7637        1.3016        19.7297\n",
      "    134       0.9965        0.0087       0.7055        1.7977        19.7571\n",
      "    135       0.9956        0.0146       0.5685        3.4242        19.8292\n",
      "    136       0.9894        0.0258       0.7705        1.2950        19.7991\n",
      "    137       0.9982        0.0084       0.6507        2.5137        19.7656\n",
      "    138       0.9920        0.0158       0.7671        1.6068        19.7960\n",
      "    139       0.9973        0.0109       0.7603        1.8009        19.8018\n",
      "    140       0.9991        0.0069       0.7397        1.4399        19.7683\n",
      "    141       0.9982        0.0095       0.7295        1.5991        19.7902\n",
      "    142       0.9956        0.0106       0.7397        1.8683        19.8230\n",
      "    143       0.9973        0.0094       0.7740        1.4039        19.8108\n",
      "    144       0.9973        0.0130       0.7192        1.8084        19.7712\n",
      "    145       0.9867        0.0290       0.7705        1.5125        19.7923\n",
      "    146       0.9734        0.0703       0.7329        1.6270        19.8106\n",
      "    147       0.9911        0.0280       0.7603        1.3164        19.7304\n",
      "    148       0.9911        0.0240       0.7637        1.5432        19.7794\n",
      "    149       0.9982        0.0093       0.7432        1.6139        19.7627\n",
      "    150       0.9982        0.0053       0.7637        1.6054        19.8022\n",
      "    151       0.9991        0.0056       0.7534        1.4803        19.7425\n",
      "    152       0.9965        0.0081       0.7705        1.3433        19.7749\n",
      "    153       0.9982        0.0068       0.7568        1.6244        19.8124\n",
      "    154       0.9965        0.0099       0.7603        1.9158        19.7840\n",
      "    155       0.9885        0.0365       0.7740        1.9825        19.7772\n",
      "    156       0.9894        0.0450       0.7637        1.5078        19.7758\n",
      "    157       0.9929        0.0175       0.6986        1.9077        19.7224\n",
      "    158       0.9876        0.0369       0.7260        1.9451        19.7527\n",
      "    159       0.9965        0.0134       0.7432        1.5248        19.7484\n",
      "    160       0.9991        0.0054       0.7740        1.8469        19.7393\n",
      "    161       0.9982        0.0116       0.7021        1.8470        19.8409\n",
      "    162       0.9982        0.0093       0.7705        1.3600        19.8593\n",
      "    163       0.9991        0.0062       0.7671        1.3364        19.7909\n",
      "    164       1.0000        \u001b[32m0.0036\u001b[0m       0.7603        1.3640        19.8159\n",
      "    165       1.0000        \u001b[32m0.0027\u001b[0m       0.7500        1.4347        19.7742\n",
      "    166       1.0000        \u001b[32m0.0021\u001b[0m       0.7808        1.5575        19.8531\n",
      "    167       0.9991        0.0033       0.7226        1.7415        19.8383\n",
      "    168       0.9991        0.0032       0.7774        1.4812        19.8399\n",
      "    169       0.9965        0.0150       0.6678        5.5854        19.8397\n",
      "    170       0.9911        0.0231       0.7534        2.0599        19.7750\n",
      "    171       0.9973        0.0107       0.7603        1.6212        19.7977\n",
      "    172       0.9522        0.1646       0.4384        6.8788        19.7518\n",
      "    173       0.9752        0.0794       0.5342        2.9624        19.7910\n",
      "    174       0.9973        0.0130       0.7397        1.0555        19.7840\n",
      "    175       0.9956        0.0154       0.6062        2.5104        19.8053\n",
      "    176       0.9973        0.0076       0.7637        1.3306        19.7910\n",
      "    177       0.9991        0.0053       0.7637        1.2246        19.7772\n",
      "    178       1.0000        0.0021       0.7637        1.1956        19.8211\n",
      "    179       0.9965        0.0080       0.7740        1.2621        19.8653\n",
      "    180       1.0000        0.0045       0.7671        1.3352        19.8187\n",
      "    181       1.0000        0.0025       0.7705        1.2681        19.8408\n",
      "    182       0.9982        0.0044       0.7671        1.2336        19.7766\n",
      "    183       0.9938        0.0132       0.7466        2.1022        19.8405\n",
      "    184       0.9938        0.0124       0.7705        1.4498        19.7734\n",
      "    185       0.9965        0.0102       0.7192        1.7780        19.7758\n",
      "    186       0.9965        0.0073       0.6986        1.9390        19.8765\n",
      "    187       0.9956        0.0123       0.7568        1.5853        19.8184\n",
      "    188       1.0000        0.0029       0.7705        1.4723        19.8066\n",
      "    189       0.9991        0.0046       0.7500        1.6140        19.8206\n",
      "    190       0.9982        0.0063       0.7637        1.3691        19.8365\n",
      "    191       0.9947        0.0178       0.7774        1.4594        19.8862\n",
      "    192       1.0000        0.0023       0.7603        1.7242        19.8284\n",
      "    193       0.9991        \u001b[32m0.0017\u001b[0m       0.7603        1.4449        19.8427\n",
      "    194       0.9982        0.0088       0.7466        2.3753        19.8367\n",
      "    195       0.9911        0.0354       0.6918        2.9942        19.7751\n",
      "    196       0.9699        0.1002       0.6986        1.7852        19.7310\n",
      "    197       0.9938        0.0162       0.7466        1.4485        19.8129\n",
      "    198       0.9991        0.0054       0.7500        1.4354        19.8142\n",
      "    199       0.9982        0.0062       0.7603        1.4915        19.8102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    200       0.9991        0.0036       0.7671        1.4391        19.8383\n",
      "    201       0.9982        0.0065       0.7500        1.5997        19.8040\n",
      "    202       0.9965        0.0066       0.7500        1.5305        19.8210\n",
      "    203       0.9973        0.0051       0.7842        1.5855        19.8001\n",
      "    204       0.9965        0.0078       0.7637        1.5003        19.7926\n",
      "    205       0.9841        0.0416       0.5205        5.0827        19.8363\n",
      "    206       0.9885        0.0304       0.7466        1.5637        19.8579\n",
      "    207       0.9947        0.0135       0.6712        5.1282        19.7431\n",
      "    208       0.9982        0.0088       0.7466        1.4682        19.8013\n",
      "    209       0.9947        0.0216       0.7740        1.7113        19.7644\n",
      "    210       0.9982        0.0062       0.7363        2.3823        19.7829\n",
      "    211       0.9929        0.0177       0.7466        1.7097        19.7994\n",
      "    212       0.9903        0.0176       0.7911        1.5676        19.7669\n",
      "    213       0.9956        0.0097       0.7055        2.0053        19.7827\n",
      "    214       0.9991        0.0047       0.7397        1.7294        19.7914\n",
      "    215       1.0000        0.0019       0.7740        1.5432        19.7829\n",
      "    216       1.0000        0.0018       0.7671        2.0560        19.7835\n",
      "    217       1.0000        \u001b[32m0.0012\u001b[0m       0.7568        1.7411        19.8021\n",
      "    218       1.0000        \u001b[32m0.0006\u001b[0m       0.7500        1.5567        19.7703\n",
      "    219       1.0000        0.0012       0.7500        1.7204        19.8058\n",
      "    220       0.9973        0.0072       0.7432        2.4877        19.8243\n",
      "    221       0.9929        0.0279       0.7842        1.3936        19.8886\n",
      "    222       0.9938        0.0199       0.5959        3.9370        19.7469\n",
      "    223       0.9876        0.0379       0.7671        1.7249        19.8325\n",
      "    224       0.9956        0.0109       0.7774        1.3319        19.7502\n",
      "    225       0.9973        0.0119       0.7466        1.8760        19.8003\n",
      "    226       0.9965        0.0116       0.7500        1.7186        19.7979\n",
      "    227       0.9956        0.0098       0.7260        2.5334        19.8003\n",
      "    228       1.0000        0.0025       0.7500        1.8619        19.8394\n",
      "    229       0.9991        0.0033       0.7568        1.4694        19.8202\n",
      "    230       1.0000        0.0006       0.7808        1.4724        19.7616\n",
      "    231       1.0000        0.0015       0.7534        1.5520        19.8267\n",
      "    232       1.0000        0.0007       0.7671        1.5221        19.7708\n",
      "    233       1.0000        0.0012       0.7740        1.6282        19.8188\n",
      "    234       0.9991        0.0021       0.7774        1.5199        19.8166\n",
      "    235       1.0000        0.0015       0.7603        1.6428        19.8371\n",
      "    236       0.9991        0.0022       0.7705        2.0268        19.8302\n",
      "    237       0.9991        0.0024       0.7192        3.0042        19.8365\n",
      "    238       0.9982        0.0047       0.7637        2.1124        19.8043\n",
      "    239       1.0000        0.0028       0.7329        2.0440        19.8261\n",
      "    240       1.0000        0.0008       0.7637        1.7205        19.8112\n",
      "    241       0.9982        0.0131       0.7397        1.9103        19.7603\n",
      "    242       0.9956        0.0151       0.7534        1.8543        19.8565\n",
      "    243       0.9973        0.0065       0.7603        1.5005        19.8263\n",
      "    244       0.9823        0.0494       0.4932        6.0882        19.8536\n",
      "    245       0.9770        0.0816       0.7568        1.6833        19.8294\n",
      "    246       0.9965        0.0152       0.7534        1.5833        19.8313\n",
      "    247       0.9956        0.0071       0.7705        1.6527        19.7805\n",
      "    248       0.9973        0.0084       0.7637        1.8394        19.7730\n",
      "    249       1.0000        0.0021       0.7671        1.5250        19.8130\n",
      "    250       1.0000        0.0013       0.7534        1.5237        19.8094\n",
      "    251       0.9991        0.0027       0.7466        1.5405        19.8173\n",
      "    252       1.0000        0.0011       0.7637        1.6357        19.8121\n",
      "    253       1.0000        0.0011       0.7740        1.6914        19.8523\n",
      "    254       1.0000        0.0016       0.7500        1.6563        19.8241\n",
      "    255       0.9991        0.0051       0.7671        2.0989        19.8087\n",
      "    256       1.0000        0.0016       0.7568        1.7675        19.7827\n",
      "    257       0.9982        0.0042       0.7603        1.9509        19.8330\n",
      "    258       1.0000        0.0007       0.7671        1.7787        19.8297\n",
      "    259       1.0000        0.0006       0.7466        1.7565        19.8268\n",
      "    260       1.0000        0.0013       0.7671        1.8637        19.7382\n",
      "    261       0.9956        0.0170       0.7295        2.5950        19.7748\n",
      "    262       0.9903        0.0288       0.7158        2.1763        19.8335\n",
      "    263       0.9903        0.0283       0.7397        2.4732        19.7680\n",
      "    264       0.9956        0.0090       0.7808        1.4956        19.7922\n",
      "    265       0.9982        0.0052       0.6404        2.8110        19.8364\n",
      "    266       0.9867        0.0513       0.6952        2.4779        19.7698\n",
      "    267       0.9832        0.0595       0.5034        4.5261        19.8383\n",
      "    268       0.9929        0.0134       0.7671        1.6028        19.8199\n",
      "    269       0.9973        0.0068       0.7397        1.6909        19.8020\n",
      "    270       0.9956        0.0091       0.7432        1.5057        19.7674\n",
      "    271       1.0000        0.0020       0.7534        1.6935        19.7329\n",
      "    272       1.0000        0.0017       0.7534        1.5802        19.7537\n",
      "    273       0.9973        0.0030       0.7774        1.7247        19.8142\n",
      "    274       1.0000        0.0018       0.7740        1.6737        19.7681\n",
      "    275       1.0000        0.0010       0.7774        1.8248        19.8066\n",
      "    276       0.9991        0.0031       0.7295        2.0227        19.7905\n",
      "    277       0.9929        0.0163       0.6336        6.8245        19.8279\n",
      "    278       0.9956        0.0186       0.7260        2.1530        19.8468\n",
      "    279       0.9947        0.0107       0.7911        1.6106        19.8161\n",
      "    280       0.9911        0.0233       0.6712        2.3395        19.8139\n",
      "    281       0.9965        0.0088       0.7842        1.5789        19.8096\n",
      "    282       0.9991        0.0046       0.7637        1.7983        19.8308\n",
      "    283       0.9982        0.0052       0.7500        1.5985        19.7886\n",
      "    284       0.9982        0.0066       0.7740        1.6786        19.8159\n",
      "    285       0.9903        0.0286       0.7123        2.1160        19.7426\n",
      "    286       0.9973        0.0084       0.7397        1.8867        19.7720\n",
      "    287       0.9973        0.0034       0.7432        2.3777        19.8053\n",
      "    288       0.9938        0.0146       0.7603        1.9444        19.7750\n",
      "    289       0.9973        0.0145       0.7192        1.7873        19.7673\n",
      "    290       0.9991        0.0026       0.7568        1.7410        19.8307\n",
      "    291       1.0000        0.0021       0.7671        1.8183        19.8203\n",
      "    292       0.9973        0.0063       0.7089        2.2852        19.8372\n",
      "    293       0.9982        0.0038       0.7740        1.7825        19.8402\n",
      "    294       0.9991        0.0021       0.6815        2.8061        19.8852\n",
      "    295       0.9973        0.0046       0.7397        2.4050        19.8406\n",
      "    296       0.9965        0.0111       0.7637        1.6265        19.7767\n",
      "    297       0.9982        0.0065       0.7295        2.2913        19.7839\n",
      "    298       0.9973        0.0086       0.7637        2.3257        19.8446\n",
      "    299       0.9982        0.0054       0.7568        2.1824        19.8107\n",
      "    300       0.9965        0.0115       0.7603        2.3721        19.7982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=VGG(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ReLU(inplace=True)\n",
       "      (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (17): ReLU(inplace=True)\n",
       "      (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (24): ReLU(inplace=True)\n",
       "      (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (27): ReLU(inplace=True)\n",
       "      (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=25088, out_features=2048, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(train_dataset, y=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.models_multi_task as md_multi\n",
    "from models.multitask_training_session import *\n",
    "from datasets.iemocap import IemocapDataset\n",
    "from datasets.ramas import RamasDataset\n",
    "from constants import *\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= INITIALIZING DATASET Ramas224BinaryMultiNormal_224_train ===============\n",
      "============================ SUCCESS! =========================\n",
      "============= INITIALIZING DATASET Ramas224BinaryMultiNormal_224_test ===============\n",
      "============================ SUCCESS! =========================\n"
     ]
    }
   ],
   "source": [
    "ramas_224_train = RamasDataset(RAMAS_PATH_TO_WAVS_BINARY, 'Ramas224BinaryMultiNormal',\n",
    "                 spectrogram_shape=224,\n",
    "                 augmentation=True, padding='repeat', mode='train',  tasks='multi', type='binary')\n",
    "ramas_224_test = RamasDataset(RAMAS_PATH_TO_WAVS_BINARY, 'Ramas224BinaryMultiNormal',\n",
    "                 spectrogram_shape=224,\n",
    "                 augmentation=False, padding='repeat', mode='test',  tasks='multi', type='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZING TRAINING SESSION...\n",
      "Loaders ready\n",
      "TRAINING SESSION VGGAverageWeighting__Ramas224BinaryMultiNormal_224_train INITIALIZED\n",
      "Trying to load checkpoint from file\n",
      "File not found, starting from scratch...\n"
     ]
    }
   ],
   "source": [
    "train_ds = ramas_224_train\n",
    "test_ds = ramas_224_test\n",
    "model = md_multi.vgg(num_emotions=2, num_speakers=12, num_genders=2, type=11, bn=True)\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\") \n",
    "# device = torch.device(\"cpu\") \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-5)\n",
    "ts = TrainingSession(name='VGGAverageWeighting',\n",
    "                      model=model,\n",
    "                      train_dataset=train_ds,\n",
    "                      test_dataset=test_ds,\n",
    "                      criterion=criterion,\n",
    "                      optimizer=optimizer,\n",
    "                      num_epochs=300,\n",
    "                      batch_size=32,\n",
    "                      device=device,\n",
    "                     path_to_weights=WEIGHTS_FOLDER,\n",
    "                     path_to_results=RESULTS_FOLDER,\n",
    "                    loss_weighter=averaged_sum  # Из файла multitask_training_session.py\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "=============TRAINING SESSION STARTED AT EPOCH 1=====================\n",
      "======================================================================\n",
      "Epoch #1\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.7608 | speaker = 2.4457 | gender = 0.7015 | total = 1.8070 |\n",
      "# Train accuracies | emotion = 0.4481842338352524 | speaker = 0.15943312666076173 | gender = 0.4765279007971656 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0081 | speaker = 0.0341 | gender = 0.0114 | total = 0.0254 |\n",
      "# Validation accuracies | emotion = 0.386986301369863 | speaker = 0.14726027397260275 | gender = 0.4897260273972603 |\n",
      "# Saving checkpoint...\n",
      "## Saving best model_alex\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #2\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.8596 | speaker = 2.3860 | gender = 0.8186 | total = 1.7511 |\n",
      "# Train accuracies | emotion = 0.4357838795394154 | speaker = 0.18113374667847654 | gender = 0.46722763507528786 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0139 | speaker = 0.0322 | gender = 0.0160 | total = 0.0240 |\n",
      "# Validation accuracies | emotion = 0.386986301369863 | speaker = 0.1917808219178082 | gender = 0.4897260273972603 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #3\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.8447 | speaker = 2.3316 | gender = 0.8258 | total = 1.7116 |\n",
      "# Train accuracies | emotion = 0.431945674638323 | speaker = 0.19161499852376734 | gender = 0.46294656037791554 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0107 | speaker = 0.0351 | gender = 0.0130 | total = 0.0258 |\n",
      "# Validation accuracies | emotion = 0.386986301369863 | speaker = 0.2191780821917808 | gender = 0.4897260273972603 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #4\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.8042 | speaker = 2.2579 | gender = 0.8133 | total = 1.6563 |\n",
      "# Train accuracies | emotion = 0.429140832595217 | speaker = 0.20372010628875112 | gender = 0.44596988485385297 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0129 | speaker = 0.0303 | gender = 0.0153 | total = 0.0226 |\n",
      "# Validation accuracies | emotion = 0.386986301369863 | speaker = 0.2465753424657534 | gender = 0.4041095890410959 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #5\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.7849 | speaker = 2.1705 | gender = 0.8003 | total = 1.5910 |\n",
      "# Train accuracies | emotion = 0.42816651904340125 | speaker = 0.21806908768821967 | gender = 0.41984056687333926 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0111 | speaker = 0.0324 | gender = 0.0109 | total = 0.0238 |\n",
      "# Validation accuracies | emotion = 0.339041095890411 | speaker = 0.2808219178082192 | gender = 0.2705479452054795 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #6\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.7553 | speaker = 2.0828 | gender = 0.7724 | total = 1.5265 |\n",
      "# Train accuracies | emotion = 0.4248597578978447 | speaker = 0.2301446708001181 | gender = 0.4112784174785946 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0094 | speaker = 0.0357 | gender = 0.0101 | total = 0.0265 |\n",
      "# Validation accuracies | emotion = 0.4520547945205479 | speaker = 0.3356164383561644 | gender = 0.2705479452054795 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #7\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.7202 | speaker = 1.9508 | gender = 0.7159 | total = 1.4294 |\n",
      "# Train accuracies | emotion = 0.43300012653422754 | speaker = 0.24446412754650132 | gender = 0.42199164874098444 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0102 | speaker = 0.0273 | gender = 0.0100 | total = 0.0200 |\n",
      "# Validation accuracies | emotion = 0.5102739726027398 | speaker = 0.3561643835616438 | gender = 0.565068493150685 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #8\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.7068 | speaker = 1.8493 | gender = 0.6829 | total = 1.3560 |\n",
      "# Train accuracies | emotion = 0.4404340124003543 | speaker = 0.2607395925597874 | gender = 0.43877325066430467 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0111 | speaker = 0.0157 | gender = 0.0086 | total = 0.0126 |\n",
      "# Validation accuracies | emotion = 0.6095890410958904 | speaker = 0.4349315068493151 | gender = 0.7397260273972602 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #9\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.6836 | speaker = 1.7103 | gender = 0.6363 | total = 1.2550 |\n",
      "# Train accuracies | emotion = 0.45251451628776695 | speaker = 0.2765475838992225 | gender = 0.46481645507331953 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0069 | speaker = 0.0138 | gender = 0.0065 | total = 0.0103 |\n",
      "# Validation accuracies | emotion = 0.613013698630137 | speaker = 0.4691780821917808 | gender = 0.7842465753424658 |\n",
      "# Saving checkpoint...\n",
      "## Saving best model_alex\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #10\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.6811 | speaker = 1.5761 | gender = 0.5827 | total = 1.1597 |\n",
      "# Train accuracies | emotion = 0.46519043401240034 | speaker = 0.29557130203720106 | gender = 0.4941541186891054 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0085 | speaker = 0.0203 | gender = 0.0079 | total = 0.0149 |\n",
      "# Validation accuracies | emotion = 0.6232876712328768 | speaker = 0.5171232876712328 | gender = 0.8732876712328768 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #11\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.6770 | speaker = 1.4557 | gender = 0.5494 | total = 1.0772 |\n",
      "# Train accuracies | emotion = 0.47346807311377725 | speaker = 0.31379338110959015 | gender = 0.5208954022062968 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0072 | speaker = 0.0125 | gender = 0.0054 | total = 0.0094 |\n",
      "# Validation accuracies | emotion = 0.6541095890410958 | speaker = 0.5547945205479452 | gender = 0.8732876712328768 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #12\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.6601 | speaker = 1.3523 | gender = 0.4973 | total = 1.0037 |\n",
      "# Train accuracies | emotion = 0.4843519338647771 | speaker = 0.33348095659876 | gender = 0.54162976085031 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0108 | speaker = 0.0172 | gender = 0.0039 | total = 0.0134 |\n",
      "# Validation accuracies | emotion = 0.6438356164383562 | speaker = 0.5787671232876712 | gender = 0.8698630136986302 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #13\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.6520 | speaker = 1.2516 | gender = 0.4662 | total = 0.9349 |\n",
      "# Train accuracies | emotion = 0.4941064250187368 | speaker = 0.35266062546842 | gender = 0.5659875996457041 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0081 | speaker = 0.0164 | gender = 0.0075 | total = 0.0122 |\n",
      "# Validation accuracies | emotion = 0.6506849315068494 | speaker = 0.589041095890411 | gender = 0.8458904109589042 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #14\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.6434 | speaker = 1.2107 | gender = 0.4649 | total = 0.9081 |\n",
      "# Train accuracies | emotion = 0.5042388966215361 | speaker = 0.37068201948627105 | gender = 0.5867392129571048 |\n",
      "# Validation process on validation set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Validation losses | emotion = 0.0069 | speaker = 0.0315 | gender = 0.0076 | total = 0.0239 |\n",
      "# Validation accuracies | emotion = 0.6541095890410958 | speaker = 0.613013698630137 | gender = 0.8835616438356164 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #15\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.6370 | speaker = 1.1191 | gender = 0.4129 | total = 0.8487 |\n",
      "# Train accuracies | emotion = 0.5131975199291409 | speaker = 0.3895482728077945 | gender = 0.6047239444936522 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0058 | speaker = 0.0150 | gender = 0.0085 | total = 0.0113 |\n",
      "# Validation accuracies | emotion = 0.684931506849315 | speaker = 0.6404109589041096 | gender = 0.9006849315068494 |\n",
      "# Saving checkpoint...\n",
      "## Saving best model_alex\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #16\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.6403 | speaker = 1.0605 | gender = 0.4021 | total = 0.8136 |\n",
      "# Train accuracies | emotion = 0.52075952170062 | speaker = 0.40716341895482727 | gender = 0.6241696191319752 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0081 | speaker = 0.0095 | gender = 0.0048 | total = 0.0080 |\n",
      "# Validation accuracies | emotion = 0.6506849315068494 | speaker = 0.6712328767123288 | gender = 0.8561643835616438 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #17\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.6278 | speaker = 0.9933 | gender = 0.3938 | total = 0.7673 |\n",
      "# Train accuracies | emotion = 0.5288386390871672 | speaker = 0.4237482415463971 | gender = 0.6404939300786745 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0054 | speaker = 0.0060 | gender = 0.0048 | total = 0.0055 |\n",
      "# Validation accuracies | emotion = 0.6917808219178082 | speaker = 0.6712328767123288 | gender = 0.8767123287671232 |\n",
      "# Saving checkpoint...\n",
      "## Saving best model_alex\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #18\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.6237 | speaker = 0.9708 | gender = 0.3737 | total = 0.7518 |\n",
      "# Train accuracies | emotion = 0.5359708690089559 | speaker = 0.43780139750024605 | gender = 0.6551520519633894 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0088 | speaker = 0.0100 | gender = 0.0064 | total = 0.0087 |\n",
      "# Validation accuracies | emotion = 0.684931506849315 | speaker = 0.7054794520547946 | gender = 0.8595890410958904 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #19\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.6096 | speaker = 0.9123 | gender = 0.3579 | total = 0.7152 |\n",
      "# Train accuracies | emotion = 0.5428651344925645 | speaker = 0.45303249265768497 | gender = 0.6684536851428838 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0037 | speaker = 0.0128 | gender = 0.0052 | total = 0.0095 |\n",
      "# Validation accuracies | emotion = 0.660958904109589 | speaker = 0.702054794520548 | gender = 0.910958904109589 |\n",
      "# Saving checkpoint...\n",
      "## Saving best model_alex\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #20\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.6018 | speaker = 0.8689 | gender = 0.3542 | total = 0.6864 |\n",
      "# Train accuracies | emotion = 0.5493799822852081 | speaker = 0.46718334809565987 | gender = 0.6799822852081489 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0087 | speaker = 0.0163 | gender = 0.0031 | total = 0.0125 |\n",
      "# Validation accuracies | emotion = 0.7054794520547946 | speaker = 0.7191780821917808 | gender = 0.9143835616438356 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #21\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.6069 | speaker = 0.8353 | gender = 0.3293 | total = 0.6690 |\n",
      "# Train accuracies | emotion = 0.5550634780041335 | speaker = 0.48053481800160275 | gender = 0.6904129233624362 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0093 | speaker = 0.0098 | gender = 0.0035 | total = 0.0087 |\n",
      "# Validation accuracies | emotion = 0.6883561643835616 | speaker = 0.7157534246575342 | gender = 0.8972602739726028 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #22\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.5922 | speaker = 0.8117 | gender = 0.3262 | total = 0.6548 |\n",
      "# Train accuracies | emotion = 0.5614783799017634 | speaker = 0.4929946050406635 | gender = 0.7012641919639262 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0104 | speaker = 0.0089 | gender = 0.0044 | total = 0.0088 |\n",
      "# Validation accuracies | emotion = 0.6883561643835616 | speaker = 0.7157534246575342 | gender = 0.9041095890410958 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #23\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.5908 | speaker = 0.7764 | gender = 0.3170 | total = 0.6300 |\n",
      "# Train accuracies | emotion = 0.5670658913236031 | speaker = 0.5051411406785535 | gender = 0.7100935803134748 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0107 | speaker = 0.0251 | gender = 0.0079 | total = 0.0185 |\n",
      "# Validation accuracies | emotion = 0.726027397260274 | speaker = 0.7363013698630136 | gender = 0.9075342465753424 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #24\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.5766 | speaker = 0.7434 | gender = 0.2993 | total = 0.6081 |\n",
      "# Train accuracies | emotion = 0.5723354000590493 | speaker = 0.5162016533805728 | gender = 0.7196634189548273 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0047 | speaker = 0.0032 | gender = 0.0030 | total = 0.0038 |\n",
      "# Validation accuracies | emotion = 0.7054794520547946 | speaker = 0.7363013698630136 | gender = 0.9041095890410958 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #25\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.5758 | speaker = 0.7206 | gender = 0.2988 | total = 0.5966 |\n",
      "# Train accuracies | emotion = 0.577289636846767 | speaker = 0.5269796279893711 | gender = 0.7281133746678476 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0069 | speaker = 0.0089 | gender = 0.0052 | total = 0.0074 |\n",
      "# Validation accuracies | emotion = 0.7294520547945206 | speaker = 0.7534246575342466 | gender = 0.8424657534246576 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #26\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.5645 | speaker = 0.7025 | gender = 0.2978 | total = 0.5845 |\n",
      "# Train accuracies | emotion = 0.5826463173673094 | speaker = 0.5369625945356681 | gender = 0.7355726647134974 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0134 | speaker = 0.0105 | gender = 0.0021 | total = 0.0113 |\n",
      "# Validation accuracies | emotion = 0.7226027397260274 | speaker = 0.7602739726027398 | gender = 0.9041095890410958 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #27\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.5580 | speaker = 0.6675 | gender = 0.2713 | total = 0.5621 |\n",
      "# Train accuracies | emotion = 0.5874749860578027 | speaker = 0.5468949906505265 | gender = 0.7430699078174721 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0067 | speaker = 0.0131 | gender = 0.0023 | total = 0.0100 |\n",
      "# Validation accuracies | emotion = 0.7157534246575342 | speaker = 0.7636986301369864 | gender = 0.9041095890410958 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #28\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.5807 | speaker = 0.6573 | gender = 0.2929 | total = 0.5653 |\n",
      "# Train accuracies | emotion = 0.5906301404529926 | speaker = 0.5552638238643554 | gender = 0.7501265342275085 |\n",
      "# Validation process on validation set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Validation losses | emotion = 0.0062 | speaker = 0.0106 | gender = 0.0032 | total = 0.0080 |\n",
      "# Validation accuracies | emotion = 0.7397260273972602 | speaker = 0.773972602739726 | gender = 0.9075342465753424 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #29\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.5562 | speaker = 0.6032 | gender = 0.2694 | total = 0.5284 |\n",
      "# Train accuracies | emotion = 0.59500320698818 | speaker = 0.564643718884579 | gender = 0.7565437830243426 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0062 | speaker = 0.0210 | gender = 0.0108 | total = 0.0157 |\n",
      "# Validation accuracies | emotion = 0.7294520547945206 | speaker = 0.7465753424657534 | gender = 0.8801369863013698 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #30\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.5528 | speaker = 0.6043 | gender = 0.2637 | total = 0.5273 |\n",
      "# Train accuracies | emotion = 0.598819013876587 | speaker = 0.5729554177738412 | gender = 0.7626808385001477 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0134 | speaker = 0.0051 | gender = 0.0021 | total = 0.0104 |\n",
      "# Validation accuracies | emotion = 0.708904109589041 | speaker = 0.7876712328767124 | gender = 0.9143835616438356 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #31\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.5534 | speaker = 0.5538 | gender = 0.2522 | total = 0.5062 |\n",
      "# Train accuracies | emotion = 0.6029315123289237 | speaker = 0.5815594731278036 | gender = 0.7687933940969742 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0095 | speaker = 0.0191 | gender = 0.0033 | total = 0.0146 |\n",
      "# Validation accuracies | emotion = 0.7328767123287672 | speaker = 0.773972602739726 | gender = 0.9075342465753424 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #32\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.5556 | speaker = 0.5440 | gender = 0.2468 | total = 0.5035 |\n",
      "# Train accuracies | emotion = 0.606205713020372 | speaker = 0.5898472099202834 | gender = 0.7741917626217892 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0113 | speaker = 0.0140 | gender = 0.0054 | total = 0.0115 |\n",
      "# Validation accuracies | emotion = 0.6917808219178082 | speaker = 0.726027397260274 | gender = 0.9006849315068494 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #33\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.5506 | speaker = 0.5585 | gender = 0.2548 | total = 0.5065 |\n",
      "# Train accuracies | emotion = 0.609684086211987 | speaker = 0.5974179348847196 | gender = 0.7796924067960384 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0117 | speaker = 0.0033 | gender = 0.0014 | total = 0.0094 |\n",
      "# Validation accuracies | emotion = 0.7157534246575342 | speaker = 0.7945205479452054 | gender = 0.886986301369863 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #34\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.5367 | speaker = 0.5140 | gender = 0.2505 | total = 0.4818 |\n",
      "# Train accuracies | emotion = 0.6132965143541916 | speaker = 0.6052206533632053 | gender = 0.7845568696920753 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0026 | speaker = 0.0025 | gender = 0.0017 | total = 0.0023 |\n",
      "# Validation accuracies | emotion = 0.7363013698630136 | speaker = 0.7808219178082192 | gender = 0.9246575342465754 |\n",
      "# Saving checkpoint...\n",
      "## Saving best model_alex\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #35\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.5501 | speaker = 0.5228 | gender = 0.2343 | total = 0.4982 |\n",
      "# Train accuracies | emotion = 0.6166772111856257 | speaker = 0.6120460584588131 | gender = 0.7892192838162723 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0096 | speaker = 0.0507 | gender = 0.0165 | total = 0.0382 |\n",
      "# Validation accuracies | emotion = 0.7054794520547946 | speaker = 0.7397260273972602 | gender = 0.928082191780822 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #36\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.5385 | speaker = 0.4975 | gender = 0.2526 | total = 0.4765 |\n",
      "# Train accuracies | emotion = 0.6201161303021356 | speaker = 0.6190581635665781 | gender = 0.7934750516681429 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0138 | speaker = 0.0067 | gender = 0.0007 | total = 0.0112 |\n",
      "# Validation accuracies | emotion = 0.726027397260274 | speaker = 0.8013698630136986 | gender = 0.8424657534246576 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #37\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.5532 | speaker = 0.4939 | gender = 0.2290 | total = 0.4801 |\n",
      "# Train accuracies | emotion = 0.6226749335695305 | speaker = 0.62533215234721 | gender = 0.7976683503698562 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0041 | speaker = 0.0089 | gender = 0.0039 | total = 0.0066 |\n",
      "# Validation accuracies | emotion = 0.7328767123287672 | speaker = 0.7671232876712328 | gender = 0.9246575342465754 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #38\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.5212 | speaker = 0.4769 | gender = 0.2331 | total = 0.4572 |\n",
      "# Train accuracies | emotion = 0.6256118595869656 | speaker = 0.6315556384317748 | gender = 0.801850729569717 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0116 | speaker = 0.0067 | gender = 0.0026 | total = 0.0089 |\n",
      "# Validation accuracies | emotion = 0.7054794520547946 | speaker = 0.8253424657534246 | gender = 0.9041095890410958 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #39\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.5378 | speaker = 0.4642 | gender = 0.2317 | total = 0.4681 |\n",
      "# Train accuracies | emotion = 0.628625286729804 | speaker = 0.6376189502850265 | gender = 0.8057959165133656 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0030 | speaker = 0.0082 | gender = 0.0042 | total = 0.0061 |\n",
      "# Validation accuracies | emotion = 0.7157534246575342 | speaker = 0.7636986301369864 | gender = 0.9212328767123288 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #40\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.5120 | speaker = 0.4498 | gender = 0.2284 | total = 0.4416 |\n",
      "# Train accuracies | emotion = 0.6318201948627103 | speaker = 0.6438219663418955 | gender = 0.8094774136403897 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0060 | speaker = 0.0039 | gender = 0.0016 | total = 0.0047 |\n",
      "# Validation accuracies | emotion = 0.726027397260274 | speaker = 0.8184931506849316 | gender = 0.9075342465753424 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #41\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.5081 | speaker = 0.4504 | gender = 0.2160 | total = 0.4424 |\n",
      "# Train accuracies | emotion = 0.6346864265808292 | speaker = 0.649614379226166 | gender = 0.8130225323510986 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0080 | speaker = 0.0063 | gender = 0.0029 | total = 0.0065 |\n",
      "# Validation accuracies | emotion = 0.7191780821917808 | speaker = 0.8013698630136986 | gender = 0.9246575342465754 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #42\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.5028 | speaker = 0.4280 | gender = 0.2153 | total = 0.4295 |\n",
      "# Train accuracies | emotion = 0.6373529039605214 | speaker = 0.6551098738875533 | gender = 0.8165464591505336 |\n",
      "# Validation process on validation set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Validation losses | emotion = 0.0054 | speaker = 0.0031 | gender = 0.0025 | total = 0.0042 |\n",
      "# Validation accuracies | emotion = 0.7123287671232876 | speaker = 0.7876712328767124 | gender = 0.9246575342465754 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #43\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.5210 | speaker = 0.4118 | gender = 0.2163 | total = 0.4335 |\n",
      "# Train accuracies | emotion = 0.6394833872329907 | speaker = 0.660699940264074 | gender = 0.8199064823779019 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0044 | speaker = 0.0080 | gender = 0.0028 | total = 0.0060 |\n",
      "# Validation accuracies | emotion = 0.708904109589041 | speaker = 0.7671232876712328 | gender = 0.928082191780822 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #44\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.5045 | speaker = 0.3900 | gender = 0.2096 | total = 0.4171 |\n",
      "# Train accuracies | emotion = 0.6421410741605604 | speaker = 0.6660560431596747 | gender = 0.8230332554956116 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0096 | speaker = 0.0027 | gender = 0.0024 | total = 0.0072 |\n",
      "# Validation accuracies | emotion = 0.7328767123287672 | speaker = 0.8116438356164384 | gender = 0.9041095890410958 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #45\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.4973 | speaker = 0.4083 | gender = 0.2167 | total = 0.4175 |\n",
      "# Train accuracies | emotion = 0.6449365219958666 | speaker = 0.6711544139356362 | gender = 0.8259423285109734 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0045 | speaker = 0.0015 | gender = 0.0013 | total = 0.0034 |\n",
      "# Validation accuracies | emotion = 0.7157534246575342 | speaker = 0.821917808219178 | gender = 0.928082191780822 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #46\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.5068 | speaker = 0.4167 | gender = 0.2014 | total = 0.4322 |\n",
      "# Train accuracies | emotion = 0.6471483036161282 | speaker = 0.6756652674548466 | gender = 0.8285901336311472 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0112 | speaker = 0.0042 | gender = 0.0023 | total = 0.0084 |\n",
      "# Validation accuracies | emotion = 0.7123287671232876 | speaker = 0.7671232876712328 | gender = 0.8698630136986302 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #47\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.5126 | speaker = 0.3743 | gender = 0.2125 | total = 0.4160 |\n",
      "# Train accuracies | emotion = 0.6492659668695702 | speaker = 0.6804741533648682 | gender = 0.831426794564951 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0037 | speaker = 0.0121 | gender = 0.0041 | total = 0.0089 |\n",
      "# Validation accuracies | emotion = 0.726027397260274 | speaker = 0.815068493150685 | gender = 0.928082191780822 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #48\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.4720 | speaker = 0.3500 | gender = 0.1995 | total = 0.3835 |\n",
      "# Train accuracies | emotion = 0.6521442279303218 | speaker = 0.6852302922940655 | gender = 0.8342928845586064 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0097 | speaker = 0.0024 | gender = 0.0006 | total = 0.0080 |\n",
      "# Validation accuracies | emotion = 0.7157534246575342 | speaker = 0.8356164383561644 | gender = 0.9315068493150684 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #49\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.4767 | speaker = 0.3468 | gender = 0.1969 | total = 0.3877 |\n",
      "# Train accuracies | emotion = 0.6548327036749155 | speaker = 0.6899369136494279 | gender = 0.8369877623325681 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0121 | speaker = 0.0010 | gender = 0.0008 | total = 0.0108 |\n",
      "# Validation accuracies | emotion = 0.726027397260274 | speaker = 0.815068493150685 | gender = 0.9212328767123288 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #50\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.4657 | speaker = 0.3557 | gender = 0.1894 | total = 0.3873 |\n",
      "# Train accuracies | emotion = 0.6572542072630646 | speaker = 0.6942604074402126 | gender = 0.839450841452613 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0054 | speaker = 0.0117 | gender = 0.0042 | total = 0.0087 |\n",
      "# Validation accuracies | emotion = 0.6541095890410958 | speaker = 0.815068493150685 | gender = 0.934931506849315 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #51\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.4824 | speaker = 0.3524 | gender = 0.1973 | total = 0.3924 |\n",
      "# Train accuracies | emotion = 0.659424442939266 | speaker = 0.6985706594418104 | gender = 0.8418694315635908 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0117 | speaker = 0.0038 | gender = 0.0036 | total = 0.0086 |\n",
      "# Validation accuracies | emotion = 0.7328767123287672 | speaker = 0.815068493150685 | gender = 0.9178082191780822 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #52\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.4772 | speaker = 0.3283 | gender = 0.1902 | total = 0.3816 |\n",
      "# Train accuracies | emotion = 0.6619029774477073 | speaker = 0.7028514001498944 | gender = 0.8441609320705866 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0122 | speaker = 0.0071 | gender = 0.0032 | total = 0.0093 |\n",
      "# Validation accuracies | emotion = 0.726027397260274 | speaker = 0.8082191780821918 | gender = 0.928082191780822 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #53\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.4608 | speaker = 0.3264 | gender = 0.1900 | total = 0.3705 |\n",
      "# Train accuracies | emotion = 0.6641041496064308 | speaker = 0.7067533465915737 | gender = 0.8464328091314738 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0112 | speaker = 0.0060 | gender = 0.0080 | total = 0.0090 |\n",
      "# Validation accuracies | emotion = 0.75 | speaker = 0.791095890410959 | gender = 0.9143835616438356 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #54\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.4662 | speaker = 0.3151 | gender = 0.1827 | total = 0.3727 |\n",
      "# Train accuracies | emotion = 0.6662566020404815 | speaker = 0.7106748023488502 | gender = 0.8486697503526556 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0042 | speaker = 0.0010 | gender = 0.0014 | total = 0.0033 |\n",
      "# Validation accuracies | emotion = 0.7465753424657534 | speaker = 0.8082191780821918 | gender = 0.934931506849315 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #55\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.4391 | speaker = 0.2822 | gender = 0.1717 | total = 0.3444 |\n",
      "# Train accuracies | emotion = 0.6688944359449231 | speaker = 0.7146952250583782 | gender = 0.8509219743940736 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0063 | speaker = 0.0018 | gender = 0.0014 | total = 0.0047 |\n",
      "# Validation accuracies | emotion = 0.75 | speaker = 0.815068493150685 | gender = 0.910958904109589 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #56\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.4215 | speaker = 0.3039 | gender = 0.1841 | total = 0.3438 |\n",
      "# Train accuracies | emotion = 0.6712324433759332 | speaker = 0.7184771605719347 | gender = 0.8528723269644439 |\n",
      "# Validation process on validation set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Validation losses | emotion = 0.0074 | speaker = 0.0053 | gender = 0.0047 | total = 0.0061 |\n",
      "# Validation accuracies | emotion = 0.7191780821917808 | speaker = 0.821917808219178 | gender = 0.928082191780822 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #57\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.4360 | speaker = 0.2943 | gender = 0.1676 | total = 0.3506 |\n",
      "# Train accuracies | emotion = 0.6734417975851942 | speaker = 0.7222196323403727 | gender = 0.8549251783133653 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0038 | speaker = 0.0068 | gender = 0.0026 | total = 0.0051 |\n",
      "# Validation accuracies | emotion = 0.7534246575342466 | speaker = 0.8356164383561644 | gender = 0.9246575342465754 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #58\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.4199 | speaker = 0.3157 | gender = 0.1730 | total = 0.3445 |\n",
      "# Train accuracies | emotion = 0.6756360526556916 | speaker = 0.7255887114016065 | gender = 0.8569072416847378 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0166 | speaker = 0.0049 | gender = 0.0006 | total = 0.0136 |\n",
      "# Validation accuracies | emotion = 0.7534246575342466 | speaker = 0.839041095890411 | gender = 0.9417808219178082 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #59\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.4209 | speaker = 0.2817 | gender = 0.1711 | total = 0.3348 |\n",
      "# Train accuracies | emotion = 0.678071189443185 | speaker = 0.7291138100313762 | gender = 0.8588671540736516 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0061 | speaker = 0.0043 | gender = 0.0015 | total = 0.0049 |\n",
      "# Validation accuracies | emotion = 0.7534246575342466 | speaker = 0.8493150684931506 | gender = 0.9452054794520548 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #60\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.4019 | speaker = 0.2561 | gender = 0.1624 | total = 0.3195 |\n",
      "# Train accuracies | emotion = 0.6804251550044287 | speaker = 0.7326395039858281 | gender = 0.8607026867434308 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0022 | speaker = 0.0095 | gender = 0.0016 | total = 0.0074 |\n",
      "# Validation accuracies | emotion = 0.7363013698630136 | speaker = 0.8321917808219178 | gender = 0.928082191780822 |\n",
      "# Saving checkpoint...\n",
      "## Saving best model_alex\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #61\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.3950 | speaker = 0.2876 | gender = 0.1648 | total = 0.3254 |\n",
      "# Train accuracies | emotion = 0.6827600226517011 | speaker = 0.7359479591688568 | gender = 0.8623763957658743 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0044 | speaker = 0.0011 | gender = 0.0009 | total = 0.0033 |\n",
      "# Validation accuracies | emotion = 0.7602739726027398 | speaker = 0.8356164383561644 | gender = 0.9486301369863014 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #62\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.4019 | speaker = 0.2619 | gender = 0.1530 | total = 0.3210 |\n",
      "# Train accuracies | emotion = 0.685062430355153 | speaker = 0.7393211234606704 | gender = 0.8641532615217578 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0046 | speaker = 0.0358 | gender = 0.0028 | total = 0.0305 |\n",
      "# Validation accuracies | emotion = 0.726027397260274 | speaker = 0.8493150684931506 | gender = 0.9315068493150684 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #63\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.3974 | speaker = 0.2785 | gender = 0.1691 | total = 0.3198 |\n",
      "# Train accuracies | emotion = 0.6873058051091709 | speaker = 0.7421654224134295 | gender = 0.8656628284617656 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0069 | speaker = 0.0027 | gender = 0.0020 | total = 0.0052 |\n",
      "# Validation accuracies | emotion = 0.7328767123287672 | speaker = 0.8253424657534246 | gender = 0.934931506849315 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #64\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.3803 | speaker = 0.2533 | gender = 0.1541 | total = 0.3009 |\n",
      "# Train accuracies | emotion = 0.6897143489813995 | speaker = 0.7452253100088574 | gender = 0.8673604960141719 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0073 | speaker = 0.0116 | gender = 0.0023 | total = 0.0092 |\n",
      "# Validation accuracies | emotion = 0.7294520547945206 | speaker = 0.8356164383561644 | gender = 0.9246575342465754 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #65\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.3724 | speaker = 0.2611 | gender = 0.1503 | total = 0.3029 |\n",
      "# Train accuracies | emotion = 0.6921441711521428 | speaker = 0.7482864345574709 | gender = 0.8689786741159637 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0022 | speaker = 0.0223 | gender = 0.0040 | total = 0.0183 |\n",
      "# Validation accuracies | emotion = 0.7671232876712328 | speaker = 0.815068493150685 | gender = 0.928082191780822 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #66\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.3600 | speaker = 0.2604 | gender = 0.1486 | total = 0.2952 |\n",
      "# Train accuracies | emotion = 0.694580884129157 | speaker = 0.7511876962718416 | gender = 0.8704538744397026 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0113 | speaker = 0.0129 | gender = 0.0017 | total = 0.0115 |\n",
      "# Validation accuracies | emotion = 0.7602739726027398 | speaker = 0.8527397260273972 | gender = 0.9212328767123288 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #67\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.3709 | speaker = 0.2387 | gender = 0.1459 | total = 0.2964 |\n",
      "# Train accuracies | emotion = 0.6968258794600954 | speaker = 0.7541213331041867 | gender = 0.8718982589268008 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0075 | speaker = 0.0023 | gender = 0.0072 | total = 0.0068 |\n",
      "# Validation accuracies | emotion = 0.613013698630137 | speaker = 0.773972602739726 | gender = 0.863013698630137 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #68\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.3498 | speaker = 0.2467 | gender = 0.1452 | total = 0.2842 |\n",
      "# Train accuracies | emotion = 0.6991872036680039 | speaker = 0.756955660918043 | gender = 0.8732871359349763 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0047 | speaker = 0.0012 | gender = 0.0006 | total = 0.0037 |\n",
      "# Validation accuracies | emotion = 0.7568493150684932 | speaker = 0.8458904109589042 | gender = 0.9417808219178082 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #69\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.3995 | speaker = 0.2481 | gender = 0.1598 | total = 0.3228 |\n",
      "# Train accuracies | emotion = 0.7009922850797807 | speaker = 0.7596308134683766 | gender = 0.8745715716101206 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0105 | speaker = 0.0051 | gender = 0.0006 | total = 0.0085 |\n",
      "# Validation accuracies | emotion = 0.6472602739726028 | speaker = 0.8013698630136986 | gender = 0.928082191780822 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #70\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.3380 | speaker = 0.2360 | gender = 0.1486 | total = 0.2747 |\n",
      "# Train accuracies | emotion = 0.7031000885739592 | speaker = 0.7623307604707074 | gender = 0.8759205365051247 |\n",
      "# Validation process on validation set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Validation losses | emotion = 0.0031 | speaker = 0.0034 | gender = 0.0008 | total = 0.0030 |\n",
      "# Validation accuracies | emotion = 0.7294520547945206 | speaker = 0.839041095890411 | gender = 0.952054794520548 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #71\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.3368 | speaker = 0.2325 | gender = 0.1335 | total = 0.2757 |\n",
      "# Train accuracies | emotion = 0.7051485173218229 | speaker = 0.7649920782444891 | gender = 0.8773437792387629 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0105 | speaker = 0.0072 | gender = 0.0007 | total = 0.0089 |\n",
      "# Validation accuracies | emotion = 0.7465753424657534 | speaker = 0.8527397260273972 | gender = 0.9417808219178082 |\n",
      "# Saving checkpoint...\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #72\n",
      "# Time passed: 8 s\n",
      "# Epoch losses | emotion = 0.3075 | speaker = 0.2190 | gender = 0.1437 | total = 0.2539 |\n",
      "# Train accuracies | emotion = 0.7073983859856313 | speaker = 0.7676409802184825 | gender = 0.8786536758193091 |\n",
      "# Validation process on validation set\n",
      "# Validation losses | emotion = 0.0005 | speaker = 0.0013 | gender = 0.0043 | total = 0.0036 |\n",
      "# Validation accuracies | emotion = 0.7705479452054794 | speaker = 0.839041095890411 | gender = 0.928082191780822 |\n",
      "# Saving checkpoint...\n",
      "## Saving best model_alex\n",
      "# Done and done!\n",
      "======================================================================\n",
      "Epoch #73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aggr/anaconda3/envs/ryabinov/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/aggr/anaconda3/envs/ryabinov/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/aggr/anaconda3/envs/ryabinov/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/aggr/anaconda3/envs/ryabinov/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/home/aggr/anaconda3/envs/ryabinov/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/aggr/anaconda3/envs/ryabinov/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/aggr/anaconda3/envs/ryabinov/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/aggr/anaconda3/envs/ryabinov/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "  File \"/home/aggr/anaconda3/envs/ryabinov/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/aggr/anaconda3/envs/ryabinov/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/aggr/anaconda3/envs/ryabinov/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/aggr/anaconda3/envs/ryabinov/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0ecf462db4d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ryabinov_project_files/SER_neuro/models/multitask_training_session.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'======================================================================'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'=============TRAINING SESSION STARTED AT EPOCH {}====================='\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             self.model, self.results_dict = self.training_loop(trainloader=self.trainloader,\n\u001b[0m\u001b[1;32m    118\u001b[0m                                                                \u001b[0mtestloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                                                                results_dict=self.results_dict)\n",
      "\u001b[0;32m~/ryabinov_project_files/SER_neuro/models/multitask_training_session.py\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(self, trainloader, testloader, results_dict)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m                 \u001b[0mepoch_loss_emotion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_emotion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m                 \u001b[0mepoch_loss_speaker\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_speaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0mepoch_loss_gender\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_gender\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ts.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
