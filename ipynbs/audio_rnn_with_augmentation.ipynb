{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "from scipy.io import wavfile\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tarfile\n",
    "\n",
    "import librosa\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths_to_wavs(path_to_dataset):\n",
    "    file_paths_list = []\n",
    "\n",
    "    for root, dirs, files in os.walk(path_to_dataset):\n",
    "        if len(files) != 0:\n",
    "            file_paths_list += [os.path.join(root, f) for f in files if f.endswith('.wav')]\n",
    "\n",
    "    return file_paths_list\n",
    "\n",
    "def get_paths_to_npys(path_to_dataset):\n",
    "    # get a list with all absolute paths to each file\n",
    "    file_paths_list = []\n",
    "\n",
    "    for root, dirs, files in os.walk(path_to_dataset):\n",
    "        if len(files) != 0:\n",
    "            file_paths_list += [os.path.join(root, f) for f in files if f.endswith('.npy')]\n",
    "            #file_paths_list += [os.path.join(root, f) for f in files if os.path.isdir(os.path.join(root, f))]\n",
    "\n",
    "    return file_paths_list\n",
    "\n",
    "class numpy_ravdess_dataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Due to librosa reads wav-files very slow it is more preferable to read the\n",
    "    numpy representations of the original wavs\n",
    "    '''\n",
    "\n",
    "    emotions_dict = {\n",
    "        0: 'neutral',\n",
    "        1: 'calm',\n",
    "        2: 'happy',\n",
    "        3: 'sad',\n",
    "        4: 'angry',\n",
    "        5: 'fearful',\n",
    "        6: 'disgust',\n",
    "        7: 'surprised'\n",
    "        }\n",
    "\n",
    "    def __init__(self, paths_to_wavs_list, spectrogram_shape, mode):\n",
    "        super(numpy_ravdess_dataset, self).__init__()\n",
    "\n",
    "        self.paths_to_wavs_list = paths_to_wavs_list\n",
    "\n",
    "        self.mfcc_rows = spectrogram_shape[0]\n",
    "        self.mfcc_cols = spectrogram_shape[1]\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths_to_wavs_list)\n",
    "    '''\n",
    "    def read_audio(self, path_to_wav):\n",
    "        return np.load(path_to_wav, allow_pickle=True)\n",
    "    '''\n",
    "    def read_audio(self, path_to_wav):\n",
    "        sr, wav = wavfile.read(path_to_wav)\n",
    "        wav = (wav / 32768).astype(np.float32)\n",
    "        return wav, sr\n",
    "\n",
    "    def get_class_label(self, path_to_file):\n",
    "        # Parse the filename, which has the following pattern:\n",
    "        # modality-vocal_channel-emotion-intensity-statement-repetition-actor.wav\n",
    "        # e.g., '02-01-06-01-02-01-12.wav'\n",
    "        file_name = os.path.split(path_to_file)[1]\n",
    "        file_name = file_name[:-4]\n",
    "        class_label = int(file_name.split('-')[2]) - 1 # 2 is a number of emotion code\n",
    "        return class_label\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path_to_wav = self.paths_to_wavs_list[idx]\n",
    "        # debug\n",
    "        #print(path_to_wav)\n",
    "\n",
    "        # read the wav file\n",
    "        wav, sr = self.read_audio(path_to_wav)       \n",
    "\n",
    "        # augmentation\n",
    "        \n",
    "        if self.mode == 'TRAIN':\n",
    "            # add noise\n",
    "            if np.random.randint(0, 2) == 1:\n",
    "                sigma = np.random.uniform(0.0009, 0.0051)\n",
    "                noise = sigma * np.random.randn(len(wav))\n",
    "                wav += noise\n",
    "            # stretch wav\n",
    "            if np.random.randint(0, 2) == 1:\n",
    "                factor = np.random.uniform(0.5, 1.2)\n",
    "                wav = librosa.effects.time_stretch(wav, 2)\n",
    "            # change pitch\n",
    "            if np.random.randint(0, 2) == 1:\n",
    "                factor = np.random.uniform(-1.5, 1.1)\n",
    "                wav = librosa.effects.pitch_shift(wav, sr=sr, n_steps=factor)\n",
    "    \n",
    "        # get mfcc coefficients\n",
    "        #mfccs = librosa.feature.mfcc(wav, sr=sr, n_mfcc=self.mfcc_rows, n_mels=self.mfcc_rows).astype(np.float32)\n",
    "        '''\n",
    "        if self.mode == 'TRAIN':\n",
    "            # augment by choosing n_fft\n",
    "            n_fft_list = [i for i in range(1024, 2049, 32)]\n",
    "            idx = np.random.randint(len(n_fft_list))\n",
    "            n_fft = n_fft_list[idx]\n",
    "        else:\n",
    "            n_fft = 2048\n",
    "\n",
    "        '''\n",
    "        '''\n",
    "        n_fft = 2048\n",
    "        mfccs = librosa.feature.melspectrogram(wav, sr=sr, n_mels=self.mfcc_rows, n_fft=n_fft, hop_length=128).astype(np.float32)\n",
    "\n",
    "        '''\n",
    "        mfccs = librosa.core.stft(wav, n_fft=self.mfcc_rows*2)#.astype(np.float32)\n",
    "        mfccs = np.abs(mfccs)#**2\n",
    "        mfccs = np.log(mfccs + 0.1)\n",
    "        mfccs = mfccs[:-1]\n",
    "        # debug\n",
    "        #print(mfccs.shape)\n",
    "        #mfccs = (mfccs - mfccs.mean())/np.std(mfccs)\n",
    "\n",
    "        actual_mfcc_cols = mfccs.shape[1]\n",
    "\n",
    "        # prmitive time-shifting augmentation\n",
    "        target_real_diff = actual_mfcc_cols - self.mfcc_cols\n",
    "        # debug\n",
    "        #print(actual_mfcc_cols)\n",
    "        if target_real_diff > 0:\n",
    "            \n",
    "            if self.mode == 'TRAIN':\n",
    "                beginning_col = np.random.randint(target_real_diff)\n",
    "            else:\n",
    "                beginning_col = actual_mfcc_cols//2 - self.mfcc_cols//2\n",
    "\n",
    "            mfccs = mfccs[:, beginning_col:beginning_col + self.mfcc_cols]\n",
    "            #mfccs = mfccs[:, beginning_col:beginning_col + self.mfcc_cols]\n",
    "\n",
    "        elif target_real_diff < 0:\n",
    "            zeros = np.zeros((self.mfcc_rows, self.mfcc_cols), dtype=np.float32)\n",
    "            # debug\n",
    "            #print(zeros.shape)\n",
    "            \n",
    "            if self.mode == 'TRAIN':\n",
    "                beginning_col = np.random.randint(self.mfcc_cols-actual_mfcc_cols)\n",
    "            else:\n",
    "            \n",
    "                beginning_col = self.mfcc_cols//2 - actual_mfcc_cols//2\n",
    "            zeros[..., beginning_col:beginning_col+actual_mfcc_cols] = mfccs\n",
    "            #zeros[..., beginning_col:beginning_col+actual_mfcc_cols] = mfccs\n",
    "            mfccs = zeros\n",
    "            #mfccs = np.pad(mfccs, ((0, 0), (0, np.abs(target_real_diff))), constant_values=(0), mode='constant')\n",
    "\n",
    "        # make the data compatible to pytorch 1-channel CNNs format\n",
    "        # !!!!!!!!!!!!!!!!!!!!!\n",
    "        #mfccs = np.expand_dims(mfccs, axis=0)\n",
    "\n",
    "        # Parse the filename, which has the following pattern:\n",
    "        # modality-vocal_channel-emotion-intensity-statement-repetition-actor.wav\n",
    "        # e.g., '02-01-06-01-02-01-12.wav'\n",
    "        #file_name = os.path.split(path_to_wav)[1]\n",
    "        #file_name = file_name[:-4]\n",
    "        #class_label = int(file_name.split('-')[2]) - 1 # 2 is a number of emotion code\n",
    "        #class_label = np.array(class_label)\n",
    "        class_label = self.get_class_label(path_to_wav)\n",
    "        # !!!!!!!!!\n",
    "        # transpose to reorder index by the time windows of the spectrograms\n",
    "        return torch.from_numpy(mfccs).transpose(1, 0), class_label#, path_to_wav\n",
    "\n",
    "class numpy_crema_dataset(numpy_ravdess_dataset):\n",
    "    emotions_dict = {\n",
    "        'ANG': 0,\n",
    "        'DIS': 1,\n",
    "        'FEA': 2,\n",
    "        'SAD': 3,\n",
    "        'HAP': 4,\n",
    "        'NEU': 5\n",
    "    }\n",
    "\n",
    "    label2str = {\n",
    "        0: 'ANG',\n",
    "        1: 'DIS',\n",
    "        2: 'FEA',\n",
    "        3: 'SAD',\n",
    "        4: 'HAP',\n",
    "        5: 'NEU'\n",
    "    }\n",
    "    \n",
    "    def get_class_label(self, path_to_file):\n",
    "        file_name = os.path.split(path_to_file)[1]\n",
    "        file_name = file_name[:-4]\n",
    "        emotion_name = file_name.split('_')[2] # 2 is a number of emotion code\n",
    "        return self.emotions_dict[emotion_name]\n",
    "\n",
    "class numpy_iemocap_dataset(numpy_ravdess_dataset):\n",
    "    '''\n",
    "    emotions_dict = {\n",
    "        'exc': 0,\n",
    "        'sad': 1,\n",
    "        'fru': 2,\n",
    "        'hap': 3,\n",
    "        'neu': 4,\n",
    "        'sur': 5,\n",
    "        'ang': 6,\n",
    "        'fea': 7,\n",
    "        'dis': 8,\n",
    "        #'oth': 9\n",
    "    }\n",
    "    '''\n",
    "    emotions_dict = {\n",
    "        'exc': 0,\n",
    "        'sad': 1,\n",
    "        'fru': 2,\n",
    "        'hap': 3,\n",
    "        'neu': 4,\n",
    "        'ang': 5,\n",
    "    }\n",
    "\n",
    "    def get_class_label(self, path_to_file):\n",
    "        file_name = os.path.split(path_to_file)[1]\n",
    "        file_name = file_name[:-4]\n",
    "        emotion_name = file_name.split('_')[-1] # the last is a position of emotion code\n",
    "        return self.emotions_dict[emotion_name]\n",
    "\n",
    "class crema_gender_dataset(numpy_ravdess_dataset):\n",
    "    emotions_dict = {\n",
    "        'ANG_Male': 0,\n",
    "        'DIS_Male': 1,\n",
    "        'FEA_Male': 2,\n",
    "        'SAD_Male': 3,\n",
    "        'HAP_Male': 4,\n",
    "        'NEU_Male': 5,\n",
    "        'ANG_Female': 6,\n",
    "        'DIS_Female': 7,\n",
    "        'FEA_Female': 8,\n",
    "        'SAD_Female': 9,\n",
    "        'HAP_Female': 10,\n",
    "        'NEU_Female': 11\n",
    "    }\n",
    "\n",
    "    label2str = {\n",
    "        0: 'ANG',\n",
    "        1: 'DIS',\n",
    "        2: 'FEA',\n",
    "        3: 'SAD',\n",
    "        4: 'HAP',\n",
    "        5: 'NEU'\n",
    "    }\n",
    "    def __init__(self, paths_to_wavs_list, spectrogram_shape, mode, gender_df):\n",
    "        super().__init__(paths_to_wavs_list, spectrogram_shape, mode)\n",
    "        self.gender_df = gender_df\n",
    "    \n",
    "    def get_class_label(self, path_to_file):\n",
    "        \n",
    "        file_name = os.path.split(path_to_file)[1]\n",
    "        file_name = file_name[:-4]\n",
    "        name_list = file_name.split('_') # 2 is a number of emotion code\n",
    "        emotion_name = name_list[2]\n",
    "        actor_id = int(name_list[0])\n",
    "\n",
    "        gender = self.gender_df[self.gender_df['ActorID'] == actor_id]['Sex'].values[0]\n",
    "\n",
    "        return self.emotions_dict['{}_{}'.format(emotion_name, gender)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class audio_rnn(nn.Module):\n",
    "    def __init__(self, rnn, layer_num, input_dim, hidden_dim, class_num, device, bidirectional=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_num = layer_num\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = device\n",
    "        \n",
    "        if bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "       \n",
    "        self.rnn = rnn(input_size=input_dim,\n",
    "                           hidden_size=hidden_dim,\n",
    "                           num_layers=layer_num,\n",
    "                           batch_first=False,\n",
    "                           bidirectional=bidirectional,\n",
    "                           dropout=0.5)\n",
    "        \n",
    "        # Bidirectional nns has twice large inputs size on Linear layer\n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(in_features=hidden_dim * self.num_directions, out_features=class_num)\n",
    "        else:\n",
    "            self.fc = nn.Linear(in_features=hidden_dim, out_features=class_num)        \n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_directions * self.layer_num, batch_size, self.hidden_dim).to(self.device)\n",
    "        cell = torch.zeros(self.num_directions * self.layer_num, batch_size, self.hidden_dim).to(self.device)\n",
    "        return (hidden, cell)\n",
    "\n",
    "    def compute_output(self, output):\n",
    "        if self.num_directions == 2:\n",
    "            # Если рекуррентная сеть является двунаправленной, то на выходной классификатор надо\n",
    "            # подавать выход последнего шага рекуррентной сети прямого прохода - output[-1,:,size//2:],\n",
    "            # а также выход последнего шага рекуррентной сети обратного прохода - output[1,:,:size//2]\n",
    "            size = output.size(2)\n",
    "            result = self.fc(torch.cat([output[1,:,:size//2], output[-1,:,size//2:]], dim=1))\n",
    "        else:\n",
    "            result = self.fc(output[-1])\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        batch_size = batch.shape[0]\n",
    "\n",
    "        batch = batch.transpose(1, 0)\n",
    "\n",
    "            \n",
    "\n",
    "        h0, c0 = self.init_hidden(batch_size=batch_size)\n",
    "\n",
    "        #print('h0 shape =', h0.shape)\n",
    "        \n",
    "        #return h0, c0\n",
    "\n",
    "        # GRU don't has memory cell\n",
    "        # We need to initialize only hidden states h\n",
    "        if isinstance(self.rnn, nn.GRU):\n",
    "            output, hn = self.rnn(batch, h0)\n",
    "        elif isinstance(self.rnn, nn.LSTM):\n",
    "            output, (hn, cn) = self.rnn(batch, (h0, c0))\n",
    "        else:\n",
    "            raise ValueError('self.rnn shoulb be torch.nn.LSTM or torch.nn.GRU')\n",
    "\n",
    "        #return output\n",
    "\n",
    "        result = self.compute_output(output)\n",
    "\n",
    "        return result\n",
    "\n",
    "class audio_rnn_avg(audio_rnn_last):\n",
    "    def compute_output(self, output):\n",
    "        output = output.mean(dim=0)\n",
    "        result = self.fc(output)\n",
    "\n",
    "        return result\n",
    "\n",
    "class audio_rnn_attention(audio_rnn):\n",
    "    def __init__(self, rnn, layer_num, input_dim, hidden_dim, class_num, device, bidirectional=False):\n",
    "        super().__init__(rnn, layer_num, input_dim, hidden_dim, class_num, device, bidirectional=False)\n",
    "        self.attention = nn.Linear(in_features=hidden_dim, out_features=1)\n",
    "    \n",
    "    def compute_output(self, x):\n",
    "        x = x.transpose(1, 0).contiguous()\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        hidden_dim = x.size(2)\n",
    "\n",
    "        # compute alpha coefficients of attention module\n",
    "        alphas = F.softmax(self.atention(x), dim=1)\n",
    "\n",
    "        # AAAAAAAAAAAAAAAAAAAAAAAAA\n",
    "        # multiply the outputs by the alphas\n",
    "        # outputs have size [batch_size, sequence_len, hidden_dim]\n",
    "        # reshape them to [batch_size * sequence_len, hidden_dim, 1]\n",
    "        # and multiply the by alphas of shape [batch_size * sequence_len, 1, 1]\n",
    "        intermediate = torch.bmm(\n",
    "            x.view(batch_size*seq_len, hidden_dim, 1),\n",
    "            alphas.view(batch_size*seq_len, 1, 1)\n",
    "            )\n",
    "        intermediate = intermediate.view(batch_size, seq_len, -1).sum(dim=1)\n",
    "\n",
    "        output = self.fc(intermediate)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "7442\n"
    }
   ],
   "source": [
    "\n",
    "# CREMA-D\n",
    "target_path = '/media/mikhail/files/datasets/emotion_recognition/CREMA-D/AudioWAV'\n",
    "# IEMOCAP\n",
    "#target_path = '/media/mikhail/files/datasets/emotion_recognition/IEMOCAP/IEMOCAP_full_release/audios'\n",
    " \n",
    "npys_list = get_paths_to_wavs(target_path)\n",
    "\n",
    "# shuffle the dataset to for the learning process stability\n",
    "random.seed(10)\n",
    "random.shuffle(npys_list)\n",
    "\n",
    "dataset_size = len(npys_list)\n",
    "\n",
    "train_size = int(0.8 * dataset_size)\n",
    "\n",
    "print(dataset_size)\n",
    "\n",
    "train_dataset = numpy_crema_dataset(npys_list[:train_size], (128, 256), mode='TRAIN')\n",
    "#train_dataset = numpy_iemocap_dataset(npys_list[:train_size], (256, 256), mode='TRAIN')\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=256, shuffle=True, num_workers=4)\n",
    "\n",
    "# set up test dataset and test dataloader\n",
    "test_dataset = numpy_crema_dataset(npys_list[train_size:], (128, 256), mode='TEST')\n",
    "#test_dataset = numpy_iemocap_dataset(npys_list[train_size:], (256, 256), mode='TEST')\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=256, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up devices\n",
    "cuda = torch.device('cuda:0')\n",
    "cpu = torch.device('cpu')\n",
    "\n",
    "device = cuda\n",
    "\n",
    "rnn = nn.GRU #Avaialable models: nn.LSTM, nn.GRU, bidirectional = True/False\n",
    "bidirectional=False\n",
    "layer_num = 1\n",
    "\n",
    "model = audio_rnn(layer_num=layer_num, rnn=rnn, input_dim=128, hidden_dim=64, class_num=len(train_dataset.emotions_dict), device=device, bidirectional=bidirectional)\n",
    "\n",
    "#summary(model, input_size=(256, 1, 128), batch_size=32, device='cpu')\n",
    "\n",
    "device = cuda\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# define an optimization algorithm and bind it with the NN parameters\n",
    "optimizer = torch.optim.Adam(params=model.parameters())\n",
    "\n",
    "starting_epoch = 0\n",
    "ending_epoch = 1000\n",
    "epoch_step = 1\n",
    "\n",
    "basic_name = '{}_log_mel_spec_256_emotion_LSTM'.format('CREMA')\n",
    "\n",
    "\n",
    "\n",
    "path_to_weights = basic_name\n",
    "path_to_pkl = basic_name\n",
    "\n",
    "if not os.path.isdir(path_to_weights):\n",
    "    os.mkdir(path_to_weights)\n",
    "if not os.path.isdir(path_to_pkl):\n",
    "    os.mkdir(path_to_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": " validation on 954 epoch\n#############################################\n\tLoss = 1.3418\tValidation acc = 0.510\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #955\nEpoch time = 60.712 s\nLoss = 1.000806\tTraining acc = 0.618512\n----------------------------------------\n#############################################\n#\tStart validation on 955 epoch\n#############################################\n\tLoss = 1.3811\tValidation acc = 0.504\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #956\nEpoch time = 61.079 s\nLoss = 1.031189\tTraining acc = 0.606585\n----------------------------------------\n#############################################\n#\tStart validation on 956 epoch\n#############################################\n\tLoss = 1.3664\tValidation acc = 0.498\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #957\nEpoch time = 60.125 s\nLoss = 0.998141\tTraining acc = 0.614144\n----------------------------------------\n#############################################\n#\tStart validation on 957 epoch\n#############################################\n\tLoss = 1.3446\tValidation acc = 0.501\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #958\nEpoch time = 59.913 s\nLoss = 0.976225\tTraining acc = 0.627415\n----------------------------------------\n#############################################\n#\tStart validation on 958 epoch\n#############################################\n\tLoss = 1.3073\tValidation acc = 0.504\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #959\nEpoch time = 62.073 s\nLoss = 0.993101\tTraining acc = 0.619184\n----------------------------------------\n#############################################\n#\tStart validation on 959 epoch\n#############################################\n\tLoss = 1.3193\tValidation acc = 0.512\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #960\nEpoch time = 57.491 s\nLoss = 0.991008\tTraining acc = 0.618176\n----------------------------------------\n#############################################\n#\tStart validation on 960 epoch\n#############################################\n\tLoss = 1.3253\tValidation acc = 0.509\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #961\nEpoch time = 60.312 s\nLoss = 0.995546\tTraining acc = 0.619016\n----------------------------------------\n#############################################\n#\tStart validation on 961 epoch\n#############################################\n\tLoss = 1.3105\tValidation acc = 0.505\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #962\nEpoch time = 61.102 s\nLoss = 0.988246\tTraining acc = 0.614816\n----------------------------------------\n#############################################\n#\tStart validation on 962 epoch\n#############################################\n\tLoss = 1.3204\tValidation acc = 0.508\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #963\nEpoch time = 60.559 s\nLoss = 1.003986\tTraining acc = 0.607761\n----------------------------------------\n#############################################\n#\tStart validation on 963 epoch\n#############################################\n\tLoss = 1.3817\tValidation acc = 0.493\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #964\nEpoch time = 60.612 s\nLoss = 0.979336\tTraining acc = 0.621031\n----------------------------------------\n#############################################\n#\tStart validation on 964 epoch\n#############################################\n\tLoss = 1.3753\tValidation acc = 0.504\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #965\nEpoch time = 62.053 s\nLoss = 1.014878\tTraining acc = 0.602049\n----------------------------------------\n#############################################\n#\tStart validation on 965 epoch\n#############################################\n\tLoss = 1.3394\tValidation acc = 0.501\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #966\nEpoch time = 59.892 s\nLoss = 1.016960\tTraining acc = 0.611456\n----------------------------------------\n#############################################\n#\tStart validation on 966 epoch\n#############################################\n\tLoss = 1.4142\tValidation acc = 0.500\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #967\nEpoch time = 59.646 s\nLoss = 0.998916\tTraining acc = 0.609777\n----------------------------------------\n#############################################\n#\tStart validation on 967 epoch\n#############################################\n\tLoss = 1.3421\tValidation acc = 0.504\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #968\nEpoch time = 61.052 s\nLoss = 0.978187\tTraining acc = 0.623383\n----------------------------------------\n#############################################\n#\tStart validation on 968 epoch\n#############################################\n\tLoss = 1.3415\tValidation acc = 0.508\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #969\nEpoch time = 62.104 s\nLoss = 0.983184\tTraining acc = 0.617336\n----------------------------------------\n#############################################\n#\tStart validation on 969 epoch\n#############################################\n\tLoss = 1.3529\tValidation acc = 0.504\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #970\nEpoch time = 59.139 s\nLoss = 0.984685\tTraining acc = 0.620527\n----------------------------------------\n#############################################\n#\tStart validation on 970 epoch\n#############################################\n\tLoss = 1.3126\tValidation acc = 0.511\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #971\nEpoch time = 60.601 s\nLoss = 0.979963\tTraining acc = 0.625903\n----------------------------------------\n#############################################\n#\tStart validation on 971 epoch\n#############################################\n\tLoss = 1.3792\tValidation acc = 0.487\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #972\nEpoch time = 60.963 s\nLoss = 1.005336\tTraining acc = 0.610616\n----------------------------------------\n#############################################\n#\tStart validation on 972 epoch\n#############################################\n\tLoss = 1.3422\tValidation acc = 0.494\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #973\nEpoch time = 60.566 s\nLoss = 0.997962\tTraining acc = 0.614144\n----------------------------------------\n#############################################\n#\tStart validation on 973 epoch\n#############################################\n\tLoss = 1.3948\tValidation acc = 0.490\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #974\nEpoch time = 59.908 s\nLoss = 0.997820\tTraining acc = 0.618344\n----------------------------------------\n#############################################\n#\tStart validation on 974 epoch\n#############################################\n\tLoss = 1.3269\tValidation acc = 0.512\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #975\nEpoch time = 61.774 s\nLoss = 0.985343\tTraining acc = 0.619688\n----------------------------------------\n#############################################\n#\tStart validation on 975 epoch\n#############################################\n\tLoss = 1.3330\tValidation acc = 0.509\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #976\nEpoch time = 57.753 s\nLoss = 1.006403\tTraining acc = 0.612464\n----------------------------------------\n#############################################\n#\tStart validation on 976 epoch\n#############################################\n\tLoss = 1.3614\tValidation acc = 0.501\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #977\nEpoch time = 59.693 s\nLoss = 1.003110\tTraining acc = 0.614648\n----------------------------------------\n#############################################\n#\tStart validation on 977 epoch\n#############################################\n\tLoss = 1.3378\tValidation acc = 0.507\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #978\nEpoch time = 61.383 s\nLoss = 0.991210\tTraining acc = 0.614816\n----------------------------------------\n#############################################\n#\tStart validation on 978 epoch\n#############################################\n\tLoss = 1.3712\tValidation acc = 0.495\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #979\nEpoch time = 59.646 s\nLoss = 1.003863\tTraining acc = 0.609777\n----------------------------------------\n#############################################\n#\tStart validation on 979 epoch\n#############################################\n\tLoss = 1.3440\tValidation acc = 0.498\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #980\nEpoch time = 59.570 s\nLoss = 0.982258\tTraining acc = 0.619352\n----------------------------------------\n#############################################\n#\tStart validation on 980 epoch\n#############################################\n\tLoss = 1.3634\tValidation acc = 0.500\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #981\nEpoch time = 61.135 s\nLoss = 0.990725\tTraining acc = 0.622543\n----------------------------------------\n#############################################\n#\tStart validation on 981 epoch\n#############################################\n\tLoss = 1.3814\tValidation acc = 0.506\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #982\nEpoch time = 60.664 s\nLoss = 0.984334\tTraining acc = 0.615152\n----------------------------------------\n#############################################\n#\tStart validation on 982 epoch\n#############################################\n\tLoss = 1.3497\tValidation acc = 0.498\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #983\nEpoch time = 59.428 s\nLoss = 0.980094\tTraining acc = 0.627079\n----------------------------------------\n#############################################\n#\tStart validation on 983 epoch\n#############################################\n\tLoss = 1.4162\tValidation acc = 0.497\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #984\nEpoch time = 61.657 s\nLoss = 1.003768\tTraining acc = 0.612800\n----------------------------------------\n#############################################\n#\tStart validation on 984 epoch\n#############################################\n\tLoss = 1.3659\tValidation acc = 0.506\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #985\nEpoch time = 60.981 s\nLoss = 0.984679\tTraining acc = 0.618344\n----------------------------------------\n#############################################\n#\tStart validation on 985 epoch\n#############################################\n\tLoss = 1.3430\tValidation acc = 0.504\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #986\nEpoch time = 60.054 s\nLoss = 0.990606\tTraining acc = 0.618848\n----------------------------------------\n#############################################\n#\tStart validation on 986 epoch\n#############################################\n\tLoss = 1.3263\tValidation acc = 0.512\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #987\nEpoch time = 60.463 s\nLoss = 0.978990\tTraining acc = 0.628255\n----------------------------------------\n#############################################\n#\tStart validation on 987 epoch\n#############################################\n\tLoss = 1.3391\tValidation acc = 0.514\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #988\nEpoch time = 58.145 s\nLoss = 0.976764\tTraining acc = 0.619856\n----------------------------------------\n#############################################\n#\tStart validation on 988 epoch\n#############################################\n\tLoss = 1.3267\tValidation acc = 0.517\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #989\nEpoch time = 59.253 s\nLoss = 1.004943\tTraining acc = 0.614816\n----------------------------------------\n#############################################\n#\tStart validation on 989 epoch\n#############################################\n\tLoss = 1.3507\tValidation acc = 0.506\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #990\nEpoch time = 62.551 s\nLoss = 0.996534\tTraining acc = 0.618176\n----------------------------------------\n#############################################\n#\tStart validation on 990 epoch\n#############################################\n\tLoss = 1.3767\tValidation acc = 0.490\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #991\nEpoch time = 60.053 s\nLoss = 1.018303\tTraining acc = 0.608433\n----------------------------------------\n#############################################\n#\tStart validation on 991 epoch\n#############################################\n\tLoss = 1.3347\tValidation acc = 0.515\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #992\nEpoch time = 56.943 s\nLoss = 1.012627\tTraining acc = 0.608433\n----------------------------------------\n#############################################\n#\tStart validation on 992 epoch\n#############################################\n\tLoss = 1.3191\tValidation acc = 0.496\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #993\nEpoch time = 61.859 s\nLoss = 0.995842\tTraining acc = 0.612632\n----------------------------------------\n#############################################\n#\tStart validation on 993 epoch\n#############################################\n\tLoss = 1.3815\tValidation acc = 0.496\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #994\nEpoch time = 60.258 s\nLoss = 1.010582\tTraining acc = 0.608769\n----------------------------------------\n#############################################\n#\tStart validation on 994 epoch\n#############################################\n\tLoss = 1.3299\tValidation acc = 0.511\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #995\nEpoch time = 57.619 s\nLoss = 0.988297\tTraining acc = 0.618176\n----------------------------------------\n#############################################\n#\tStart validation on 995 epoch\n#############################################\n\tLoss = 1.3261\tValidation acc = 0.522\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #996\nEpoch time = 60.679 s\nLoss = 0.984699\tTraining acc = 0.620192\n----------------------------------------\n#############################################\n#\tStart validation on 996 epoch\n#############################################\n\tLoss = 1.3853\tValidation acc = 0.508\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #997\nEpoch time = 61.718 s\nLoss = 1.020469\tTraining acc = 0.599194\n----------------------------------------\n#############################################\n#\tStart validation on 997 epoch\n#############################################\n\tLoss = 1.3440\tValidation acc = 0.516\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #998\nEpoch time = 58.916 s\nLoss = 0.991443\tTraining acc = 0.622711\n----------------------------------------\n#############################################\n#\tStart validation on 998 epoch\n#############################################\n\tLoss = 1.3387\tValidation acc = 0.510\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #999\nEpoch time = 60.202 s\nLoss = 0.995727\tTraining acc = 0.612800\n----------------------------------------\n#############################################\n#\tStart validation on 999 epoch\n#############################################\n\tLoss = 1.3141\tValidation acc = 0.510\n---------------------------------------------\n"
    }
   ],
   "source": [
    "start_epoch = 500\n",
    "epochs = 1000\n",
    "epoch_step = 1\n",
    "\n",
    "print('Start learning')\n",
    "\n",
    "\n",
    "best_acc = 0.0\n",
    "\n",
    "train_dataset_size = len(train_dataloader.dataset)  \n",
    "test_dataset_size = len(test_dataloader.dataset)  \n",
    "\n",
    "\n",
    "if os.path.exists(os.path.join(path_to_pkl, basic_name + '_train_loss.pkl')):\n",
    "    # Update existing classifier\n",
    "    with open(os.path.join(path_to_pkl, basic_name + '_train_loss.pkl'), \"rb\") as f:\n",
    "        train_loss_list = pickle.load(f)\n",
    "else:\n",
    "  train_loss_list = []\n",
    "\n",
    "if os.path.exists(os.path.join(path_to_pkl, basic_name + '_train_acc.pkl')):\n",
    "    # Update existing classifier\n",
    "    with open(os.path.join(path_to_pkl, basic_name + '_train_acc.pkl'), \"rb\") as f:\n",
    "        train_acc_list = pickle.load(f)\n",
    "else:\n",
    "  train_acc_list = []\n",
    "\n",
    "if os.path.exists(os.path.join(path_to_pkl, basic_name + '_val_loss.pkl')):\n",
    "    # Update existing classifier\n",
    "    with open(os.path.join(path_to_pkl, basic_name + '_val_loss.pkl'), \"rb\") as f:\n",
    "        val_loss_list = pickle.load(f)\n",
    "else:\n",
    "  val_loss_list = []\n",
    "\n",
    "if os.path.exists(os.path.join(path_to_pkl, basic_name + '_val_acc.pkl')):\n",
    "    # Update existing classifier\n",
    "    with open(os.path.join(path_to_pkl, basic_name + '_val_acc.pkl'), \"rb\") as f:\n",
    "        val_acc_list = pickle.load(f)\n",
    "else:\n",
    "    val_acc_list = []\n",
    "\n",
    "\n",
    "t = 0.0\n",
    "\n",
    "for epoch_idx in range(start_epoch, epochs, epoch_step):\n",
    "\n",
    "    print('#############################################')\n",
    "    print('#\\tStart training process')\n",
    "    print('#############################################\\n\\n')\n",
    "\n",
    "    # iterate over epochs\n",
    "    for epoch in range(epoch_step):\n",
    "        print('Epoch #{}'.format(epoch_idx + epoch))\n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "        # define losses and correct valuse number for each epoch\n",
    "        epoch_train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # iterate over batches\n",
    "        for data, labels in train_dataloader:\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(data)\n",
    "            loss = criterion(pred, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item() * data.size(0)\n",
    "            total += labels.size(0)\n",
    "            _, pred_labels = torch.max(pred.data, 1)\n",
    "\n",
    "            correct += (pred_labels == labels).sum().item()\n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        print('Epoch time = {:.3f} s'.format(t1 - t0))\n",
    "\n",
    "        train_loss = epoch_train_loss / train_dataset_size\n",
    "        train_acc = correct/total\n",
    "\n",
    "        train_acc_list.append(train_acc)\n",
    "        train_loss_list.append(train_loss)\n",
    "\n",
    "        print('Loss = %f\\tTraining acc = %f' % (train_loss, train_acc))\n",
    "        print('----------------------------------------')\n",
    "        \n",
    "        print('#############################################')\n",
    "        print('#\\tStart validation on %d epoch' % (epoch_idx + epoch))\n",
    "        print('#############################################')\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            true_values = 0.0\n",
    "            epoch_test_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for data, labels in test_dataloader:\n",
    "                data = data.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # run the model\n",
    "                pred = model(data)\n",
    "                loss = criterion(pred, labels)\n",
    "                epoch_test_loss += loss.item() * data.size(0)\n",
    "                total += labels.size(0)\n",
    "                _, pred_labels = torch.max(pred.data, 1)\n",
    "                correct += (pred_labels == labels).sum().item()\n",
    "        val_acc = correct/total\n",
    "        val_loss = epoch_test_loss / test_dataset_size\n",
    "        val_acc_list.append(val_acc)\n",
    "        val_loss_list.append(val_loss)\n",
    "                \n",
    "        print('\\tLoss = {:.4f}\\tValidation acc = {:.3f}'.format(val_loss, val_acc))\n",
    "        print('---------------------------------------------')\n",
    "      \n",
    "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        #save current model to resume training\n",
    "        #после каждой эпохи сохраняем веса для того, чтобы потом продолжить обучение именно с последней эпохи\n",
    "        #а не с эпохи с лучшими весами \n",
    "        path_to_saving_model = os.path.join(path_to_weights, basic_name + '_current.pth')\n",
    "        torch.save(model.state_dict(), path_to_saving_model)\n",
    "            \n",
    "        if val_acc > best_acc:\n",
    "            print('#############################################')\n",
    "            print('#\\tBest accuracy has achieved')\n",
    "            print('#\\tSaving weights...')\n",
    "            print('#############################################\\n\\n')\n",
    "\n",
    "            model_name = basic_name + '_ep-{}_loss-{:.3}_acc-{:.3}.pth'.format(epoch_idx + epoch, val_loss, val_acc)\n",
    "            path_to_saving_model = os.path.join(path_to_weights, model_name)\n",
    "\n",
    "            torch.save(model.state_dict(), path_to_saving_model)\n",
    "            print('model {} have been saved'.format(path_to_saving_model))\n",
    "            best_acc = val_acc\n",
    "\n",
    "        with open(os.path.join(path_to_pkl, basic_name + '_train_loss.pkl'), 'wb') as f:\n",
    "            pickle.dump(train_loss_list, f)\n",
    "\n",
    "        with open(os.path.join(path_to_pkl, basic_name + '_train_acc.pkl'), 'wb') as f:\n",
    "            pickle.dump(train_acc_list, f)\n",
    "\n",
    "        with open(os.path.join(path_to_pkl, basic_name + '_val_loss.pkl'), 'wb') as f:\n",
    "            pickle.dump(val_loss_list, f)\n",
    "\n",
    "        with open(os.path.join(path_to_pkl, basic_name + '_val_acc.pkl'), 'wb') as f:\n",
    "            pickle.dump(val_acc_list, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model(data.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([64, 6])"
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pytorch1.4-env",
   "display_name": "pytorch1.4-env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}