{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "from scipy.io import wavfile\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tarfile\n",
    "\n",
    "import librosa\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths_to_wavs(path_to_dataset):\n",
    "    file_paths_list = []\n",
    "\n",
    "    for root, dirs, files in os.walk(path_to_dataset):\n",
    "        if len(files) != 0:\n",
    "            file_paths_list += [os.path.join(root, f) for f in files if f.endswith('.wav')]\n",
    "\n",
    "    return file_paths_list\n",
    "\n",
    "def get_paths_to_npys(path_to_dataset):\n",
    "    # get a list with all absolute paths to each file\n",
    "    file_paths_list = []\n",
    "\n",
    "    for root, dirs, files in os.walk(path_to_dataset):\n",
    "        if len(files) != 0:\n",
    "            file_paths_list += [os.path.join(root, f) for f in files if f.endswith('.npy')]\n",
    "            #file_paths_list += [os.path.join(root, f) for f in files if os.path.isdir(os.path.join(root, f))]\n",
    "\n",
    "    return file_paths_list\n",
    "\n",
    "class numpy_ravdess_dataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Due to librosa reads wav-files very slow it is more preferable to read the\n",
    "    numpy representations of the original wavs\n",
    "    '''\n",
    "\n",
    "    emotions_dict = {\n",
    "        0: 'neutral',\n",
    "        1: 'calm',\n",
    "        2: 'happy',\n",
    "        3: 'sad',\n",
    "        4: 'angry',\n",
    "        5: 'fearful',\n",
    "        6: 'disgust',\n",
    "        7: 'surprised'\n",
    "        }\n",
    "\n",
    "    def __init__(self, paths_to_wavs_list, spectrogram_shape, mode):\n",
    "        super(numpy_ravdess_dataset, self).__init__()\n",
    "\n",
    "        self.paths_to_wavs_list = paths_to_wavs_list\n",
    "\n",
    "        self.mfcc_rows = spectrogram_shape[0]\n",
    "        self.mfcc_cols = spectrogram_shape[1]\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths_to_wavs_list)\n",
    "    '''\n",
    "    def read_audio(self, path_to_wav):\n",
    "        return np.load(path_to_wav, allow_pickle=True)\n",
    "    '''\n",
    "    def read_audio(self, path_to_wav):\n",
    "        sr, wav = wavfile.read(path_to_wav)\n",
    "        wav = (wav / 32768).astype(np.float32)\n",
    "        return wav, sr\n",
    "\n",
    "    def get_class_label(self, path_to_file):\n",
    "        # Parse the filename, which has the following pattern:\n",
    "        # modality-vocal_channel-emotion-intensity-statement-repetition-actor.wav\n",
    "        # e.g., '02-01-06-01-02-01-12.wav'\n",
    "        file_name = os.path.split(path_to_file)[1]\n",
    "        file_name = file_name[:-4]\n",
    "        class_label = int(file_name.split('-')[2]) - 1 # 2 is a number of emotion code\n",
    "        return class_label\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path_to_wav = self.paths_to_wavs_list[idx]\n",
    "        # debug\n",
    "        #print(path_to_wav)\n",
    "\n",
    "        # read the wav file\n",
    "        wav, sr = self.read_audio(path_to_wav)       \n",
    "\n",
    "        # augmentation\n",
    "        \n",
    "        if self.mode == 'TRAIN':\n",
    "            # add noise\n",
    "            if np.random.randint(0, 2) == 1:\n",
    "                sigma = np.random.uniform(0.0009, 0.0051)\n",
    "                noise = sigma * np.random.randn(len(wav))\n",
    "                wav += noise\n",
    "            # stretch wav\n",
    "            if np.random.randint(0, 2) == 1:\n",
    "                factor = np.random.uniform(0.5, 1.2)\n",
    "                wav = librosa.effects.time_stretch(wav, 2)\n",
    "            # change pitch\n",
    "            if np.random.randint(0, 2) == 1:\n",
    "                factor = np.random.uniform(-1.5, 1.1)\n",
    "                wav = librosa.effects.pitch_shift(wav, sr=sr, n_steps=factor)\n",
    "    \n",
    "        # get mfcc coefficients\n",
    "        #mfccs = librosa.feature.mfcc(wav, sr=sr, n_mfcc=self.mfcc_rows, n_mels=self.mfcc_rows).astype(np.float32)\n",
    "        '''\n",
    "        if self.mode == 'TRAIN':\n",
    "            # augment by choosing n_fft\n",
    "            n_fft_list = [i for i in range(1024, 2049, 32)]\n",
    "            idx = np.random.randint(len(n_fft_list))\n",
    "            n_fft = n_fft_list[idx]\n",
    "        else:\n",
    "            n_fft = 2048\n",
    "\n",
    "        '''\n",
    "        '''\n",
    "        n_fft = 2048\n",
    "        mfccs = librosa.feature.melspectrogram(wav, sr=sr, n_mels=self.mfcc_rows, n_fft=n_fft, hop_length=128).astype(np.float32)\n",
    "\n",
    "        '''\n",
    "        mfccs = librosa.core.stft(wav, n_fft=self.mfcc_rows*2)#.astype(np.float32)\n",
    "        mfccs = np.abs(mfccs)#**2\n",
    "        mfccs = np.log(mfccs + 0.1)\n",
    "        mfccs = mfccs[:-1]\n",
    "        # debug\n",
    "        #print(mfccs.shape)\n",
    "        #mfccs = (mfccs - mfccs.mean())/np.std(mfccs)\n",
    "\n",
    "        actual_mfcc_cols = mfccs.shape[1]\n",
    "\n",
    "        # prmitive time-shifting augmentation\n",
    "        target_real_diff = actual_mfcc_cols - self.mfcc_cols\n",
    "        # debug\n",
    "        #print(actual_mfcc_cols)\n",
    "        if target_real_diff > 0:\n",
    "            \n",
    "            if self.mode == 'TRAIN':\n",
    "                beginning_col = np.random.randint(target_real_diff)\n",
    "            else:\n",
    "                beginning_col = actual_mfcc_cols//2 - self.mfcc_cols//2\n",
    "\n",
    "            mfccs = mfccs[:, beginning_col:beginning_col + self.mfcc_cols]\n",
    "            #mfccs = mfccs[:, beginning_col:beginning_col + self.mfcc_cols]\n",
    "\n",
    "        elif target_real_diff < 0:\n",
    "            zeros = np.zeros((self.mfcc_rows, self.mfcc_cols), dtype=np.float32)\n",
    "            # debug\n",
    "            #print(zeros.shape)\n",
    "            \n",
    "            if self.mode == 'TRAIN':\n",
    "                beginning_col = np.random.randint(self.mfcc_cols-actual_mfcc_cols)\n",
    "            else:\n",
    "            \n",
    "                beginning_col = self.mfcc_cols//2 - actual_mfcc_cols//2\n",
    "            zeros[..., beginning_col:beginning_col+actual_mfcc_cols] = mfccs\n",
    "            #zeros[..., beginning_col:beginning_col+actual_mfcc_cols] = mfccs\n",
    "            mfccs = zeros\n",
    "            #mfccs = np.pad(mfccs, ((0, 0), (0, np.abs(target_real_diff))), constant_values=(0), mode='constant')\n",
    "\n",
    "        # make the data compatible to pytorch 1-channel CNNs format\n",
    "        # !!!!!!!!!!!!!!!!!!!!!\n",
    "        #mfccs = np.expand_dims(mfccs, axis=0)\n",
    "\n",
    "        # Parse the filename, which has the following pattern:\n",
    "        # modality-vocal_channel-emotion-intensity-statement-repetition-actor.wav\n",
    "        # e.g., '02-01-06-01-02-01-12.wav'\n",
    "        #file_name = os.path.split(path_to_wav)[1]\n",
    "        #file_name = file_name[:-4]\n",
    "        #class_label = int(file_name.split('-')[2]) - 1 # 2 is a number of emotion code\n",
    "        #class_label = np.array(class_label)\n",
    "        class_label = self.get_class_label(path_to_wav)\n",
    "        # !!!!!!!!!\n",
    "        # transpose to reorder index by the time windows of the spectrograms\n",
    "        return torch.from_numpy(mfccs).transpose(1, 0), class_label#, path_to_wav\n",
    "\n",
    "class numpy_crema_dataset(numpy_ravdess_dataset):\n",
    "    emotions_dict = {\n",
    "        'ANG': 0,\n",
    "        'DIS': 1,\n",
    "        'FEA': 2,\n",
    "        'SAD': 3,\n",
    "        'HAP': 4,\n",
    "        'NEU': 5\n",
    "    }\n",
    "\n",
    "    label2str = {\n",
    "        0: 'ANG',\n",
    "        1: 'DIS',\n",
    "        2: 'FEA',\n",
    "        3: 'SAD',\n",
    "        4: 'HAP',\n",
    "        5: 'NEU'\n",
    "    }\n",
    "    \n",
    "    def get_class_label(self, path_to_file):\n",
    "        file_name = os.path.split(path_to_file)[1]\n",
    "        file_name = file_name[:-4]\n",
    "        emotion_name = file_name.split('_')[2] # 2 is a number of emotion code\n",
    "        return self.emotions_dict[emotion_name]\n",
    "\n",
    "class numpy_iemocap_dataset(numpy_ravdess_dataset):\n",
    "    '''\n",
    "    emotions_dict = {\n",
    "        'exc': 0,\n",
    "        'sad': 1,\n",
    "        'fru': 2,\n",
    "        'hap': 3,\n",
    "        'neu': 4,\n",
    "        'sur': 5,\n",
    "        'ang': 6,\n",
    "        'fea': 7,\n",
    "        'dis': 8,\n",
    "        #'oth': 9\n",
    "    }\n",
    "    '''\n",
    "    emotions_dict = {\n",
    "        'exc': 0,\n",
    "        'sad': 1,\n",
    "        'fru': 2,\n",
    "        'hap': 3,\n",
    "        'neu': 4,\n",
    "        'ang': 5,\n",
    "    }\n",
    "\n",
    "    def get_class_label(self, path_to_file):\n",
    "        file_name = os.path.split(path_to_file)[1]\n",
    "        file_name = file_name[:-4]\n",
    "        emotion_name = file_name.split('_')[-1] # the last is a position of emotion code\n",
    "        return self.emotions_dict[emotion_name]\n",
    "\n",
    "class crema_gender_dataset(numpy_ravdess_dataset):\n",
    "    emotions_dict = {\n",
    "        'ANG_Male': 0,\n",
    "        'DIS_Male': 1,\n",
    "        'FEA_Male': 2,\n",
    "        'SAD_Male': 3,\n",
    "        'HAP_Male': 4,\n",
    "        'NEU_Male': 5,\n",
    "        'ANG_Female': 6,\n",
    "        'DIS_Female': 7,\n",
    "        'FEA_Female': 8,\n",
    "        'SAD_Female': 9,\n",
    "        'HAP_Female': 10,\n",
    "        'NEU_Female': 11\n",
    "    }\n",
    "\n",
    "    label2str = {\n",
    "        0: 'ANG',\n",
    "        1: 'DIS',\n",
    "        2: 'FEA',\n",
    "        3: 'SAD',\n",
    "        4: 'HAP',\n",
    "        5: 'NEU'\n",
    "    }\n",
    "    def __init__(self, paths_to_wavs_list, spectrogram_shape, mode, gender_df):\n",
    "        super().__init__(paths_to_wavs_list, spectrogram_shape, mode)\n",
    "        self.gender_df = gender_df\n",
    "    \n",
    "    def get_class_label(self, path_to_file):\n",
    "        \n",
    "        file_name = os.path.split(path_to_file)[1]\n",
    "        file_name = file_name[:-4]\n",
    "        name_list = file_name.split('_') # 2 is a number of emotion code\n",
    "        emotion_name = name_list[2]\n",
    "        actor_id = int(name_list[0])\n",
    "\n",
    "        gender = self.gender_df[self.gender_df['ActorID'] == actor_id]['Sex'].values[0]\n",
    "\n",
    "        return self.emotions_dict['{}_{}'.format(emotion_name, gender)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class audio_rnn(nn.Module):\n",
    "    def __init__(self, rnn, layer_num, input_dim, hidden_dim, class_num, device, bidirectional=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_num = layer_num\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = device\n",
    "        \n",
    "        if bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "       \n",
    "        self.rnn = rnn(input_size=input_dim,\n",
    "                           hidden_size=hidden_dim,\n",
    "                           num_layers=layer_num,\n",
    "                           batch_first=False,\n",
    "                           bidirectional=bidirectional,\n",
    "                           dropout=0.5)\n",
    "        \n",
    "        # Bidirectional nns has twice large inputs size on Linear layer\n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(in_features=hidden_dim * self.num_directions, out_features=class_num)\n",
    "        else:\n",
    "            self.fc = nn.Linear(in_features=hidden_dim, out_features=class_num)        \n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_directions * self.layer_num, batch_size, self.hidden_dim).to(self.device)\n",
    "        cell = torch.zeros(self.num_directions * self.layer_num, batch_size, self.hidden_dim).to(self.device)\n",
    "        return (hidden, cell)\n",
    "\n",
    "    def compute_output(self, output):\n",
    "        if self.num_directions == 2:\n",
    "            # Если рекуррентная сеть является двунаправленной, то на выходной классификатор надо\n",
    "            # подавать выход последнего шага рекуррентной сети прямого прохода - output[-1,:,size//2:],\n",
    "            # а также выход последнего шага рекуррентной сети обратного прохода - output[1,:,:size//2]\n",
    "            size = output.size(2)\n",
    "            result = self.fc(torch.cat([output[1,:,:size//2], output[-1,:,size//2:]], dim=1))\n",
    "        else:\n",
    "            result = self.fc(output[-1])\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        batch_size = batch.shape[0]\n",
    "\n",
    "        batch = batch.transpose(1, 0)\n",
    "\n",
    "            \n",
    "\n",
    "        h0, c0 = self.init_hidden(batch_size=batch_size)\n",
    "\n",
    "        #print('h0 shape =', h0.shape)\n",
    "        \n",
    "        #return h0, c0\n",
    "\n",
    "        # GRU don't has memory cell\n",
    "        # We need to initialize only hidden states h\n",
    "        if isinstance(self.rnn, nn.GRU):\n",
    "            output, hn = self.rnn(batch, h0)\n",
    "        elif isinstance(self.rnn, nn.LSTM):\n",
    "            output, (hn, cn) = self.rnn(batch, (h0, c0))\n",
    "        else:\n",
    "            raise ValueError('self.rnn shoulb be torch.nn.LSTM or torch.nn.GRU')\n",
    "\n",
    "        #return output\n",
    "\n",
    "        result = self.compute_output(output)\n",
    "\n",
    "        return result\n",
    "\n",
    "class audio_rnn_avg(audio_rnn):\n",
    "    def compute_output(self, output):\n",
    "        output = output.mean(dim=0)\n",
    "        result = self.fc(output)\n",
    "\n",
    "        return result\n",
    "\n",
    "class audio_rnn_attention(audio_rnn):\n",
    "    def __init__(self, rnn, layer_num, input_dim, hidden_dim, class_num, device, bidirectional=False):\n",
    "        super().__init__(rnn, layer_num, input_dim, hidden_dim, class_num, device, bidirectional=False)\n",
    "        self.attention = nn.Linear(in_features=hidden_dim, out_features=1)\n",
    "    \n",
    "    def compute_output(self, x):\n",
    "        x = x.transpose(1, 0).contiguous()\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        hidden_dim = x.size(2)\n",
    "\n",
    "        # compute alpha coefficients of attention module\n",
    "        alphas = F.softmax(self.atention(x), dim=1)\n",
    "\n",
    "        # AAAAAAAAAAAAAAAAAAAAAAAAA\n",
    "        # multiply the outputs by the alphas\n",
    "        # outputs have size [batch_size, sequence_len, hidden_dim]\n",
    "        # reshape them to [batch_size * sequence_len, hidden_dim, 1]\n",
    "        # and multiply the by alphas of shape [batch_size * sequence_len, 1, 1]\n",
    "        intermediate = torch.bmm(\n",
    "            x.view(batch_size*seq_len, hidden_dim, 1),\n",
    "            alphas.view(batch_size*seq_len, 1, 1)\n",
    "            )\n",
    "        intermediate = intermediate.view(batch_size, seq_len, -1).sum(dim=1)\n",
    "\n",
    "        output = self.fc(intermediate)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "7442\n"
    }
   ],
   "source": [
    "\n",
    "# CREMA-D\n",
    "target_path = '/media/mikhail/files/datasets/emotion_recognition/CREMA-D/AudioWAV'\n",
    "# IEMOCAP\n",
    "#target_path = '/media/mikhail/files/datasets/emotion_recognition/IEMOCAP/IEMOCAP_full_release/audios'\n",
    " \n",
    "npys_list = get_paths_to_wavs(target_path)\n",
    "\n",
    "# shuffle the dataset to for the learning process stability\n",
    "random.seed(0)\n",
    "random.shuffle(npys_list)\n",
    "\n",
    "dataset_size = len(npys_list)\n",
    "\n",
    "train_size = int(0.8 * dataset_size)\n",
    "\n",
    "print(dataset_size)\n",
    "\n",
    "train_dataset = numpy_crema_dataset(npys_list[:train_size], (128, 256), mode='TRAIN')\n",
    "#train_dataset = numpy_iemocap_dataset(npys_list[:train_size], (256, 256), mode='TRAIN')\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=256, shuffle=True, num_workers=4)\n",
    "\n",
    "# set up test dataset and test dataloader\n",
    "test_dataset = numpy_crema_dataset(npys_list[train_size:], (128, 256), mode='TEST')\n",
    "#test_dataset = numpy_iemocap_dataset(npys_list[train_size:], (256, 256), mode='TEST')\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=256, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up devices\n",
    "cuda = torch.device('cuda:0')\n",
    "cpu = torch.device('cpu')\n",
    "\n",
    "device = cpu\n",
    "\n",
    "rnn = nn.GRU #Avaialable models: nn.LSTM, nn.GRU, bidirectional = True/False\n",
    "bidirectional=False\n",
    "layer_num = 1\n",
    "\n",
    "model = audio_rnn_avg(layer_num=layer_num, rnn=rnn, input_dim=128, hidden_dim=64, class_num=len(train_dataset.emotions_dict), device=device, bidirectional=bidirectional)\n",
    "\n",
    "#summary(model, input_size=(256, 1, 128), batch_size=32, device='cpu')\n",
    "\n",
    "#device = cuda\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# define an optimization algorithm and bind it with the NN parameters\n",
    "optimizer = torch.optim.Adam(params=model.parameters())\n",
    "\n",
    "starting_epoch = 0\n",
    "ending_epoch = 1000\n",
    "epoch_step = 1\n",
    "\n",
    "basic_name = '{}_log_mel_spec_256_emotion_GRU_avg'.format('CREMA')\n",
    "\n",
    "\n",
    "\n",
    "path_to_weights = basic_name\n",
    "path_to_pkl = basic_name\n",
    "\n",
    "if not os.path.isdir(path_to_weights):\n",
    "    os.mkdir(path_to_weights)\n",
    "if not os.path.isdir(path_to_pkl):\n",
    "    os.mkdir(path_to_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "t learning\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #0\nEpoch time = 81.294 s\nLoss = 1.731541\tTraining acc = 0.259533\n----------------------------------------\n#############################################\n#\tStart validation on 0 epoch\n#############################################\n\tLoss = 1.6707\tValidation acc = 0.319\n---------------------------------------------\n#############################################\n#\tBest accuracy has achieved\n#\tSaving weights...\n#############################################\n\n\nmodel CREMA_log_mel_spec_256_emotion_GRU_avg/CREMA_log_mel_spec_256_emotion_GRU_avg_ep-0_loss-1.67_acc-0.319.pth have been saved\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #1\nEpoch time = 88.110 s\nLoss = 1.622019\tTraining acc = 0.311776\n----------------------------------------\n#############################################\n#\tStart validation on 1 epoch\n#############################################\n\tLoss = 1.6402\tValidation acc = 0.308\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #2\nEpoch time = 80.503 s\nLoss = 1.564005\tTraining acc = 0.346044\n----------------------------------------\n#############################################\n#\tStart validation on 2 epoch\n#############################################\n\tLoss = 1.6611\tValidation acc = 0.289\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #3\nEpoch time = 92.137 s\nLoss = 1.547056\tTraining acc = 0.351083\n----------------------------------------\n#############################################\n#\tStart validation on 3 epoch\n#############################################\n\tLoss = 1.5776\tValidation acc = 0.336\n---------------------------------------------\n#############################################\n#\tBest accuracy has achieved\n#\tSaving weights...\n#############################################\n\n\nmodel CREMA_log_mel_spec_256_emotion_GRU_avg/CREMA_log_mel_spec_256_emotion_GRU_avg_ep-3_loss-1.58_acc-0.336.pth have been saved\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #4\nEpoch time = 87.145 s\nLoss = 1.539294\tTraining acc = 0.361330\n----------------------------------------\n#############################################\n#\tStart validation on 4 epoch\n#############################################\n\tLoss = 1.6412\tValidation acc = 0.324\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #5\nEpoch time = 100.261 s\nLoss = 1.525588\tTraining acc = 0.363514\n----------------------------------------\n#############################################\n#\tStart validation on 5 epoch\n#############################################\n\tLoss = 1.5442\tValidation acc = 0.359\n---------------------------------------------\n#############################################\n#\tBest accuracy has achieved\n#\tSaving weights...\n#############################################\n\n\nmodel CREMA_log_mel_spec_256_emotion_GRU_avg/CREMA_log_mel_spec_256_emotion_GRU_avg_ep-5_loss-1.54_acc-0.359.pth have been saved\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #6\nEpoch time = 122.052 s\nLoss = 1.517798\tTraining acc = 0.376113\n----------------------------------------\n#############################################\n#\tStart validation on 6 epoch\n#############################################\n\tLoss = 1.7658\tValidation acc = 0.281\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #7\nEpoch time = 146.437 s\nLoss = 1.516857\tTraining acc = 0.373593\n----------------------------------------\n#############################################\n#\tStart validation on 7 epoch\n#############################################\n\tLoss = 1.5524\tValidation acc = 0.346\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #8\nEpoch time = 138.370 s\nLoss = 1.504596\tTraining acc = 0.380984\n----------------------------------------\n#############################################\n#\tStart validation on 8 epoch\n#############################################\n\tLoss = 1.6078\tValidation acc = 0.342\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #9\nEpoch time = 135.609 s\nLoss = 1.513384\tTraining acc = 0.367546\n----------------------------------------\n#############################################\n#\tStart validation on 9 epoch\n#############################################\n\tLoss = 1.5791\tValidation acc = 0.343\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #10\nEpoch time = 143.448 s\nLoss = 1.510790\tTraining acc = 0.371241\n----------------------------------------\n#############################################\n#\tStart validation on 10 epoch\n#############################################\n\tLoss = 1.5311\tValidation acc = 0.360\n---------------------------------------------\n#############################################\n#\tBest accuracy has achieved\n#\tSaving weights...\n#############################################\n\n\nmodel CREMA_log_mel_spec_256_emotion_GRU_avg/CREMA_log_mel_spec_256_emotion_GRU_avg_ep-10_loss-1.53_acc-0.36.pth have been saved\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #11\nEpoch time = 142.790 s\nLoss = 1.506732\tTraining acc = 0.372249\n----------------------------------------\n#############################################\n#\tStart validation on 11 epoch\n#############################################\n\tLoss = 1.5113\tValidation acc = 0.375\n---------------------------------------------\n#############################################\n#\tBest accuracy has achieved\n#\tSaving weights...\n#############################################\n\n\nmodel CREMA_log_mel_spec_256_emotion_GRU_avg/CREMA_log_mel_spec_256_emotion_GRU_avg_ep-11_loss-1.51_acc-0.375.pth have been saved\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #12\nEpoch time = 145.611 s\nLoss = 1.493527\tTraining acc = 0.385352\n----------------------------------------\n#############################################\n#\tStart validation on 12 epoch\n#############################################\n\tLoss = 1.5062\tValidation acc = 0.379\n---------------------------------------------\n#############################################\n#\tBest accuracy has achieved\n#\tSaving weights...\n#############################################\n\n\nmodel CREMA_log_mel_spec_256_emotion_GRU_avg/CREMA_log_mel_spec_256_emotion_GRU_avg_ep-12_loss-1.51_acc-0.379.pth have been saved\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #13\nEpoch time = 133.018 s\nLoss = 1.493244\tTraining acc = 0.387704\n----------------------------------------\n#############################################\n#\tStart validation on 13 epoch\n#############################################\n\tLoss = 1.5079\tValidation acc = 0.371\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #14\nEpoch time = 138.259 s\nLoss = 1.486835\tTraining acc = 0.384512\n----------------------------------------\n#############################################\n#\tStart validation on 14 epoch\n#############################################\n\tLoss = 1.5046\tValidation acc = 0.376\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #15\nEpoch time = 140.099 s\nLoss = 1.481302\tTraining acc = 0.378969\n----------------------------------------\n#############################################\n#\tStart validation on 15 epoch\n#############################################\n\tLoss = 1.4861\tValidation acc = 0.375\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #16\nEpoch time = 144.969 s\nLoss = 1.474240\tTraining acc = 0.386528\n----------------------------------------\n#############################################\n#\tStart validation on 16 epoch\n#############################################\n\tLoss = 1.5110\tValidation acc = 0.362\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #17\nEpoch time = 132.588 s\nLoss = 1.474585\tTraining acc = 0.392911\n----------------------------------------\n#############################################\n#\tStart validation on 17 epoch\n#############################################\n\tLoss = 1.5251\tValidation acc = 0.355\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #18\nEpoch time = 143.123 s\nLoss = 1.466932\tTraining acc = 0.387536\n----------------------------------------\n#############################################\n#\tStart validation on 18 epoch\n#############################################\n\tLoss = 1.5024\tValidation acc = 0.359\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #19\nEpoch time = 137.188 s\nLoss = 1.466143\tTraining acc = 0.399630\n----------------------------------------\n#############################################\n#\tStart validation on 19 epoch\n#############################################\n\tLoss = 1.4982\tValidation acc = 0.383\n---------------------------------------------\n#############################################\n#\tBest accuracy has achieved\n#\tSaving weights...\n#############################################\n\n\nmodel CREMA_log_mel_spec_256_emotion_GRU_avg/CREMA_log_mel_spec_256_emotion_GRU_avg_ep-19_loss-1.5_acc-0.383.pth have been saved\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #20\nEpoch time = 146.833 s\nLoss = 1.464269\tTraining acc = 0.391231\n----------------------------------------\n#############################################\n#\tStart validation on 20 epoch\n#############################################\n\tLoss = 1.4758\tValidation acc = 0.376\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #21\nEpoch time = 142.490 s\nLoss = 1.471298\tTraining acc = 0.394087\n----------------------------------------\n#############################################\n#\tStart validation on 21 epoch\n#############################################\n\tLoss = 1.5202\tValidation acc = 0.361\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #22\nEpoch time = 141.748 s\nLoss = 1.468466\tTraining acc = 0.394255\n----------------------------------------\n#############################################\n#\tStart validation on 22 epoch\n#############################################\n\tLoss = 1.4627\tValidation acc = 0.379\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #23\nEpoch time = 134.060 s\nLoss = 1.467584\tTraining acc = 0.387872\n----------------------------------------\n#############################################\n#\tStart validation on 23 epoch\n#############################################\n\tLoss = 1.4913\tValidation acc = 0.394\n---------------------------------------------\n#############################################\n#\tBest accuracy has achieved\n#\tSaving weights...\n#############################################\n\n\nmodel CREMA_log_mel_spec_256_emotion_GRU_avg/CREMA_log_mel_spec_256_emotion_GRU_avg_ep-23_loss-1.49_acc-0.394.pth have been saved\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #24\nEpoch time = 135.495 s\nLoss = 1.458891\tTraining acc = 0.403158\n----------------------------------------\n#############################################\n#\tStart validation on 24 epoch\n#############################################\n\tLoss = 1.5180\tValidation acc = 0.362\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #25\nEpoch time = 139.324 s\nLoss = 1.452661\tTraining acc = 0.401310\n----------------------------------------\n#############################################\n#\tStart validation on 25 epoch\n#############################################\n\tLoss = 1.4408\tValidation acc = 0.393\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #26\nEpoch time = 136.843 s\nLoss = 1.457736\tTraining acc = 0.393247\n----------------------------------------\n#############################################\n#\tStart validation on 26 epoch\n#############################################\n\tLoss = 1.5163\tValidation acc = 0.366\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #27\nEpoch time = 142.748 s\nLoss = 1.444217\tTraining acc = 0.411893\n----------------------------------------\n#############################################\n#\tStart validation on 27 epoch\n#############################################\n\tLoss = 1.4966\tValidation acc = 0.372\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #28\nEpoch time = 139.660 s\nLoss = 1.434711\tTraining acc = 0.412733\n----------------------------------------\n#############################################\n#\tStart validation on 28 epoch\n#############################################\n\tLoss = 1.4705\tValidation acc = 0.398\n---------------------------------------------\n#############################################\n#\tBest accuracy has achieved\n#\tSaving weights...\n#############################################\n\n\nmodel CREMA_log_mel_spec_256_emotion_GRU_avg/CREMA_log_mel_spec_256_emotion_GRU_avg_ep-28_loss-1.47_acc-0.398.pth have been saved\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #29\nEpoch time = 134.241 s\nLoss = 1.431516\tTraining acc = 0.411893\n----------------------------------------\n#############################################\n#\tStart validation on 29 epoch\n#############################################\n\tLoss = 1.4803\tValidation acc = 0.398\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #30\nEpoch time = 145.006 s\nLoss = 1.435123\tTraining acc = 0.411725\n----------------------------------------\n#############################################\n#\tStart validation on 30 epoch\n#############################################\n\tLoss = 1.4442\tValidation acc = 0.412\n---------------------------------------------\n#############################################\n#\tBest accuracy has achieved\n#\tSaving weights...\n#############################################\n\n\nmodel CREMA_log_mel_spec_256_emotion_GRU_avg/CREMA_log_mel_spec_256_emotion_GRU_avg_ep-30_loss-1.44_acc-0.412.pth have been saved\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #31\nEpoch time = 140.200 s\nLoss = 1.428139\tTraining acc = 0.415253\n----------------------------------------\n#############################################\n#\tStart validation on 31 epoch\n#############################################\n\tLoss = 1.5636\tValidation acc = 0.377\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #32\nEpoch time = 140.501 s\nLoss = 1.427765\tTraining acc = 0.418444\n----------------------------------------\n#############################################\n#\tStart validation on 32 epoch\n#############################################\n\tLoss = 1.4644\tValidation acc = 0.394\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #33\nEpoch time = 141.732 s\nLoss = 1.439334\tTraining acc = 0.416429\n----------------------------------------\n#############################################\n#\tStart validation on 33 epoch\n#############################################\n\tLoss = 1.4839\tValidation acc = 0.381\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #34\nEpoch time = 137.763 s\nLoss = 1.435405\tTraining acc = 0.412229\n----------------------------------------\n#############################################\n#\tStart validation on 34 epoch\n#############################################\n\tLoss = 1.4702\tValidation acc = 0.390\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #35\nEpoch time = 139.666 s\nLoss = 1.435821\tTraining acc = 0.407190\n----------------------------------------\n#############################################\n#\tStart validation on 35 epoch\n#############################################\n\tLoss = 1.4380\tValidation acc = 0.398\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #36\nEpoch time = 142.258 s\nLoss = 1.426317\tTraining acc = 0.421132\n----------------------------------------\n#############################################\n#\tStart validation on 36 epoch\n#############################################\n\tLoss = 1.4574\tValidation acc = 0.410\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #37\nEpoch time = 135.142 s\nLoss = 1.417736\tTraining acc = 0.419284\n----------------------------------------\n#############################################\n#\tStart validation on 37 epoch\n#############################################\n\tLoss = 1.4468\tValidation acc = 0.404\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #38\nEpoch time = 132.081 s\nLoss = 1.406002\tTraining acc = 0.418109\n----------------------------------------\n#############################################\n#\tStart validation on 38 epoch\n#############################################\n\tLoss = 1.4927\tValidation acc = 0.402\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #39\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f91b9778953f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python_programming/pytorch_projects/pytorch1.4-env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-0559c69d2427>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# We need to initialize only hidden states h\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python_programming/pytorch_projects/pytorch1.4-env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python_programming/pytorch_projects/pytorch1.4-env/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 727\u001b[0;31m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    728\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m             result = _VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "epochs = 500\n",
    "epoch_step = 1\n",
    "\n",
    "print('Start learning')\n",
    "\n",
    "\n",
    "best_acc = 0.0\n",
    "\n",
    "train_dataset_size = len(train_dataloader.dataset)  \n",
    "test_dataset_size = len(test_dataloader.dataset)  \n",
    "\n",
    "\n",
    "if os.path.exists(os.path.join(path_to_pkl, basic_name + '_train_loss.pkl')):\n",
    "    # Update existing classifier\n",
    "    with open(os.path.join(path_to_pkl, basic_name + '_train_loss.pkl'), \"rb\") as f:\n",
    "        train_loss_list = pickle.load(f)\n",
    "else:\n",
    "  train_loss_list = []\n",
    "\n",
    "if os.path.exists(os.path.join(path_to_pkl, basic_name + '_train_acc.pkl')):\n",
    "    # Update existing classifier\n",
    "    with open(os.path.join(path_to_pkl, basic_name + '_train_acc.pkl'), \"rb\") as f:\n",
    "        train_acc_list = pickle.load(f)\n",
    "else:\n",
    "  train_acc_list = []\n",
    "\n",
    "if os.path.exists(os.path.join(path_to_pkl, basic_name + '_val_loss.pkl')):\n",
    "    # Update existing classifier\n",
    "    with open(os.path.join(path_to_pkl, basic_name + '_val_loss.pkl'), \"rb\") as f:\n",
    "        val_loss_list = pickle.load(f)\n",
    "else:\n",
    "  val_loss_list = []\n",
    "\n",
    "if os.path.exists(os.path.join(path_to_pkl, basic_name + '_val_acc.pkl')):\n",
    "    # Update existing classifier\n",
    "    with open(os.path.join(path_to_pkl, basic_name + '_val_acc.pkl'), \"rb\") as f:\n",
    "        val_acc_list = pickle.load(f)\n",
    "else:\n",
    "    val_acc_list = []\n",
    "\n",
    "\n",
    "t = 0.0\n",
    "\n",
    "for epoch_idx in range(start_epoch, epochs, epoch_step):\n",
    "\n",
    "    print('#############################################')\n",
    "    print('#\\tStart training process')\n",
    "    print('#############################################\\n\\n')\n",
    "\n",
    "    # iterate over epochs\n",
    "    for epoch in range(epoch_step):\n",
    "        print('Epoch #{}'.format(epoch_idx + epoch))\n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "        # define losses and correct valuse number for each epoch\n",
    "        epoch_train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # iterate over batches\n",
    "        for data, labels in train_dataloader:\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(data)\n",
    "            loss = criterion(pred, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item() * data.size(0)\n",
    "            total += labels.size(0)\n",
    "            _, pred_labels = torch.max(pred.data, 1)\n",
    "\n",
    "            correct += (pred_labels == labels).sum().item()\n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        print('Epoch time = {:.3f} s'.format(t1 - t0))\n",
    "\n",
    "        train_loss = epoch_train_loss / train_dataset_size\n",
    "        train_acc = correct/total\n",
    "\n",
    "        train_acc_list.append(train_acc)\n",
    "        train_loss_list.append(train_loss)\n",
    "\n",
    "        print('Loss = %f\\tTraining acc = %f' % (train_loss, train_acc))\n",
    "        print('----------------------------------------')\n",
    "        \n",
    "        print('#############################################')\n",
    "        print('#\\tStart validation on %d epoch' % (epoch_idx + epoch))\n",
    "        print('#############################################')\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            true_values = 0.0\n",
    "            epoch_test_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for data, labels in test_dataloader:\n",
    "                data = data.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # run the model\n",
    "                pred = model(data)\n",
    "                loss = criterion(pred, labels)\n",
    "                epoch_test_loss += loss.item() * data.size(0)\n",
    "                total += labels.size(0)\n",
    "                _, pred_labels = torch.max(pred.data, 1)\n",
    "                correct += (pred_labels == labels).sum().item()\n",
    "        val_acc = correct/total\n",
    "        val_loss = epoch_test_loss / test_dataset_size\n",
    "        val_acc_list.append(val_acc)\n",
    "        val_loss_list.append(val_loss)\n",
    "                \n",
    "        print('\\tLoss = {:.4f}\\tValidation acc = {:.3f}'.format(val_loss, val_acc))\n",
    "        print('---------------------------------------------')\n",
    "      \n",
    "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        #save current model to resume training\n",
    "        #после каждой эпохи сохраняем веса для того, чтобы потом продолжить обучение именно с последней эпохи\n",
    "        #а не с эпохи с лучшими весами \n",
    "        path_to_saving_model = os.path.join(path_to_weights, basic_name + '_current.pth')\n",
    "        torch.save(model.state_dict(), path_to_saving_model)\n",
    "            \n",
    "        if val_acc > best_acc:\n",
    "            print('#############################################')\n",
    "            print('#\\tBest accuracy has achieved')\n",
    "            print('#\\tSaving weights...')\n",
    "            print('#############################################\\n\\n')\n",
    "\n",
    "            model_name = basic_name + '_ep-{}_loss-{:.3}_acc-{:.3}.pth'.format(epoch_idx + epoch, val_loss, val_acc)\n",
    "            path_to_saving_model = os.path.join(path_to_weights, model_name)\n",
    "\n",
    "            torch.save(model.state_dict(), path_to_saving_model)\n",
    "            print('model {} have been saved'.format(path_to_saving_model))\n",
    "            best_acc = val_acc\n",
    "\n",
    "        with open(os.path.join(path_to_pkl, basic_name + '_train_loss.pkl'), 'wb') as f:\n",
    "            pickle.dump(train_loss_list, f)\n",
    "\n",
    "        with open(os.path.join(path_to_pkl, basic_name + '_train_acc.pkl'), 'wb') as f:\n",
    "            pickle.dump(train_acc_list, f)\n",
    "\n",
    "        with open(os.path.join(path_to_pkl, basic_name + '_val_loss.pkl'), 'wb') as f:\n",
    "            pickle.dump(val_loss_list, f)\n",
    "\n",
    "        with open(os.path.join(path_to_pkl, basic_name + '_val_acc.pkl'), 'wb') as f:\n",
    "            pickle.dump(val_acc_list, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pytorch1.4-env",
   "display_name": "pytorch1.4-env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}