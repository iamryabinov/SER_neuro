{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from scipy.io import wavfile\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tarfile\n",
    "\n",
    "import librosa\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths_to_wavs(path_to_dataset):\n",
    "    file_paths_list = []\n",
    "\n",
    "    for root, dirs, files in os.walk(path_to_dataset):\n",
    "        if len(files) != 0:\n",
    "            file_paths_list += [os.path.join(root, f) for f in files if f.endswith('.wav')]\n",
    "\n",
    "    return file_paths_list\n",
    "\n",
    "def get_paths_to_npys(path_to_dataset):\n",
    "    # get a list with all absolute paths to each file\n",
    "    file_paths_list = []\n",
    "\n",
    "    for root, dirs, files in os.walk(path_to_dataset):\n",
    "        if len(files) != 0:\n",
    "            file_paths_list += [os.path.join(root, f) for f in files if f.endswith('.npy')]\n",
    "            #file_paths_list += [os.path.join(root, f) for f in files if os.path.isdir(os.path.join(root, f))]\n",
    "\n",
    "    return file_paths_list\n",
    "\n",
    "class numpy_ravdess_dataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Due to librosa reads wav-files very slow it is more preferable to read the\n",
    "    numpy representations of the original wavs\n",
    "    '''\n",
    "\n",
    "    emotions_dict = {\n",
    "        0: 'neutral',\n",
    "        1: 'calm',\n",
    "        2: 'happy',\n",
    "        3: 'sad',\n",
    "        4: 'angry',\n",
    "        5: 'fearful',\n",
    "        6: 'disgust',\n",
    "        7: 'surprised'\n",
    "        }\n",
    "\n",
    "    def __init__(self, paths_to_wavs_list, spectrogram_shape, mode):\n",
    "        super(numpy_ravdess_dataset, self).__init__()\n",
    "\n",
    "        self.paths_to_wavs_list = paths_to_wavs_list\n",
    "\n",
    "        self.mfcc_rows = spectrogram_shape[0]\n",
    "        self.mfcc_cols = spectrogram_shape[1]\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths_to_wavs_list)\n",
    "    '''\n",
    "    def read_audio(self, path_to_wav):\n",
    "        return np.load(path_to_wav, allow_pickle=True)\n",
    "    '''\n",
    "    def read_audio(self, path_to_wav):\n",
    "        sr, wav = wavfile.read(path_to_wav)\n",
    "        wav = (wav / 32768).astype(np.float32)\n",
    "        return wav, sr\n",
    "\n",
    "    def get_class_label(self, path_to_file):\n",
    "        # Parse the filename, which has the following pattern:\n",
    "        # modality-vocal_channel-emotion-intensity-statement-repetition-actor.wav\n",
    "        # e.g., '02-01-06-01-02-01-12.wav'\n",
    "        file_name = os.path.split(path_to_file)[1]\n",
    "        file_name = file_name[:-4]\n",
    "        class_label = int(file_name.split('-')[2]) - 1 # 2 is a number of emotion code\n",
    "        return class_label\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path_to_wav = self.paths_to_wavs_list[idx]\n",
    "        # debug\n",
    "        #print(path_to_wav)\n",
    "\n",
    "        # read the wav file\n",
    "        wav, sr = self.read_audio(path_to_wav)       \n",
    "\n",
    "        # augmentation\n",
    "        \n",
    "        if self.mode == 'TRAIN':\n",
    "            # add noise\n",
    "            if np.random.randint(0, 2) == 1:\n",
    "                sigma = np.random.uniform(0.0009, 0.0051)\n",
    "                noise = sigma * np.random.randn(len(wav))\n",
    "                wav += noise\n",
    "            # stretch wav\n",
    "            if np.random.randint(0, 2) == 1:\n",
    "                factor = np.random.uniform(0.5, 1.2)\n",
    "                wav = librosa.effects.time_stretch(wav, 2)\n",
    "            # change pitch\n",
    "            if np.random.randint(0, 2) == 1:\n",
    "                factor = np.random.uniform(-1.5, 1.1)\n",
    "                wav = librosa.effects.pitch_shift(wav, sr=sr, n_steps=factor)\n",
    "    \n",
    "        # get mfcc coefficients\n",
    "        #mfccs = librosa.feature.mfcc(wav, sr=sr, n_mfcc=self.mfcc_rows, n_mels=self.mfcc_rows).astype(np.float32)\n",
    "        '''\n",
    "        if self.mode == 'TRAIN':\n",
    "            # augment by choosing n_fft\n",
    "            n_fft_list = [i for i in range(1024, 2049, 32)]\n",
    "            idx = np.random.randint(len(n_fft_list))\n",
    "            n_fft = n_fft_list[idx]\n",
    "        else:\n",
    "            n_fft = 2048\n",
    "\n",
    "        '''\n",
    "        \n",
    "        n_fft = 2048\n",
    "        mfccs = librosa.feature.melspectrogram(wav, sr=sr, n_mels=self.mfcc_rows, n_fft=n_fft, hop_length=128).astype(np.float32)\n",
    "\n",
    "        '''\n",
    "        mfccs = librosa.core.stft(wav, n_fft=self.mfcc_rows*2)#.astype(np.float32)\n",
    "        mfccs = np.abs(mfccs)#**2\n",
    "        mfccs = np.log(mfccs + 0.1)\n",
    "        mfccs = mfccs[:-1]\n",
    "        # debug\n",
    "        #print(mfccs.shape)\n",
    "        #mfccs = (mfccs - mfccs.mean())/np.std(mfccs)\n",
    "        '''\n",
    "        actual_mfcc_cols = mfccs.shape[1]\n",
    "\n",
    "        # prmitive time-shifting augmentation\n",
    "        target_real_diff = actual_mfcc_cols - self.mfcc_cols\n",
    "        # debug\n",
    "        #print(actual_mfcc_cols)\n",
    "        if target_real_diff > 0:\n",
    "            \n",
    "            if self.mode == 'TRAIN':\n",
    "                beginning_col = np.random.randint(target_real_diff)\n",
    "            else:\n",
    "                beginning_col = actual_mfcc_cols//2 - self.mfcc_cols//2\n",
    "\n",
    "            mfccs = mfccs[:, beginning_col:beginning_col + self.mfcc_cols]\n",
    "            #mfccs = mfccs[:, beginning_col:beginning_col + self.mfcc_cols]\n",
    "\n",
    "        elif target_real_diff < 0:\n",
    "            zeros = np.zeros((self.mfcc_rows, self.mfcc_cols), dtype=np.float32)\n",
    "            # debug\n",
    "            #print(zeros.shape)\n",
    "            \n",
    "            if self.mode == 'TRAIN':\n",
    "                beginning_col = np.random.randint(self.mfcc_cols-actual_mfcc_cols)\n",
    "            else:\n",
    "            \n",
    "                beginning_col = self.mfcc_cols//2 - actual_mfcc_cols//2\n",
    "            zeros[..., beginning_col:beginning_col+actual_mfcc_cols] = mfccs\n",
    "            #zeros[..., beginning_col:beginning_col+actual_mfcc_cols] = mfccs\n",
    "            mfccs = zeros\n",
    "            #mfccs = np.pad(mfccs, ((0, 0), (0, np.abs(target_real_diff))), constant_values=(0), mode='constant')\n",
    "\n",
    "        # make the data compatible to pytorch 1-channel CNNs format\n",
    "        # !!!!!!!!!!!!!!!!!!!!!\n",
    "        #mfccs = np.expand_dims(mfccs, axis=0)\n",
    "\n",
    "        # Parse the filename, which has the following pattern:\n",
    "        # modality-vocal_channel-emotion-intensity-statement-repetition-actor.wav\n",
    "        # e.g., '02-01-06-01-02-01-12.wav'\n",
    "        #file_name = os.path.split(path_to_wav)[1]\n",
    "        #file_name = file_name[:-4]\n",
    "        #class_label = int(file_name.split('-')[2]) - 1 # 2 is a number of emotion code\n",
    "        #class_label = np.array(class_label)\n",
    "        class_label = self.get_class_label(path_to_wav)\n",
    "        # !!!!!!!!!\n",
    "        # transpose to reorder index by the time windows of the spectrograms\n",
    "        return torch.from_numpy(mfccs).transpose(1, 0), class_label#, path_to_wav\n",
    "\n",
    "class numpy_crema_dataset(numpy_ravdess_dataset):\n",
    "    emotions_dict = {\n",
    "        'ANG': 0,\n",
    "        'DIS': 1,\n",
    "        'FEA': 2,\n",
    "        'SAD': 3,\n",
    "        'HAP': 4,\n",
    "        'NEU': 5\n",
    "    }\n",
    "\n",
    "    label2str = {\n",
    "        0: 'ANG',\n",
    "        1: 'DIS',\n",
    "        2: 'FEA',\n",
    "        3: 'SAD',\n",
    "        4: 'HAP',\n",
    "        5: 'NEU'\n",
    "    }\n",
    "    \n",
    "    def get_class_label(self, path_to_file):\n",
    "        file_name = os.path.split(path_to_file)[1]\n",
    "        file_name = file_name[:-4]\n",
    "        emotion_name = file_name.split('_')[2] # 2 is a number of emotion code\n",
    "        return self.emotions_dict[emotion_name]\n",
    "\n",
    "class numpy_iemocap_dataset(numpy_ravdess_dataset):\n",
    "    '''\n",
    "    emotions_dict = {\n",
    "        'exc': 0,\n",
    "        'sad': 1,\n",
    "        'fru': 2,\n",
    "        'hap': 3,\n",
    "        'neu': 4,\n",
    "        'sur': 5,\n",
    "        'ang': 6,\n",
    "        'fea': 7,\n",
    "        'dis': 8,\n",
    "        #'oth': 9\n",
    "    }\n",
    "    '''\n",
    "    emotions_dict = {\n",
    "        'exc': 0,\n",
    "        'sad': 1,\n",
    "        'fru': 2,\n",
    "        'hap': 3,\n",
    "        'neu': 4,\n",
    "        'ang': 5,\n",
    "    }\n",
    "\n",
    "    def get_class_label(self, path_to_file):\n",
    "        file_name = os.path.split(path_to_file)[1]\n",
    "        file_name = file_name[:-4]\n",
    "        emotion_name = file_name.split('_')[-1] # the last is a position of emotion code\n",
    "        return self.emotions_dict[emotion_name]\n",
    "\n",
    "class crema_gender_dataset(numpy_ravdess_dataset):\n",
    "    emotions_dict = {\n",
    "        'ANG_Male': 0,\n",
    "        'DIS_Male': 1,\n",
    "        'FEA_Male': 2,\n",
    "        'SAD_Male': 3,\n",
    "        'HAP_Male': 4,\n",
    "        'NEU_Male': 5,\n",
    "        'ANG_Female': 6,\n",
    "        'DIS_Female': 7,\n",
    "        'FEA_Female': 8,\n",
    "        'SAD_Female': 9,\n",
    "        'HAP_Female': 10,\n",
    "        'NEU_Female': 11\n",
    "    }\n",
    "\n",
    "    label2str = {\n",
    "        0: 'ANG',\n",
    "        1: 'DIS',\n",
    "        2: 'FEA',\n",
    "        3: 'SAD',\n",
    "        4: 'HAP',\n",
    "        5: 'NEU'\n",
    "    }\n",
    "    def __init__(self, paths_to_wavs_list, spectrogram_shape, mode, gender_df):\n",
    "        super().__init__(paths_to_wavs_list, spectrogram_shape, mode)\n",
    "        self.gender_df = gender_df\n",
    "    \n",
    "    def get_class_label(self, path_to_file):\n",
    "        \n",
    "        file_name = os.path.split(path_to_file)[1]\n",
    "        file_name = file_name[:-4]\n",
    "        name_list = file_name.split('_') # 2 is a number of emotion code\n",
    "        emotion_name = name_list[2]\n",
    "        actor_id = int(name_list[0])\n",
    "\n",
    "        gender = self.gender_df[self.gender_df['ActorID'] == actor_id]['Sex'].values[0]\n",
    "\n",
    "        return self.emotions_dict['{}_{}'.format(emotion_name, gender)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class audio_rnn(nn.Module):\n",
    "    def __init__(self, rnn, layer_num, input_dim, hidden_dim, class_num, device, bidirectional=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_num = layer_num\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = device\n",
    "        \n",
    "        if bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "       \n",
    "        self.rnn = rnn(input_size=input_dim,\n",
    "                           hidden_size=hidden_dim,\n",
    "                           num_layers=layer_num,\n",
    "                           batch_first=False,\n",
    "                           bidirectional=bidirectional,\n",
    "                           dropout=0.5)\n",
    "        \n",
    "        # Bidirectional nns has twice large inputs size on Linear layer\n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(in_features=hidden_dim * self.num_directions, out_features=class_num)\n",
    "        else:\n",
    "            self.fc = nn.Linear(in_features=hidden_dim, out_features=class_num)        \n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_directions * self.layer_num, batch_size, self.hidden_dim).to(self.device)\n",
    "        cell = torch.zeros(self.num_directions * self.layer_num, batch_size, self.hidden_dim).to(self.device)\n",
    "        return (hidden, cell)\n",
    "\n",
    "    def compute_output(self, output):\n",
    "        if self.num_directions == 2:\n",
    "            # Если рекуррентная сеть является двунаправленной, то на выходной классификатор надо\n",
    "            # подавать выход последнего шага рекуррентной сети прямого прохода - output[-1,:,size//2:],\n",
    "            # а также выход последнего шага рекуррентной сети обратного прохода - output[1,:,:size//2]\n",
    "            size = output.size(2)\n",
    "            result = self.fc(torch.cat([output[1,:,:size//2], output[-1,:,size//2:]], dim=1))\n",
    "        else:\n",
    "            result = self.fc(output[-1])\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        batch_size = batch.shape[0]\n",
    "\n",
    "        batch = batch.transpose(1, 0)\n",
    "\n",
    "            \n",
    "\n",
    "        h0, c0 = self.init_hidden(batch_size=batch_size)\n",
    "\n",
    "        #print('h0 shape =', h0.shape)\n",
    "        \n",
    "        #return h0, c0\n",
    "\n",
    "        # GRU don't has memory cell\n",
    "        # We need to initialize only hidden states h\n",
    "        if isinstance(self.rnn, nn.GRU):\n",
    "            output, hn = self.rnn(batch, h0)\n",
    "        elif isinstance(self.rnn, nn.LSTM):\n",
    "            output, (hn, cn) = self.rnn(batch, (h0, c0))\n",
    "        else:\n",
    "            raise ValueError('self.rnn shoulb be torch.nn.LSTM or torch.nn.GRU')\n",
    "\n",
    "        #return output\n",
    "\n",
    "        result = self.compute_output(output)\n",
    "\n",
    "        return result\n",
    "\n",
    "class audio_rnn_avg(audio_rnn):\n",
    "    def compute_output(self, output):\n",
    "        output = output.mean(dim=0)\n",
    "        result = self.fc(output)\n",
    "\n",
    "        return result\n",
    "\n",
    "class audio_rnn_attention(audio_rnn):\n",
    "    def __init__(self, rnn, layer_num, input_dim, hidden_dim, class_num, device, bidirectional=False):\n",
    "        super().__init__(rnn, layer_num, input_dim, hidden_dim, class_num, device, bidirectional=False)\n",
    "        self.attention = nn.Linear(in_features=hidden_dim, out_features=1)\n",
    "    \n",
    "    def compute_output(self, x):\n",
    "        x = x.transpose(1, 0).contiguous()\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        hidden_dim = x.size(2)\n",
    "\n",
    "        # compute alpha coefficients of attention module\n",
    "        alphas = F.softmax(self.attention(x), dim=1)\n",
    "\n",
    "        # AAAAAAAAAAAAAAAAAAAAAAAAA\n",
    "        # multiply the outputs by the alphas\n",
    "        # outputs have size [batch_size, sequence_len, hidden_dim]\n",
    "        # reshape them to [batch_size * sequence_len, hidden_dim, 1]\n",
    "        # and multiply the by alphas of shape [batch_size * sequence_len, 1, 1]\n",
    "        intermediate = torch.bmm(\n",
    "            x.view(batch_size*seq_len, hidden_dim, 1),\n",
    "            alphas.view(batch_size*seq_len, 1, 1)\n",
    "            )\n",
    "        intermediate = intermediate.view(batch_size, seq_len, -1).sum(dim=1)\n",
    "\n",
    "        output = self.fc(intermediate)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "7442\n"
    }
   ],
   "source": [
    "\n",
    "# CREMA-D\n",
    "target_path = '/media/mikhail/files/datasets/emotion_recognition/CREMA-D/AudioWAV'\n",
    "# IEMOCAP\n",
    "#target_path = '/media/mikhail/files/datasets/emotion_recognition/IEMOCAP/IEMOCAP_full_release/audios'\n",
    " \n",
    "npys_list = get_paths_to_wavs(target_path)\n",
    "\n",
    "# shuffle the dataset to for the learning process stability\n",
    "random.seed(10)\n",
    "random.shuffle(npys_list)\n",
    "\n",
    "dataset_size = len(npys_list)\n",
    "\n",
    "train_size = int(0.8 * dataset_size)\n",
    "\n",
    "print(dataset_size)\n",
    "\n",
    "train_dataset = numpy_crema_dataset(npys_list[:train_size], (128, 256), mode='TRAIN')\n",
    "#train_dataset = numpy_iemocap_dataset(npys_list[:train_size], (256, 256), mode='TRAIN')\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=256, shuffle=True, num_workers=4)\n",
    "\n",
    "# set up test dataset and test dataloader\n",
    "test_dataset = numpy_crema_dataset(npys_list[train_size:], (128, 256), mode='TEST')\n",
    "#test_dataset = numpy_iemocap_dataset(npys_list[train_size:], (256, 256), mode='TEST')\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=256, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up devices\n",
    "cuda = torch.device('cuda:0')\n",
    "cpu = torch.device('cpu')\n",
    "\n",
    "device = cuda\n",
    "\n",
    "rnn = nn.GRU #Avaialable models: nn.LSTM, nn.GRU, bidirectional = True/False\n",
    "bidirectional=True\n",
    "layer_num = 3\n",
    "\n",
    "model = audio_rnn_attention(layer_num=layer_num, rnn=rnn, input_dim=256, hidden_dim=128, class_num=len(train_dataset.emotions_dict), device=device, bidirectional=bidirectional)\n",
    "\n",
    "#summary(model, input_size=(256, 1, 128), batch_size=32, device='cpu')\n",
    "\n",
    "#device = cuda\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# define an optimization algorithm and bind it with the NN parameters\n",
    "optimizer = torch.optim.Adam(params=model.parameters())\n",
    "\n",
    "starting_epoch = 0\n",
    "ending_epoch = 1000\n",
    "epoch_step = 1\n",
    "\n",
    "basic_name = '{}_log_mel_spec_256_emotion_GRU_attention'.format('CREMA')\n",
    "\n",
    "\n",
    "\n",
    "path_to_weights = basic_name\n",
    "path_to_pkl = basic_name\n",
    "\n",
    "if not os.path.isdir(path_to_weights):\n",
    "    os.mkdir(path_to_weights)\n",
    "if not os.path.isdir(path_to_pkl):\n",
    "    os.mkdir(path_to_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nEpoch #8\nEpoch time = 99.690 s\nLoss = 1.482736\tTraining acc = 0.388880\n----------------------------------------\n#############################################\n#\tStart validation on 8 epoch\n#############################################\n\tLoss = 1.5062\tValidation acc = 0.381\n---------------------------------------------\n#############################################\n#\tBest accuracy has achieved\n#\tSaving weights...\n#############################################\n\n\nmodel CREMA_log_mel_spec_256_emotion_GRU_attention/CREMA_log_mel_spec_256_emotion_GRU_attention_ep-8_loss-1.51_acc-0.381.pth have been saved\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #9\nEpoch time = 92.754 s\nLoss = 1.479115\tTraining acc = 0.384176\n----------------------------------------\n#############################################\n#\tStart validation on 9 epoch\n#############################################\n\tLoss = 1.5987\tValidation acc = 0.324\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #10\nEpoch time = 94.696 s\nLoss = 1.468062\tTraining acc = 0.389216\n----------------------------------------\n#############################################\n#\tStart validation on 10 epoch\n#############################################\n\tLoss = 1.5538\tValidation acc = 0.333\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #11\nEpoch time = 91.640 s\nLoss = 1.463518\tTraining acc = 0.388544\n----------------------------------------\n#############################################\n#\tStart validation on 11 epoch\n#############################################\n\tLoss = 1.5139\tValidation acc = 0.370\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #12\nEpoch time = 94.027 s\nLoss = 1.454267\tTraining acc = 0.396439\n----------------------------------------\n#############################################\n#\tStart validation on 12 epoch\n#############################################\n\tLoss = 1.5271\tValidation acc = 0.346\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #13\nEpoch time = 92.686 s\nLoss = 1.446564\tTraining acc = 0.412229\n----------------------------------------\n#############################################\n#\tStart validation on 13 epoch\n#############################################\n\tLoss = 1.5324\tValidation acc = 0.364\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #14\nEpoch time = 95.266 s\nLoss = 1.455557\tTraining acc = 0.392575\n----------------------------------------\n#############################################\n#\tStart validation on 14 epoch\n#############################################\n\tLoss = 1.5088\tValidation acc = 0.383\n---------------------------------------------\n#############################################\n#\tBest accuracy has achieved\n#\tSaving weights...\n#############################################\n\n\nmodel CREMA_log_mel_spec_256_emotion_GRU_attention/CREMA_log_mel_spec_256_emotion_GRU_attention_ep-14_loss-1.51_acc-0.383.pth have been saved\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #15\nEpoch time = 97.546 s\nLoss = 1.439129\tTraining acc = 0.413741\n----------------------------------------\n#############################################\n#\tStart validation on 15 epoch\n#############################################\n\tLoss = 1.5342\tValidation acc = 0.346\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #16\nEpoch time = 98.759 s\nLoss = 1.439247\tTraining acc = 0.409373\n----------------------------------------\n#############################################\n#\tStart validation on 16 epoch\n#############################################\n\tLoss = 1.5169\tValidation acc = 0.363\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #17\nEpoch time = 97.308 s\nLoss = 1.441932\tTraining acc = 0.403326\n----------------------------------------\n#############################################\n#\tStart validation on 17 epoch\n#############################################\n\tLoss = 1.4925\tValidation acc = 0.375\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #18\nEpoch time = 95.844 s\nLoss = 1.428322\tTraining acc = 0.408869\n----------------------------------------\n#############################################\n#\tStart validation on 18 epoch\n#############################################\n\tLoss = 1.4934\tValidation acc = 0.385\n---------------------------------------------\n#############################################\n#\tBest accuracy has achieved\n#\tSaving weights...\n#############################################\n\n\nmodel CREMA_log_mel_spec_256_emotion_GRU_attention/CREMA_log_mel_spec_256_emotion_GRU_attention_ep-18_loss-1.49_acc-0.385.pth have been saved\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #19\nEpoch time = 94.767 s\nLoss = 1.415979\tTraining acc = 0.423316\n----------------------------------------\n#############################################\n#\tStart validation on 19 epoch\n#############################################\n\tLoss = 1.5489\tValidation acc = 0.355\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #20\nEpoch time = 95.044 s\nLoss = 1.423799\tTraining acc = 0.416093\n----------------------------------------\n#############################################\n#\tStart validation on 20 epoch\n#############################################\n\tLoss = 1.5195\tValidation acc = 0.373\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #21\nEpoch time = 98.633 s\nLoss = 1.420961\tTraining acc = 0.416261\n----------------------------------------\n#############################################\n#\tStart validation on 21 epoch\n#############################################\n\tLoss = 1.5132\tValidation acc = 0.379\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #22\nEpoch time = 93.571 s\nLoss = 1.420554\tTraining acc = 0.413741\n----------------------------------------\n#############################################\n#\tStart validation on 22 epoch\n#############################################\n\tLoss = 1.4475\tValidation acc = 0.415\n---------------------------------------------\n#############################################\n#\tBest accuracy has achieved\n#\tSaving weights...\n#############################################\n\n\nmodel CREMA_log_mel_spec_256_emotion_GRU_attention/CREMA_log_mel_spec_256_emotion_GRU_attention_ep-22_loss-1.45_acc-0.415.pth have been saved\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #23\nEpoch time = 91.509 s\nLoss = 1.414771\tTraining acc = 0.414245\n----------------------------------------\n#############################################\n#\tStart validation on 23 epoch\n#############################################\n\tLoss = 1.5005\tValidation acc = 0.381\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #24\nEpoch time = 93.337 s\nLoss = 1.412887\tTraining acc = 0.425500\n----------------------------------------\n#############################################\n#\tStart validation on 24 epoch\n#############################################\n\tLoss = 1.5592\tValidation acc = 0.342\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #25\nEpoch time = 94.563 s\nLoss = 1.401615\tTraining acc = 0.425332\n----------------------------------------\n#############################################\n#\tStart validation on 25 epoch\n#############################################\n\tLoss = 1.5157\tValidation acc = 0.396\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #26\nEpoch time = 94.605 s\nLoss = 1.396598\tTraining acc = 0.427852\n----------------------------------------\n#############################################\n#\tStart validation on 26 epoch\n#############################################\n\tLoss = 1.4792\tValidation acc = 0.387\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #27\nEpoch time = 93.958 s\nLoss = 1.395494\tTraining acc = 0.427684\n----------------------------------------\n#############################################\n#\tStart validation on 27 epoch\n#############################################\n\tLoss = 1.4435\tValidation acc = 0.412\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #28\nEpoch time = 95.827 s\nLoss = 1.397944\tTraining acc = 0.424996\n----------------------------------------\n#############################################\n#\tStart validation on 28 epoch\n#############################################\n\tLoss = 1.5217\tValidation acc = 0.384\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #29\nEpoch time = 93.804 s\nLoss = 1.389069\tTraining acc = 0.433563\n----------------------------------------\n#############################################\n#\tStart validation on 29 epoch\n#############################################\n\tLoss = 1.4580\tValidation acc = 0.390\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #30\nEpoch time = 94.497 s\nLoss = 1.403632\tTraining acc = 0.425668\n----------------------------------------\n#############################################\n#\tStart validation on 30 epoch\n#############################################\n\tLoss = 1.5458\tValidation acc = 0.350\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #31\nEpoch time = 95.091 s\nLoss = 1.409890\tTraining acc = 0.420124\n----------------------------------------\n#############################################\n#\tStart validation on 31 epoch\n#############################################\n\tLoss = 1.4656\tValidation acc = 0.406\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #32\nEpoch time = 93.423 s\nLoss = 1.386072\tTraining acc = 0.436419\n----------------------------------------\n#############################################\n#\tStart validation on 32 epoch\n#############################################\n\tLoss = 1.4728\tValidation acc = 0.408\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #33\nEpoch time = 92.658 s\nLoss = 1.383194\tTraining acc = 0.431547\n----------------------------------------\n#############################################\n#\tStart validation on 33 epoch\n#############################################\n\tLoss = 1.4950\tValidation acc = 0.404\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #34\nEpoch time = 95.427 s\nLoss = 1.376035\tTraining acc = 0.437091\n----------------------------------------\n#############################################\n#\tStart validation on 34 epoch\n#############################################\n\tLoss = 1.4602\tValidation acc = 0.424\n---------------------------------------------\n#############################################\n#\tBest accuracy has achieved\n#\tSaving weights...\n#############################################\n\n\nmodel CREMA_log_mel_spec_256_emotion_GRU_attention/CREMA_log_mel_spec_256_emotion_GRU_attention_ep-34_loss-1.46_acc-0.424.pth have been saved\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #35\nEpoch time = 94.024 s\nLoss = 1.382301\tTraining acc = 0.444482\n----------------------------------------\n#############################################\n#\tStart validation on 35 epoch\n#############################################\n\tLoss = 1.4333\tValidation acc = 0.422\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #36\nEpoch time = 91.487 s\nLoss = 1.368530\tTraining acc = 0.441122\n----------------------------------------\n#############################################\n#\tStart validation on 36 epoch\n#############################################\n\tLoss = 1.5037\tValidation acc = 0.402\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #37\nEpoch time = 94.752 s\nLoss = 1.383542\tTraining acc = 0.436083\n----------------------------------------\n#############################################\n#\tStart validation on 37 epoch\n#############################################\n\tLoss = 1.4472\tValidation acc = 0.421\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #38\nEpoch time = 93.200 s\nLoss = 1.365491\tTraining acc = 0.448513\n----------------------------------------\n#############################################\n#\tStart validation on 38 epoch\n#############################################\n\tLoss = 1.4334\tValidation acc = 0.414\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #39\nEpoch time = 89.950 s\nLoss = 1.371764\tTraining acc = 0.444650\n----------------------------------------\n#############################################\n#\tStart validation on 39 epoch\n#############################################\n\tLoss = 1.4213\tValidation acc = 0.439\n---------------------------------------------\n#############################################\n#\tBest accuracy has achieved\n#\tSaving weights...\n#############################################\n\n\nmodel CREMA_log_mel_spec_256_emotion_GRU_attention/CREMA_log_mel_spec_256_emotion_GRU_attention_ep-39_loss-1.42_acc-0.439.pth have been saved\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #40\nEpoch time = 94.766 s\nLoss = 1.376647\tTraining acc = 0.437930\n----------------------------------------\n#############################################\n#\tStart validation on 40 epoch\n#############################################\n\tLoss = 1.4894\tValidation acc = 0.395\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #41\nEpoch time = 94.345 s\nLoss = 1.371520\tTraining acc = 0.441794\n----------------------------------------\n#############################################\n#\tStart validation on 41 epoch\n#############################################\n\tLoss = 1.4370\tValidation acc = 0.435\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #42\nEpoch time = 92.942 s\nLoss = 1.376734\tTraining acc = 0.439610\n----------------------------------------\n#############################################\n#\tStart validation on 42 epoch\n#############################################\n\tLoss = 1.4529\tValidation acc = 0.432\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #43\nEpoch time = 92.653 s\nLoss = 1.379116\tTraining acc = 0.434907\n----------------------------------------\n#############################################\n#\tStart validation on 43 epoch\n#############################################\n\tLoss = 1.4260\tValidation acc = 0.404\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #44\nEpoch time = 90.816 s\nLoss = 1.368923\tTraining acc = 0.439442\n----------------------------------------\n#############################################\n#\tStart validation on 44 epoch\n#############################################\n\tLoss = 1.4171\tValidation acc = 0.433\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #45\nEpoch time = 93.143 s\nLoss = 1.358356\tTraining acc = 0.443810\n----------------------------------------\n#############################################\n#\tStart validation on 45 epoch\n#############################################\n\tLoss = 1.4224\tValidation acc = 0.429\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #46\nEpoch time = 92.466 s\nLoss = 1.363122\tTraining acc = 0.448849\n----------------------------------------\n#############################################\n#\tStart validation on 46 epoch\n#############################################\n\tLoss = 1.4161\tValidation acc = 0.442\n---------------------------------------------\n#############################################\n#\tBest accuracy has achieved\n#\tSaving weights...\n#############################################\n\n\nmodel CREMA_log_mel_spec_256_emotion_GRU_attention/CREMA_log_mel_spec_256_emotion_GRU_attention_ep-46_loss-1.42_acc-0.442.pth have been saved\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #47\nEpoch time = 92.705 s\nLoss = 1.356760\tTraining acc = 0.448345\n----------------------------------------\n#############################################\n#\tStart validation on 47 epoch\n#############################################\n\tLoss = 1.4851\tValidation acc = 0.387\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #48\nEpoch time = 91.934 s\nLoss = 1.359477\tTraining acc = 0.447673\n----------------------------------------\n#############################################\n#\tStart validation on 48 epoch\n#############################################\n\tLoss = 1.4226\tValidation acc = 0.420\n---------------------------------------------\n#############################################\n#\tStart training process\n#############################################\n\n\nEpoch #49\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f91b9778953f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# iterate over batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python_programming/pytorch_projects/pytorch1.4-env/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python_programming/pytorch_projects/pytorch1.4-env/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python_programming/pytorch_projects/pytorch1.4-env/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python_programming/pytorch_projects/pytorch1.4-env/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "epochs = 500\n",
    "epoch_step = 1\n",
    "\n",
    "print('Start learning')\n",
    "\n",
    "\n",
    "best_acc = 0.0\n",
    "\n",
    "train_dataset_size = len(train_dataloader.dataset)  \n",
    "test_dataset_size = len(test_dataloader.dataset)  \n",
    "\n",
    "\n",
    "if os.path.exists(os.path.join(path_to_pkl, basic_name + '_train_loss.pkl')):\n",
    "    # Update existing classifier\n",
    "    with open(os.path.join(path_to_pkl, basic_name + '_train_loss.pkl'), \"rb\") as f:\n",
    "        train_loss_list = pickle.load(f)\n",
    "else:\n",
    "  train_loss_list = []\n",
    "\n",
    "if os.path.exists(os.path.join(path_to_pkl, basic_name + '_train_acc.pkl')):\n",
    "    # Update existing classifier\n",
    "    with open(os.path.join(path_to_pkl, basic_name + '_train_acc.pkl'), \"rb\") as f:\n",
    "        train_acc_list = pickle.load(f)\n",
    "else:\n",
    "  train_acc_list = []\n",
    "\n",
    "if os.path.exists(os.path.join(path_to_pkl, basic_name + '_val_loss.pkl')):\n",
    "    # Update existing classifier\n",
    "    with open(os.path.join(path_to_pkl, basic_name + '_val_loss.pkl'), \"rb\") as f:\n",
    "        val_loss_list = pickle.load(f)\n",
    "else:\n",
    "  val_loss_list = []\n",
    "\n",
    "if os.path.exists(os.path.join(path_to_pkl, basic_name + '_val_acc.pkl')):\n",
    "    # Update existing classifier\n",
    "    with open(os.path.join(path_to_pkl, basic_name + '_val_acc.pkl'), \"rb\") as f:\n",
    "        val_acc_list = pickle.load(f)\n",
    "else:\n",
    "    val_acc_list = []\n",
    "\n",
    "\n",
    "t = 0.0\n",
    "\n",
    "for epoch_idx in range(start_epoch, epochs, epoch_step):\n",
    "\n",
    "    print('#############################################')\n",
    "    print('#\\tStart training process')\n",
    "    print('#############################################\\n\\n')\n",
    "\n",
    "    # iterate over epochs\n",
    "    for epoch in range(epoch_step):\n",
    "        print('Epoch #{}'.format(epoch_idx + epoch))\n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "        # define losses and correct valuse number for each epoch\n",
    "        epoch_train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # iterate over batches\n",
    "        for data, labels in train_dataloader:\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(data)\n",
    "            loss = criterion(pred, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item() * data.size(0)\n",
    "            total += labels.size(0)\n",
    "            _, pred_labels = torch.max(pred.data, 1)\n",
    "\n",
    "            correct += (pred_labels == labels).sum().item()\n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        print('Epoch time = {:.3f} s'.format(t1 - t0))\n",
    "\n",
    "        train_loss = epoch_train_loss / train_dataset_size\n",
    "        train_acc = correct/total\n",
    "\n",
    "        train_acc_list.append(train_acc)\n",
    "        train_loss_list.append(train_loss)\n",
    "\n",
    "        print('Loss = %f\\tTraining acc = %f' % (train_loss, train_acc))\n",
    "        print('----------------------------------------')\n",
    "        \n",
    "        print('#############################################')\n",
    "        print('#\\tStart validation on %d epoch' % (epoch_idx + epoch))\n",
    "        print('#############################################')\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            true_values = 0.0\n",
    "            epoch_test_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for data, labels in test_dataloader:\n",
    "                data = data.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # run the model\n",
    "                pred = model(data)\n",
    "                loss = criterion(pred, labels)\n",
    "                epoch_test_loss += loss.item() * data.size(0)\n",
    "                total += labels.size(0)\n",
    "                _, pred_labels = torch.max(pred.data, 1)\n",
    "                correct += (pred_labels == labels).sum().item()\n",
    "        val_acc = correct/total\n",
    "        val_loss = epoch_test_loss / test_dataset_size\n",
    "        val_acc_list.append(val_acc)\n",
    "        val_loss_list.append(val_loss)\n",
    "                \n",
    "        print('\\tLoss = {:.4f}\\tValidation acc = {:.3f}'.format(val_loss, val_acc))\n",
    "        print('---------------------------------------------')\n",
    "      \n",
    "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        #save current model to resume training\n",
    "        #после каждой эпохи сохраняем веса для того, чтобы потом продолжить обучение именно с последней эпохи\n",
    "        #а не с эпохи с лучшими весами \n",
    "        path_to_saving_model = os.path.join(path_to_weights, basic_name + '_current.pth')\n",
    "        torch.save(model.state_dict(), path_to_saving_model)\n",
    "            \n",
    "        if val_acc > best_acc:\n",
    "            print('#############################################')\n",
    "            print('#\\tBest accuracy has achieved')\n",
    "            print('#\\tSaving weights...')\n",
    "            print('#############################################\\n\\n')\n",
    "\n",
    "            model_name = basic_name + '_ep-{}_loss-{:.3}_acc-{:.3}.pth'.format(epoch_idx + epoch, val_loss, val_acc)\n",
    "            path_to_saving_model = os.path.join(path_to_weights, model_name)\n",
    "\n",
    "            torch.save(model.state_dict(), path_to_saving_model)\n",
    "            print('model {} have been saved'.format(path_to_saving_model))\n",
    "            best_acc = val_acc\n",
    "\n",
    "        with open(os.path.join(path_to_pkl, basic_name + '_train_loss.pkl'), 'wb') as f:\n",
    "            pickle.dump(train_loss_list, f)\n",
    "\n",
    "        with open(os.path.join(path_to_pkl, basic_name + '_train_acc.pkl'), 'wb') as f:\n",
    "            pickle.dump(train_acc_list, f)\n",
    "\n",
    "        with open(os.path.join(path_to_pkl, basic_name + '_val_loss.pkl'), 'wb') as f:\n",
    "            pickle.dump(val_loss_list, f)\n",
    "\n",
    "        with open(os.path.join(path_to_pkl, basic_name + '_val_acc.pkl'), 'wb') as f:\n",
    "            pickle.dump(val_acc_list, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "my_project",
   "display_name": "my_project"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}