{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w14l2b0j6-oh"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "import zipfile\n",
    "import tarfile\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "import wave\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import pickle\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "-VfPedSu_XKu",
    "outputId": "f9eac6ce-d9cd-4334-8665-e1a0d418887e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have picked CREMA-D dataset\n"
     ]
    }
   ],
   "source": [
    "dataset_type = input('Pick a dataset from the following\\nby writing a letter in the upper case\\nI (IEMOCAP), R (RAVDESS), C (CREMA-D):')\n",
    "\n",
    "dataset_type = dataset_type.upper()\n",
    "\n",
    "dataset_type_dict = {\n",
    "    'Name': ['CREMA-D', 'IEMOCAP', 'RAVDESS'],\n",
    "    'Archive name': ['crema_d-audio_np-sr_22050.tar.xz', 'iemocap_numpy_audio_sr22050.tar.xz', 'ravdess-speech-sr22050.tar.xz']}\n",
    "datasets_info = pd.DataFrame(dataset_type_dict, index=['C', 'I', 'R'])\n",
    " \n",
    "\n",
    "print('You have picked {} dataset'.format(datasets_info['Name'][dataset_type]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "5tz70bjjALXc",
    "outputId": "d2679f85-3fa7-418b-cb07-abb5c7d34ac3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The archive with RAVDESS audio files is copied\n",
      "The archive with RAVDESS audio files is unpacked\n"
     ]
    }
   ],
   "source": [
    "# process the dataset\n",
    "path_to_packed_dataset = os.path.join('/content/drive/My Drive/datasets', datasets_info['Archive name'][dataset_type])\n",
    "target_path = os.path.join('/content', datasets_info['Name'][dataset_type])\n",
    "\n",
    "if not os.path.isdir(target_path):\n",
    "    os.mkdir(target_path)\n",
    "\n",
    "path_to_packed_dataset = shutil.copy2(path_to_packed_dataset, target_path)\n",
    "\n",
    "print('The archive with {} audio files is copied'.format(datasets_info['Name'][dataset_type]))\n",
    "\n",
    "with tarfile.open(path_to_packed_dataset) as tar:\n",
    "    tar.extractall(target_path)\n",
    "\n",
    "print('The archive with {} audio files is unpacked'.format(datasets_info['Name'][dataset_type]))################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Hyjr0Ns8vOq"
   },
   "outputs": [],
   "source": [
    "def get_paths_to_wavs(path_to_dataset):\n",
    "    file_paths_list = []\n",
    "\n",
    "    for root, dirs, files in os.walk(path_to_dataset):\n",
    "        if len(files) != 0:\n",
    "            file_paths_list += [os.path.join(root, f) for f in files if f.endswith('.wav')]\n",
    "\n",
    "    return file_paths_list\n",
    "\n",
    "def get_paths_to_npys(path_to_dataset):\n",
    "    # get a list with all absolute paths to each file\n",
    "    file_paths_list = []\n",
    "\n",
    "    for root, dirs, files in os.walk(path_to_dataset):\n",
    "        if len(files) != 0:\n",
    "            file_paths_list += [os.path.join(root, f) for f in files if f.endswith('.npy')]\n",
    "            #file_paths_list += [os.path.join(root, f) for f in files if os.path.isdir(os.path.join(root, f))]\n",
    "\n",
    "    return file_paths_list\n",
    "\n",
    "class numpy_ravdess_dataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Due to librosa reads wav-files very slow it is more preferable to read the\n",
    "    numpy representations of the original wavs\n",
    "    '''\n",
    "\n",
    "    emotion_dict = {\n",
    "        0: 'neutral',\n",
    "        1: 'calm',\n",
    "        2: 'happy',\n",
    "        3: 'sad',\n",
    "        4: 'angry',\n",
    "        5: 'fearful',\n",
    "        6: 'disgust',\n",
    "        7: 'surprised'\n",
    "        }\n",
    "\n",
    "    def __init__(self, paths_to_wavs_list, spectrogram_shape):\n",
    "        super(numpy_ravdess_dataset, self).__init__()\n",
    "\n",
    "        self.paths_to_wavs_list = paths_to_wavs_list\n",
    "\n",
    "        self.mfcc_rows = spectrogram_shape[0]\n",
    "        self.mfcc_cols = spectrogram_shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths_to_wavs_list)\n",
    "\n",
    "    def read_audio(self, path_to_wav):\n",
    "        return np.load(path_to_wav, allow_pickle=True)\n",
    "\n",
    "    def get_class_label(self, path_to_file):\n",
    "        # Parse the filename, which has the following pattern:\n",
    "        # modality-vocal_channel-emotion-intensity-statement-repetition-actor.wav\n",
    "        # e.g., '02-01-06-01-02-01-12.wav'\n",
    "        file_name = os.path.split(path_to_file)[1]\n",
    "        file_name = file_name[:-4]\n",
    "        class_label = int(file_name.split('-')[2]) - 1 # 2 is a number of emotion code\n",
    "        return class_label\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path_to_wav = self.paths_to_wavs_list[idx]\n",
    "        # debug\n",
    "        #print(path_to_wav)\n",
    "\n",
    "        # read the wav file\n",
    "        wav, sr = self.read_audio(path_to_wav)\n",
    "        #wav, sr = librosa.load(path_to_wav)\n",
    "\n",
    "        # get mfcc coefficients\n",
    "        mfccs = librosa.feature.mfcc(wav, sr=sr, n_mfcc=self.mfcc_rows).astype(np.float32)\n",
    "\n",
    "        actual_mfcc_cols = mfccs.shape[1]\n",
    "\n",
    "        # prmitive time-shifting augmentation\n",
    "        target_real_diff = actual_mfcc_cols - self.mfcc_cols\n",
    "        if target_real_diff > 0:\n",
    "            beginning_col = np.random.randint(target_real_diff)\n",
    "            mfccs = mfccs[..., beginning_col:beginning_col + self.mfcc_cols]\n",
    "        elif target_real_diff < 0:\n",
    "            mfccs = np.pad(mfccs, ((0, 0), (0, np.abs(target_real_diff))), constant_values=(0), mode='constant')\n",
    "\n",
    "        # make the data compatible to pytorch 1-channel CNNs format\n",
    "        mfccs = np.expand_dims(mfccs, axis=0)\n",
    "\n",
    "        # Parse the filename, which has the following pattern:\n",
    "        # modality-vocal_channel-emotion-intensity-statement-repetition-actor.wav\n",
    "        # e.g., '02-01-06-01-02-01-12.wav'\n",
    "        #file_name = os.path.split(path_to_wav)[1]\n",
    "        #file_name = file_name[:-4]\n",
    "        #class_label = int(file_name.split('-')[2]) - 1 # 2 is a number of emotion code\n",
    "        #class_label = np.array(class_label)\n",
    "        class_label = self.get_class_label(path_to_wav)\n",
    "\n",
    "        return torch.from_numpy(mfccs), class_label\n",
    "\n",
    "class numpy_crema_dataset(numpy_ravdess_dataset):\n",
    "    emotions_dict = {\n",
    "        'ANG': 0,\n",
    "        'DIS': 1,\n",
    "        'FEA': 2,\n",
    "        'SAD': 3,\n",
    "        'HAP': 4,\n",
    "        'NEU': 5\n",
    "    }\n",
    "\n",
    "    label2str = {\n",
    "        0: 'ANG',\n",
    "        1: 'DIS',\n",
    "        2: 'FEA',\n",
    "        3: 'SAD',\n",
    "        4: 'HAP',\n",
    "        5: 'NEU'\n",
    "    }\n",
    "    \n",
    "    def get_class_label(self, path_to_file):\n",
    "        file_name = os.path.split(path_to_file)[1]\n",
    "        file_name = file_name[:-4]\n",
    "        emotion_name = file_name.split('_')[2] # 2 is a number of emotion code\n",
    "        return self.emotions_dict[emotion_name]\n",
    "\n",
    "class numpy_iemocap_dataset(numpy_ravdess_dataset):\n",
    "    emotions_dict = {\n",
    "        'exc': 0,\n",
    "        'sad': 1,\n",
    "        'fru': 2,\n",
    "        'hap': 3,\n",
    "        'neu': 4,\n",
    "        'sur': 5,\n",
    "        'ang': 6,\n",
    "        'fea': 7,\n",
    "        'dis': 8,\n",
    "        #'oth': 9\n",
    "    }\n",
    "\n",
    "    def get_class_label(self, path_to_file):\n",
    "        file_name = os.path.split(path_to_file)[1]\n",
    "        file_name = file_name[:-4]\n",
    "        emotion_name = file_name.split('_')[-1] # the last is a position of emotion code\n",
    "        return self.emotions_dict[emotion_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9h5vcEAcHzmo"
   },
   "outputs": [],
   "source": [
    "# define a class that describes an audio CNN\n",
    "class audio_cnn(nn.Module):\n",
    "    def __init__(self, rows, cols, num_classes):\n",
    "        super(audio_cnn, self).__init__()\n",
    "\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.conv_extractor = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3,3), padding=1),\n",
    "            nn.BatchNorm2d(num_features=32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2,2)),\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3,3), padding=1),\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2,2)),\n",
    "            \n",
    "            nn.Conv2d(in_channels=64, out_channels=128,kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(num_features=128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2,2)),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=128,kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(num_features=128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2,2)),\n",
    "            \n",
    "            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=(1,1)),\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(in_features=2048, out_features=512),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(in_features=512, out_features=128),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(in_features=128, out_features=32),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=32, out_features=num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_extractor(x)\n",
    "        #print(x.shape, end='\\n\\n')\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        #print(x.shape, end='\\n\\n')\n",
    "        x = self.fc1(x)\n",
    "        #print(x.shape, end='\\n\\n')\n",
    "        x = self.fc2(x)\n",
    "        #print(x.shape, end='\\n\\n')\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def index_dataset(paths_list):\n",
    "    freq_dict = {}\n",
    "    for path in paths_list:\n",
    "        file_name = os.path.split(path)[1]\n",
    "        file_name = file_name[:-4]\n",
    "        emotion_name = file_name.split('_')[2] # 2 is a number of emotion code\n",
    "        try:\n",
    "            freq_dict[emotion_name] += 1\n",
    "        except KeyError:\n",
    "            freq_dict[emotion_name] = 1\n",
    "\n",
    "    for key in feq_dict:\n",
    "        freq_dict[key] /= len(paths_list)\n",
    "    return freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "28tI9u1X94I6"
   },
   "outputs": [],
   "source": [
    "def validate(model, criterion, testloader, device):\n",
    "\n",
    "    dataset_size = len(testloader.dataset)  \n",
    "        \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for i, (data, target) in enumerate(testloader):\n",
    "        t0 = time.time()\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # run forward step\n",
    "            predicted = model(data)\n",
    "\n",
    "            loss = criterion(predicted, target)\n",
    "\n",
    "            epoch_loss += loss.item() * data.size(0)\n",
    "\n",
    "        _, pred_labels = torch.max(predicted.data, 1)\n",
    "\n",
    "        total += target.size(0)\n",
    "        correct += (pred_labels == target).sum().item()\n",
    "\n",
    "\n",
    "    return epoch_loss/dataset_size, correct/total\n",
    "\n",
    "\n",
    "def train_num_epochs(model, trainloader, testloader, device, criterion, optimizer, starting_epoch, ending_epoch):\n",
    "    '''\n",
    "    model - neural network\n",
    "    trainloader - pytorch dataloader for training set\n",
    "    testloader - pytorch dataloader for test set\n",
    "    device - cpu / cuda\n",
    "    criterion - loss function (nn.CrossEntropyLoss())\n",
    "    optimizer - (Adam)\n",
    "    starting_epoch - \n",
    "    ending_epoch - \n",
    "    '''\n",
    "    dataset_size = len(trainloader.dataset)  \n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # iterate over epochs\n",
    "    for epoch_num in range(starting_epoch, ending_epoch):\n",
    "        print('Epoch #%d' % (epoch_num))\n",
    "\n",
    "        # iterate over batches\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        t = 0.0\n",
    "\n",
    "        for i, (data, target) in enumerate(trainloader):\n",
    "            t0 = time.time()\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # zero all the gradient tensors\n",
    "            optimizer.zero_grad()\n",
    "            # run forward step\n",
    "            predicted = model(data)\n",
    "\n",
    "            # compute loss\n",
    "            loss = criterion(predicted, target)\n",
    "\n",
    "            # compute gradient tensors\n",
    "            loss.backward()\n",
    "\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # compute the loss value\n",
    "            epoch_loss += loss.item() * data.size(0)\n",
    "\n",
    "            t += time.time() - t0\n",
    "            \n",
    "            total += target.size(0)\n",
    "            _, pred_labels = torch.max(predicted.data, 1)\n",
    "\n",
    "            correct += (pred_labels == target).sum().item()\n",
    "            \n",
    "        \n",
    "        epoch_loss /=  dataset_size\n",
    "        print('# Time passed: %.0f s' % (t))\n",
    "        print('# Epoch loss = %.4f' % (epoch_loss))\n",
    "        print('# Train acc = {}'.format(correct/total))\n",
    "        print('# Validation process on validation set')\n",
    "        val_loss, val_acc = validate(model, criterion, testloader, device)\n",
    "        print('# Validation loss = {}'.format(val_loss))\n",
    "        print('# Validation acc = {}'.format(val_acc))\n",
    "\n",
    "    return model, val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ob2dqqq1Wahv",
    "outputId": "1f261fc5-7ef3-4bcf-b365-a7bd46a5bb33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7442\n",
      "{'FEA': 0.1707874227358237, 'DIS': 0.1707874227358237, 'ANG': 0.1707874227358237, 'HAP': 0.1707874227358237, 'SAD': 0.1707874227358237, 'NEU': 0.1460628863208815}\n",
      "{'FEA': 0.17436586594994122, 'DIS': 0.1723500755921384, 'ANG': 0.17201411053250462, 'HAP': 0.16882244246598355, 'SAD': 0.16882244246598355, 'NEU': 0.14362506299344868}\n",
      "{'NEU': 0.15580926796507724, 'FEA': 0.15648085963734049, 'DIS': 0.16453995970449967, 'SAD': 0.1786433848220282, 'HAP': 0.1786433848220282, 'ANG': 0.16588314304902618}\n"
     ]
    }
   ],
   "source": [
    "# prepare dataloaders\n",
    "if dataset_type == 'C':\n",
    "    target_path = '/media/mikhail/files/datasets/emotion_recognition/CREMA-D/audio_npy'\n",
    "elif dataset_type == 'R':\n",
    "    target_path = '/media/mikhail/files/datasets/emotion_recognition/RAVDESS/npy'\n",
    "elif dataset_type == 'I':\n",
    "    target_path = ''\n",
    "npys_list = get_paths_to_npys(target_path)\n",
    "\n",
    "# shuffle the dataset to for the learning process stability\n",
    "random.seed(0)\n",
    "random.shuffle(npys_list)\n",
    "\n",
    "dataset_size = len(npys_list)\n",
    "\n",
    "print(dataset_size)\n",
    "\n",
    "# index dataset\n",
    "print(index_dataset(npys_list))\n",
    "print(index_dataset(npys_list[:train_size]))\n",
    "print(index_dataset(npys_list[train_size:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FEA': 0.1707874227358237,\n",
       " 'DIS': 0.1707874227358237,\n",
       " 'ANG': 0.1707874227358237,\n",
       " 'HAP': 0.1707874227358237,\n",
       " 'SAD': 0.1707874227358237,\n",
       " 'NEU': 0.1460628863208815}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_dataset(npys_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HAP'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BwBxhybY4O00",
    "outputId": "29409872-d60a-455a-bea5-f723a3753787"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 1, 3, 2]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = [1, 2, 3, 4]\n",
    "random.seed(1)\n",
    "random.shuffle(lst)\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lzR-md_M4FWu"
   },
   "outputs": [],
   "source": [
    "# get the size of the training set\n",
    "train_size = int(dataset_size * 0.8)\n",
    "\n",
    "# set up train dataset and train dataloader\n",
    "#\n",
    "# pick a dataset type\n",
    "if dataset_type == 'C': # CREMA-D case\n",
    "    numpy_audio_dataset = numpy_crema_dataset\n",
    "elif dataset_type == 'I':\n",
    "    numpy_audio_dataset = numpy_iemocap_dataset\n",
    "elif dataset_type == 'R':\n",
    "    numpy_audio_dataset = numpy_ravdess_dataset\n",
    "\n",
    "train_dataset = numpy_audio_dataset(npys_list[:train_size], (64, 128))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=256, shuffle=True, num_workers=4)\n",
    "\n",
    "# set up test dataset and test dataloader\n",
    "test_dataset = numpy_audio_dataset(npys_list[train_size:], (64, 128))\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=256, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "98UC6zRS7ila",
    "outputId": "cb78a362-6287-4e21-fdcc-2fc09787f4bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'neutral',\n",
       " 1: 'calm',\n",
       " 2: 'happy',\n",
       " 3: 'sad',\n",
       " 4: 'angry',\n",
       " 5: 'fearful',\n",
       " 6: 'disgust',\n",
       " 7: 'surprised'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.emotion_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "colab_type": "code",
    "id": "KaCGLhb_YbxO",
    "outputId": "570889d5-190a-48f4-a8a2-bbd29c4916bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 32, 64, 128]             320\n",
      "       BatchNorm2d-2          [-1, 32, 64, 128]              64\n",
      "              ReLU-3          [-1, 32, 64, 128]               0\n",
      "         MaxPool2d-4           [-1, 32, 32, 64]               0\n",
      "            Conv2d-5           [-1, 64, 32, 64]          18,496\n",
      "       BatchNorm2d-6           [-1, 64, 32, 64]             128\n",
      "              ReLU-7           [-1, 64, 32, 64]               0\n",
      "         MaxPool2d-8           [-1, 64, 16, 32]               0\n",
      "            Conv2d-9          [-1, 128, 16, 32]          73,856\n",
      "      BatchNorm2d-10          [-1, 128, 16, 32]             256\n",
      "             ReLU-11          [-1, 128, 16, 32]               0\n",
      "        MaxPool2d-12           [-1, 128, 8, 16]               0\n",
      "           Conv2d-13           [-1, 128, 8, 16]         147,584\n",
      "      BatchNorm2d-14           [-1, 128, 8, 16]             256\n",
      "             ReLU-15           [-1, 128, 8, 16]               0\n",
      "        MaxPool2d-16            [-1, 128, 4, 8]               0\n",
      "           Conv2d-17             [-1, 64, 4, 8]           8,256\n",
      "      BatchNorm2d-18             [-1, 64, 4, 8]             128\n",
      "             ReLU-19             [-1, 64, 4, 8]               0\n",
      "           Linear-20                  [-1, 512]       1,049,088\n",
      "          Dropout-21                  [-1, 512]               0\n",
      "             ReLU-22                  [-1, 512]               0\n",
      "           Linear-23                  [-1, 128]          65,664\n",
      "          Dropout-24                  [-1, 128]               0\n",
      "             ReLU-25                  [-1, 128]               0\n",
      "           Linear-26                   [-1, 32]           4,128\n",
      "          Dropout-27                   [-1, 32]               0\n",
      "             ReLU-28                   [-1, 32]               0\n",
      "           Linear-29                    [-1, 8]             264\n",
      "================================================================\n",
      "Total params: 1,368,488\n",
      "Trainable params: 1,368,488\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.03\n",
      "Forward/backward pass size (MB): 11.84\n",
      "Params size (MB): 5.22\n",
      "Estimated Total Size (MB): 17.10\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'CREMA-D_mfcc_emotion_cnn'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set-up devices\n",
    "cuda = torch.device('cuda:1')\n",
    "cpu = torch.device('cpu')\n",
    "\n",
    "mfcc_emotion_cnn = audio_cnn(rows=64, cols=128, num_classes=len(train_dataset.emotion_dict))\n",
    "\n",
    "summary(mfcc_emotion_cnn, input_size=(1, 64, 128), device='cpu')\n",
    "\n",
    "mfcc_emotion_cnn.to(cuda)\n",
    "\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "# define an optimization algorithm and bind it with the NN parameters\n",
    "optimizer = torch.optim.Adam(params=mfcc_emotion_cnn.parameters())\n",
    "\n",
    "starting_epoch = 0\n",
    "ending_epoch = 300\n",
    "epoch_step = 300\n",
    "\n",
    "path_to_weights = 'weights'\n",
    "\n",
    "if not os.path.isdir(path_to_weights):\n",
    "    os.mkdir(path_to_weights)\n",
    "\n",
    "basic_name = '{}_mfcc_emotion_cnn'.format(datasets_info['Name'][dataset_type])\n",
    "\n",
    "basic_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "colab_type": "code",
    "id": "x5oRQpHZY0fF",
    "outputId": "68c28915-6949-48fa-c6de-c71148b5e40c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################################################################\n",
      "#\tEpoch number is 0\n",
      "###################################################################\n",
      "Epoch #0\n",
      "# Time passed: 3 s\n",
      "# Epoch loss = 1.9379\n",
      "# Train acc = 0.21736939358306737\n",
      "# Validation process on validation set\n",
      "# Validation loss = 1.6491444700912312\n",
      "# Validation acc = 0.3498992612491605\n",
      "Epoch #1\n",
      "# Time passed: 3 s\n",
      "# Epoch loss = 1.7090\n",
      "# Train acc = 0.2592810347723837\n",
      "# Validation process on validation set\n",
      "# Validation loss = 1.5629244825838395\n",
      "# Validation acc = 0.38146406984553394\n",
      "Epoch #2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-67f4d3d4e6e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                                            \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                                            \u001b[0mstarting_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                                                            ending_epoch=epoch+epoch_step)\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbasic_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_ep-{}_loss-{:.3}_acc-{:.3}.pth'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepoch_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-258714ee207b>\u001b[0m in \u001b[0;36mtrain_num_epochs\u001b[0;34m(model, trainloader, testloader, device, criterion, optimizer, starting_epoch, ending_epoch)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python_programming/pytorch_projects/pytorch1.4-env/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python_programming/pytorch_projects/pytorch1.4-env/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python_programming/pytorch_projects/pytorch1.4-env/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python_programming/pytorch_projects/pytorch1.4-env/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "min_val_acc = 0\n",
    "for epoch in range(starting_epoch, ending_epoch, epoch_step):\n",
    "    print('###################################################################')\n",
    "    print('#\\tEpoch number is %d' % (epoch))\n",
    "    print('###################################################################')\n",
    "    mfcc_emotion_cnn, val_loss, val_acc = train_num_epochs(model=mfcc_emotion_cnn,\n",
    "                                                           trainloader=train_dataloader,\n",
    "                                                           testloader=test_dataloader,\n",
    "                                                           device=cuda,\n",
    "                                                           criterion=cross_entropy,\n",
    "                                                           optimizer=optimizer,\n",
    "                                                           starting_epoch=epoch,\n",
    "                                                           ending_epoch=epoch+epoch_step)\n",
    "                                                           \n",
    "\n",
    "    if val_acc > min_val_acc:\n",
    "        min_val_acc = val_acc\n",
    "        model_name = basic_name + '_ep-{}_loss-{:.3}_acc-{:.3}.pth'.format(epoch + epoch_step, val_loss, val_acc)\n",
    "        path_to_saving_model = os.path.join(path_to_weights, model_name)\n",
    "        torch.save(mfcc_emotion_cnn.state_dict(), path_to_saving_model)\n",
    "        print('model %s have been saved' % (path_to_saving_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "audio_cnns.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
