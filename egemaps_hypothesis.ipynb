{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# В этом ноутбуке эксперименты, связанные с инъекцией вектора eGeMAPS в полносвязный слой нейронной сети. <br>\n",
    "### Гипотеза: это поможет улучшить результат по сравнению с только лишь спектрограммами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.models_one_task_egemaps import AlexNetEgemaps2048, AlexNetEgemaps1792\n",
    "from  datasets.iemocap import IemocapDataset, train_test_loaders\n",
    "from constants import *\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import skorch\n",
    "from skorch.helper import predefined_split\n",
    "from skorch.dataset import Dataset\n",
    "from skorch.classifier import NeuralNetClassifier\n",
    "import skorch.callbacks as callbacks\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "from IPython.display import display\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель \n",
    "Используется AlexNet, как модель, показавшая один из наилучших результатов. Позже также проведу этот эксперимент на VGG-11. Отобразим результаты этих моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result</th>\n",
       "      <th>epochs</th>\n",
       "      <th>subset</th>\n",
       "      <th>metric</th>\n",
       "      <th>model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>augmentation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12731</th>\n",
       "      <td>0.701568</td>\n",
       "      <td>172</td>\n",
       "      <td>train</td>\n",
       "      <td>acc</td>\n",
       "      <td>AlexNet</td>\n",
       "      <td>IEMOCAP-4</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12909</th>\n",
       "      <td>0.683091</td>\n",
       "      <td>172</td>\n",
       "      <td>valid</td>\n",
       "      <td>acc</td>\n",
       "      <td>AlexNet</td>\n",
       "      <td>IEMOCAP-4</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13087</th>\n",
       "      <td>0.741412</td>\n",
       "      <td>172</td>\n",
       "      <td>train</td>\n",
       "      <td>loss</td>\n",
       "      <td>AlexNet</td>\n",
       "      <td>IEMOCAP-4</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13265</th>\n",
       "      <td>0.808721</td>\n",
       "      <td>172</td>\n",
       "      <td>valid</td>\n",
       "      <td>loss</td>\n",
       "      <td>AlexNet</td>\n",
       "      <td>IEMOCAP-4</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         result  epochs subset metric    model    dataset  preprocessing  \\\n",
       "12731  0.701568     172  train    acc  AlexNet  IEMOCAP-4          False   \n",
       "12909  0.683091     172  valid    acc  AlexNet  IEMOCAP-4          False   \n",
       "13087  0.741412     172  train   loss  AlexNet  IEMOCAP-4          False   \n",
       "13265  0.808721     172  valid   loss  AlexNet  IEMOCAP-4          False   \n",
       "\n",
       "       augmentation  \n",
       "12731          True  \n",
       "12909          True  \n",
       "13087          True  \n",
       "13265          True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result</th>\n",
       "      <th>epochs</th>\n",
       "      <th>subset</th>\n",
       "      <th>metric</th>\n",
       "      <th>model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>augmentation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6037</th>\n",
       "      <td>0.739362</td>\n",
       "      <td>210</td>\n",
       "      <td>train</td>\n",
       "      <td>acc</td>\n",
       "      <td>VggNet</td>\n",
       "      <td>IEMOCAP-4</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6283</th>\n",
       "      <td>0.683091</td>\n",
       "      <td>210</td>\n",
       "      <td>valid</td>\n",
       "      <td>acc</td>\n",
       "      <td>VggNet</td>\n",
       "      <td>IEMOCAP-4</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6529</th>\n",
       "      <td>0.671230</td>\n",
       "      <td>210</td>\n",
       "      <td>train</td>\n",
       "      <td>loss</td>\n",
       "      <td>VggNet</td>\n",
       "      <td>IEMOCAP-4</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6775</th>\n",
       "      <td>0.794945</td>\n",
       "      <td>210</td>\n",
       "      <td>valid</td>\n",
       "      <td>loss</td>\n",
       "      <td>VggNet</td>\n",
       "      <td>IEMOCAP-4</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        result  epochs subset metric   model    dataset  preprocessing  \\\n",
       "6037  0.739362     210  train    acc  VggNet  IEMOCAP-4          False   \n",
       "6283  0.683091     210  valid    acc  VggNet  IEMOCAP-4          False   \n",
       "6529  0.671230     210  train   loss  VggNet  IEMOCAP-4          False   \n",
       "6775  0.794945     210  valid   loss  VggNet  IEMOCAP-4          False   \n",
       "\n",
       "      augmentation  \n",
       "6037          True  \n",
       "6283          True  \n",
       "6529          True  \n",
       "6775          True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(RESULTS_FOLDER, 'iemocap_all_results.csv'), delimiter=';')\n",
    "alex_results = df.loc[(df['model'] == 'AlexNet') & (df['preprocessing'] == False) & (df['augmentation'] == True) ]\n",
    "display(alex_results.loc[alex_results['epochs'] == 172])\n",
    "vgg_results = df.loc[(df['model'] == 'VggNet') & (df['preprocessing'] == False) & (df['augmentation'] == True)]\n",
    "display(vgg_results.loc[vgg_results['epochs'] == 210])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вопрос: куда мы будем конкатенировать eGeMAPS? \n",
    "У модели AlexNet три полносвязных слоя: 6400 нейронов, 2048 нейронов, 512 нейронов, 4 нейрона (классы) <br>\n",
    "Для начала я попробую конкатенировать слой с 2048 нейронов. Есть и другие варианты, но начнем с простого.<br>\n",
    "Этот слой будет подвергнут нормализации (torch.nn.LayerNorm). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Конкатенация со слоем 2048 нейронов, LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNetEgemaps2048(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(5, 5))\n",
      "  (fc1): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=6400, out_features=2048, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (ln1): LayerNorm((2136,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc2): Sequential(\n",
      "    (0): Linear(in_features=2136, out_features=512, bias=True)\n",
      "    (1): Dropout(p=0.75, inplace=False)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AlexNetEgemaps2048(num_classes=4)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Датасет\n",
    "Используется IEMOCAP без препроцессинга и без аугментацией данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= INITIALIZING DATASET IEMOCAP-4_four_prep-false_224_train ===============\n",
      "=========================== SUCCESS! ====================================\n",
      "============= INITIALIZING DATASET IEMOCAP-4_four_prep-false_224_test ===============\n",
      "=========================== SUCCESS! ====================================\n"
     ]
    }
   ],
   "source": [
    "train_ds = IemocapDataset( \n",
    "    PATH_TO_PICKLE, IEMOCAP_PATH_TO_WAVS, IEMOCAP_PATH_TO_EGEMAPS, IEMOCAP_PATH_FOR_PARSER, \n",
    "    base_name='IEMOCAP-4', label_type='four', mode='train', preprocessing=False, \n",
    "    augmentation=False, padding='repeat', spectrogram_shape=224, spectrogram_type='melspec', tasks='emotion', egemaps=True \n",
    ")\n",
    "valid_ds = IemocapDataset(  \n",
    "    PATH_TO_PICKLE, IEMOCAP_PATH_TO_WAVS, IEMOCAP_PATH_TO_EGEMAPS, IEMOCAP_PATH_FOR_PARSER, \n",
    "    base_name='IEMOCAP-4', label_type='four', mode='test', preprocessing=False, \n",
    "    augmentation=False, padding='repeat', spectrogram_shape=224, spectrogram_type='melspec', tasks='emotion', egemaps=True \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение\n",
    "Будем обучать с помощью Skorch, используем оптимизатор Adam, learning rate 1e-5, 300 эпох (сохраняем все гиперпараметры такими же, как и у бейслайна)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'AlexNetEgemaps2048LN--{}_augmentation-{}.md'.format(train_ds.name, str(train_ds.augmentation).lower())\n",
    "best_model_file_path = os.path.join(RESULTS_FOLDER, filename)\n",
    "callback_train_acc = callbacks.EpochScoring(scoring=\"accuracy\", \n",
    "                                            lower_is_better=False, \n",
    "                                            on_train=True, \n",
    "                                            name='train_acc')\n",
    "callback_save_best = callbacks.Checkpoint(monitor='valid_loss_best', \n",
    "                                          f_params=None, \n",
    "                                          f_optimizer=None, \n",
    "                                          f_criterion=None, \n",
    "                                          f_history=None, \n",
    "                                          f_pickle=best_model_file_path,  \n",
    "                                          event_name='event_cp')\n",
    "callback_early_stop = callbacks.EarlyStopping(monitor='valid_loss', patience=30, \n",
    "                                              threshold_mode='rel', lower_is_better=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = skorch.classifier.NeuralNetClassifier(\n",
    "    model, criterion=nn.CrossEntropyLoss, optimizer=torch.optim.Adam,\n",
    "    lr=1e-5, max_epochs=300, batch_size=32, train_split=predefined_split(valid_ds), \n",
    "    device=device, iterator_train__shuffle=True, \n",
    "    callbacks=[\n",
    "        callback_train_acc,\n",
    "        callback_save_best,\n",
    "        callback_early_stop\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_acc    train_loss    valid_acc    valid_loss    cp      dur\n",
      "-------  -----------  ------------  -----------  ------------  ----  -------\n",
      "      1       \u001b[36m0.3519\u001b[0m        \u001b[32m1.3542\u001b[0m       \u001b[35m0.4211\u001b[0m        \u001b[31m1.2350\u001b[0m     +  18.6267\n",
      "      2       \u001b[36m0.4516\u001b[0m        \u001b[32m1.2440\u001b[0m       \u001b[35m0.5185\u001b[0m        \u001b[31m1.1522\u001b[0m     +  18.3658\n",
      "      3       \u001b[36m0.5003\u001b[0m        \u001b[32m1.1453\u001b[0m       \u001b[35m0.5745\u001b[0m        \u001b[31m1.0373\u001b[0m     +  18.2437\n",
      "      4       \u001b[36m0.5428\u001b[0m        \u001b[32m1.0839\u001b[0m       \u001b[35m0.5823\u001b[0m        \u001b[31m1.0026\u001b[0m     +  18.2562\n",
      "      5       \u001b[36m0.5622\u001b[0m        \u001b[32m1.0618\u001b[0m       0.5633        1.0266        18.3714\n",
      "      6       0.5608        \u001b[32m1.0459\u001b[0m       \u001b[35m0.5857\u001b[0m        \u001b[31m0.9779\u001b[0m     +  18.3271\n",
      "      7       \u001b[36m0.5719\u001b[0m        \u001b[32m1.0378\u001b[0m       0.5845        0.9885        18.5022\n",
      "      8       \u001b[36m0.5767\u001b[0m        \u001b[32m1.0237\u001b[0m       \u001b[35m0.6114\u001b[0m        \u001b[31m0.9656\u001b[0m     +  18.4624\n",
      "      9       \u001b[36m0.5885\u001b[0m        \u001b[32m1.0165\u001b[0m       0.6025        \u001b[31m0.9581\u001b[0m     +  18.4088\n",
      "     10       \u001b[36m0.5938\u001b[0m        \u001b[32m1.0079\u001b[0m       0.6036        0.9583        18.5729\n",
      "     11       0.5851        \u001b[32m1.0004\u001b[0m       0.6013        \u001b[31m0.9516\u001b[0m     +  18.4611\n",
      "     12       \u001b[36m0.5946\u001b[0m        \u001b[32m0.9906\u001b[0m       \u001b[35m0.6181\u001b[0m        \u001b[31m0.9357\u001b[0m     +  18.4226\n",
      "     13       0.5943        \u001b[32m0.9813\u001b[0m       0.5957        0.9777        18.2853\n",
      "     14       \u001b[36m0.5963\u001b[0m        \u001b[32m0.9789\u001b[0m       \u001b[35m0.6237\u001b[0m        \u001b[31m0.9332\u001b[0m     +  18.4459\n",
      "     15       \u001b[36m0.6036\u001b[0m        \u001b[32m0.9669\u001b[0m       \u001b[35m0.6249\u001b[0m        \u001b[31m0.9318\u001b[0m     +  18.5215\n",
      "     16       \u001b[36m0.6081\u001b[0m        0.9672       0.6036        0.9396        18.5639\n",
      "     17       0.6061        0.9708       0.6081        0.9326        18.4407\n",
      "     18       \u001b[36m0.6167\u001b[0m        \u001b[32m0.9524\u001b[0m       0.6069        0.9430        18.4817\n",
      "     19       0.6128        0.9576       0.6137        \u001b[31m0.9248\u001b[0m     +  18.4410\n",
      "     20       \u001b[36m0.6271\u001b[0m        \u001b[32m0.9442\u001b[0m       \u001b[35m0.6305\u001b[0m        \u001b[31m0.9136\u001b[0m     +  18.4551\n",
      "     21       0.6207        \u001b[32m0.9373\u001b[0m       0.6293        0.9259        18.3079\n",
      "     22       0.6198        \u001b[32m0.9369\u001b[0m       0.6193        \u001b[31m0.9098\u001b[0m     +  18.3956\n",
      "     23       0.6187        0.9430       0.6237        0.9322        18.5455\n",
      "     24       0.6254        0.9378       \u001b[35m0.6316\u001b[0m        \u001b[31m0.9071\u001b[0m     +  18.5233\n",
      "     25       \u001b[36m0.6291\u001b[0m        \u001b[32m0.9314\u001b[0m       \u001b[35m0.6394\u001b[0m        \u001b[31m0.9007\u001b[0m     +  18.5221\n",
      "     26       0.6268        \u001b[32m0.9249\u001b[0m       0.6215        0.9225        18.4021\n",
      "     27       \u001b[36m0.6352\u001b[0m        \u001b[32m0.9132\u001b[0m       0.6372        \u001b[31m0.8916\u001b[0m     +  18.4092\n",
      "     28       0.6321        0.9207       0.6361        \u001b[31m0.8881\u001b[0m     +  18.3665\n",
      "     29       \u001b[36m0.6417\u001b[0m        \u001b[32m0.8948\u001b[0m       0.6383        \u001b[31m0.8865\u001b[0m     +  18.4105\n",
      "     30       0.6411        0.8997       0.6316        0.8999        18.3680\n",
      "     31       0.6375        0.8984       0.6361        \u001b[31m0.8841\u001b[0m     +  18.3712\n",
      "     32       \u001b[36m0.6475\u001b[0m        \u001b[32m0.8895\u001b[0m       0.6372        \u001b[31m0.8786\u001b[0m     +  18.4081\n",
      "     33       0.6391        0.8957       \u001b[35m0.6439\u001b[0m        \u001b[31m0.8749\u001b[0m     +  18.3530\n",
      "     34       \u001b[36m0.6489\u001b[0m        0.8934       \u001b[35m0.6506\u001b[0m        \u001b[31m0.8696\u001b[0m     +  18.4080\n",
      "     35       0.6456        \u001b[32m0.8797\u001b[0m       0.6260        0.9195        18.5895\n",
      "     36       0.6439        0.8911       0.6293        0.9010        18.5095\n",
      "     37       0.6442        \u001b[32m0.8796\u001b[0m       \u001b[35m0.6517\u001b[0m        \u001b[31m0.8676\u001b[0m     +  18.4356\n",
      "     38       \u001b[36m0.6534\u001b[0m        \u001b[32m0.8700\u001b[0m       0.6506        \u001b[31m0.8648\u001b[0m     +  18.4274\n",
      "     39       \u001b[36m0.6610\u001b[0m        0.8705       0.6450        0.8706        18.2688\n",
      "     40       0.6512        0.8729       0.6473        0.8683        18.3818\n",
      "     41       0.6509        \u001b[32m0.8696\u001b[0m       0.6517        0.8680        18.5436\n",
      "     42       \u001b[36m0.6638\u001b[0m        \u001b[32m0.8582\u001b[0m       0.6383        0.8814        18.3002\n",
      "     43       0.6559        0.8723       \u001b[35m0.6540\u001b[0m        \u001b[31m0.8585\u001b[0m     +  18.3383\n",
      "     44       0.6627        \u001b[32m0.8435\u001b[0m       0.6316        0.9374        18.6191\n",
      "     45       0.6545        0.8606       0.6529        \u001b[31m0.8539\u001b[0m     +  18.4118\n",
      "     46       \u001b[36m0.6655\u001b[0m        0.8524       \u001b[35m0.6573\u001b[0m        0.8542        18.2758\n",
      "     47       \u001b[36m0.6657\u001b[0m        \u001b[32m0.8396\u001b[0m       0.6372        0.8912        18.4490\n",
      "     48       0.6646        0.8474       0.6473        0.8570        18.3780\n",
      "     49       0.6618        \u001b[32m0.8389\u001b[0m       \u001b[35m0.6596\u001b[0m        \u001b[31m0.8448\u001b[0m     +  18.5022\n",
      "     50       \u001b[36m0.6677\u001b[0m        \u001b[32m0.8309\u001b[0m       0.6450        0.8657        18.3826\n",
      "     51       0.6652        0.8327       0.6495        0.8525        18.4450\n",
      "     52       \u001b[36m0.6680\u001b[0m        0.8331       0.6596        0.8642        18.3836\n",
      "     53       \u001b[36m0.6753\u001b[0m        \u001b[32m0.8194\u001b[0m       0.6529        0.8575        18.3802\n",
      "     54       \u001b[36m0.6809\u001b[0m        \u001b[32m0.8090\u001b[0m       \u001b[35m0.6663\u001b[0m        \u001b[31m0.8400\u001b[0m     +  18.4370\n",
      "     55       0.6669        0.8196       0.6596        0.8530        18.3239\n",
      "     56       0.6795        0.8094       0.6562        0.8523        18.3701\n",
      "     57       \u001b[36m0.6848\u001b[0m        0.8113       0.6629        \u001b[31m0.8299\u001b[0m     +  18.3928\n",
      "     58       0.6834        \u001b[32m0.8057\u001b[0m       0.6596        \u001b[31m0.8296\u001b[0m     +  18.3627\n",
      "     59       0.6797        0.8061       0.6652        0.8321        18.3451\n",
      "     60       \u001b[36m0.6934\u001b[0m        \u001b[32m0.7949\u001b[0m       0.6652        0.8379        18.3116\n",
      "     61       0.6859        \u001b[32m0.7920\u001b[0m       0.6585        0.8648        18.5765\n",
      "     62       0.6876        \u001b[32m0.7883\u001b[0m       0.6562        \u001b[31m0.8238\u001b[0m     +  18.5426\n",
      "     63       0.6881        \u001b[32m0.7843\u001b[0m       0.6607        0.8391        18.5021\n",
      "     64       \u001b[36m0.6957\u001b[0m        \u001b[32m0.7688\u001b[0m       0.6652        0.8388        18.3556\n",
      "     65       0.6884        0.7833       0.6652        0.8304        18.4396\n",
      "     66       0.6834        0.7854       0.6652        0.8595        18.4585\n",
      "     67       0.6895        0.7796       0.6663        0.8375        18.2901\n",
      "     68       0.6943        \u001b[32m0.7568\u001b[0m       \u001b[35m0.6719\u001b[0m        0.8357        18.3710\n",
      "     69       \u001b[36m0.6982\u001b[0m        \u001b[32m0.7538\u001b[0m       0.6685        0.8331        18.4199\n",
      "     70       \u001b[36m0.7038\u001b[0m        0.7607       0.6708        \u001b[31m0.8163\u001b[0m     +  18.3564\n",
      "     71       \u001b[36m0.7074\u001b[0m        \u001b[32m0.7446\u001b[0m       \u001b[35m0.6764\u001b[0m        \u001b[31m0.8149\u001b[0m     +  18.3656\n",
      "     72       0.7035        0.7469       0.6652        0.8526        18.5389\n",
      "     73       \u001b[36m0.7091\u001b[0m        \u001b[32m0.7359\u001b[0m       \u001b[35m0.6809\u001b[0m        \u001b[31m0.8087\u001b[0m     +  18.4009\n",
      "     74       \u001b[36m0.7189\u001b[0m        \u001b[32m0.7290\u001b[0m       0.6607        0.8378        18.4107\n",
      "     75       0.7080        0.7448       0.6753        0.8140        18.4425\n",
      "     76       0.7136        \u001b[32m0.7164\u001b[0m       0.6730        0.8193        18.4909\n",
      "     77       \u001b[36m0.7195\u001b[0m        \u001b[32m0.7138\u001b[0m       0.6618        0.8668        18.3621\n",
      "     78       0.7150        \u001b[32m0.7089\u001b[0m       0.6741        0.8188        18.3108\n",
      "     79       \u001b[36m0.7307\u001b[0m        \u001b[32m0.6979\u001b[0m       0.6797        0.8301        18.4444\n",
      "     80       0.7231        0.7006       0.6551        0.8732        18.4555\n",
      "     81       \u001b[36m0.7329\u001b[0m        \u001b[32m0.6927\u001b[0m       0.6730        \u001b[31m0.8050\u001b[0m     +  18.4221\n",
      "     82       \u001b[36m0.7380\u001b[0m        \u001b[32m0.6789\u001b[0m       0.6697        0.8375        18.5125\n",
      "     83       0.7301        0.6831       0.6674        0.8389        18.4014\n",
      "     84       0.7374        \u001b[32m0.6765\u001b[0m       0.6764        0.8360        18.3736\n",
      "     85       0.7357        0.6864       0.6797        0.8105        18.5626\n",
      "     86       \u001b[36m0.7413\u001b[0m        \u001b[32m0.6748\u001b[0m       0.6697        0.8393        18.4762\n",
      "     87       0.7312        0.6913       \u001b[35m0.6842\u001b[0m        0.8073        18.5035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     88       \u001b[36m0.7427\u001b[0m        \u001b[32m0.6662\u001b[0m       0.6842        0.8163        18.4531\n",
      "     89       \u001b[36m0.7492\u001b[0m        \u001b[32m0.6466\u001b[0m       \u001b[35m0.6898\u001b[0m        0.8071        18.5458\n",
      "     90       0.7402        0.6645       0.6719        0.8440        18.4693\n",
      "     91       \u001b[36m0.7531\u001b[0m        \u001b[32m0.6443\u001b[0m       0.6741        0.8162        18.4278\n",
      "     92       \u001b[36m0.7623\u001b[0m        \u001b[32m0.6320\u001b[0m       0.6876        0.8144        18.3892\n",
      "     93       0.7595        \u001b[32m0.6264\u001b[0m       0.6775        0.8075        18.3617\n",
      "     94       0.7612        \u001b[32m0.6188\u001b[0m       0.6898        0.8254        18.3706\n",
      "     95       0.7618        \u001b[32m0.6184\u001b[0m       \u001b[35m0.6920\u001b[0m        0.8247        18.4175\n",
      "     96       0.7595        \u001b[32m0.6153\u001b[0m       0.6506        0.9086        18.2834\n",
      "     97       0.7592        0.6183       0.6909        0.8360        18.3771\n",
      "     98       \u001b[36m0.7696\u001b[0m        \u001b[32m0.6055\u001b[0m       0.6697        0.8883        18.6372\n",
      "     99       0.7679        \u001b[32m0.6021\u001b[0m       0.6898        0.8290        18.4940\n",
      "    100       \u001b[36m0.7858\u001b[0m        \u001b[32m0.5784\u001b[0m       0.6809        0.8426        18.5352\n",
      "    101       0.7772        0.5859       0.6887        0.8186        18.4489\n",
      "    102       0.7699        0.5822       0.6876        0.8293        18.5895\n",
      "    103       0.7774        0.5860       0.6786        0.8342        18.4621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=AlexNetEgemaps2048(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): ReLU(inplace=True)\n",
       "      (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): ReLU(inplace=True)\n",
       "      (10): Conv2d(256, 256, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(5, 5))\n",
       "    (fc1): Sequential(\n",
       "      (0): Dropout(p=0.5, inplace=False)\n",
       "      (1): Linear(in_features=6400, out_features=2048, bias=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (ln1): LayerNorm((2136,), eps=1e-05, elementwise_affine=True)\n",
       "    (fc2): Sequential(\n",
       "      (0): Linear(in_features=2136, out_features=512, bias=True)\n",
       "      (1): Dropout(p=0.75, inplace=False)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (classifier): Linear(in_features=512, out_features=4, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(train_ds, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2. Изменяем структуру модели: второй полносвязный слой делаем не 2048 нейронов, а 1792. eGeMAPS пропускаем через полносвязный слой 256 нейронов, и его конкатенируем со слоем с 1792 нейронов. <br>\n",
    "Эту модель протестим без LayerNorm и с LayerNorm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Без LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNetEgemaps1792(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(5, 5))\n",
      "  (spec_fc): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=6400, out_features=1792, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (egemaps_fc): Sequential(\n",
      "    (0): Linear(in_features=88, out_features=256, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (joint_fc): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): Dropout(p=0.75, inplace=False)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AlexNetEgemaps1792(4)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'AlexNetEgemaps1792--{}_augmentation-{}.md'.format(train_ds.name, str(train_ds.augmentation).lower())\n",
    "best_model_file_path = os.path.join(RESULTS_FOLDER, filename)\n",
    "callback_train_acc = callbacks.EpochScoring(scoring=\"accuracy\", \n",
    "                                            lower_is_better=False, \n",
    "                                            on_train=True, \n",
    "                                            name='train_acc')\n",
    "callback_save_best = callbacks.Checkpoint(monitor='valid_loss_best', \n",
    "                                          f_params=None, \n",
    "                                          f_optimizer=None, \n",
    "                                          f_criterion=None, \n",
    "                                          f_history=None, \n",
    "                                          f_pickle=best_model_file_path,  \n",
    "                                          event_name='event_cp')\n",
    "callback_early_stop = callbacks.EarlyStopping(monitor='valid_loss', patience=30, \n",
    "                                              threshold_mode='rel', lower_is_better=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = skorch.classifier.NeuralNetClassifier(\n",
    "    model, criterion=nn.CrossEntropyLoss, optimizer=torch.optim.Adam,\n",
    "    lr=1e-5, max_epochs=300, batch_size=32, train_split=predefined_split(valid_ds), \n",
    "    device=device, iterator_train__shuffle=True, \n",
    "    callbacks=[\n",
    "        callback_train_acc,\n",
    "        callback_save_best,\n",
    "        callback_early_stop\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_acc    train_loss    valid_acc    valid_loss    cp      dur\n",
      "-------  -----------  ------------  -----------  ------------  ----  -------\n",
      "      1       \u001b[36m0.3401\u001b[0m        \u001b[32m1.3461\u001b[0m       \u001b[35m0.3807\u001b[0m        \u001b[31m1.3145\u001b[0m     +  18.3485\n",
      "      2       \u001b[36m0.3788\u001b[0m        \u001b[32m1.3119\u001b[0m       0.3807        \u001b[31m1.2991\u001b[0m     +  18.2514\n",
      "      3       \u001b[36m0.4065\u001b[0m        \u001b[32m1.2798\u001b[0m       \u001b[35m0.4938\u001b[0m        \u001b[31m1.1809\u001b[0m     +  18.2304\n",
      "      4       \u001b[36m0.5176\u001b[0m        \u001b[32m1.1303\u001b[0m       \u001b[35m0.5442\u001b[0m        \u001b[31m1.0659\u001b[0m     +  18.2034\n",
      "      5       \u001b[36m0.5496\u001b[0m        \u001b[32m1.0674\u001b[0m       \u001b[35m0.5543\u001b[0m        \u001b[31m1.0241\u001b[0m     +  18.2525\n",
      "      6       \u001b[36m0.5605\u001b[0m        \u001b[32m1.0487\u001b[0m       \u001b[35m0.5677\u001b[0m        \u001b[31m1.0010\u001b[0m     +  18.3884\n",
      "      7       \u001b[36m0.5669\u001b[0m        \u001b[32m1.0409\u001b[0m       0.5610        1.0231        18.2834\n",
      "      8       \u001b[36m0.5798\u001b[0m        \u001b[32m1.0304\u001b[0m       \u001b[35m0.5857\u001b[0m        \u001b[31m0.9892\u001b[0m     +  18.2765\n",
      "      9       0.5728        \u001b[32m1.0296\u001b[0m       0.5476        1.0347        18.2134\n",
      "     10       0.5747        \u001b[32m1.0228\u001b[0m       0.5733        1.0048        18.2231\n",
      "     11       \u001b[36m0.5890\u001b[0m        \u001b[32m1.0113\u001b[0m       0.5689        0.9938        18.2409\n",
      "     12       0.5859        \u001b[32m1.0060\u001b[0m       \u001b[35m0.6013\u001b[0m        \u001b[31m0.9661\u001b[0m     +  18.2084\n",
      "     13       \u001b[36m0.5927\u001b[0m        \u001b[32m1.0027\u001b[0m       0.6002        0.9779        18.1787\n",
      "     14       0.5904        \u001b[32m0.9993\u001b[0m       \u001b[35m0.6025\u001b[0m        \u001b[31m0.9597\u001b[0m     +  18.2088\n",
      "     15       \u001b[36m0.5941\u001b[0m        \u001b[32m0.9843\u001b[0m       \u001b[35m0.6047\u001b[0m        \u001b[31m0.9534\u001b[0m     +  18.1540\n",
      "     16       \u001b[36m0.6053\u001b[0m        \u001b[32m0.9708\u001b[0m       \u001b[35m0.6081\u001b[0m        0.9562        18.1962\n",
      "     17       0.5938        0.9856       0.5969        \u001b[31m0.9517\u001b[0m     +  18.3149\n",
      "     18       0.6033        0.9791       0.6058        \u001b[31m0.9453\u001b[0m     +  18.3268\n",
      "     19       \u001b[36m0.6072\u001b[0m        \u001b[32m0.9698\u001b[0m       0.5980        0.9582        18.3152\n",
      "     20       \u001b[36m0.6106\u001b[0m        \u001b[32m0.9595\u001b[0m       \u001b[35m0.6092\u001b[0m        \u001b[31m0.9450\u001b[0m     +  18.3563\n",
      "     21       0.6083        0.9659       \u001b[35m0.6181\u001b[0m        \u001b[31m0.9333\u001b[0m     +  18.3691\n",
      "     22       0.6081        0.9613       0.6069        0.9368        18.4241\n",
      "     23       0.6069        0.9627       0.6069        0.9353        18.3532\n",
      "     24       \u001b[36m0.6198\u001b[0m        \u001b[32m0.9533\u001b[0m       0.6114        0.9375        18.2578\n",
      "     25       0.6170        0.9538       \u001b[35m0.6305\u001b[0m        0.9334        18.2201\n",
      "     26       0.6142        \u001b[32m0.9530\u001b[0m       0.6260        0.9547        18.2099\n",
      "     27       0.6187        0.9539       0.6204        \u001b[31m0.9295\u001b[0m     +  18.2472\n",
      "     28       0.6193        \u001b[32m0.9465\u001b[0m       0.6215        \u001b[31m0.9226\u001b[0m     +  18.1911\n",
      "     29       0.6179        \u001b[32m0.9443\u001b[0m       0.6271        \u001b[31m0.9214\u001b[0m     +  18.1072\n",
      "     30       \u001b[36m0.6265\u001b[0m        \u001b[32m0.9348\u001b[0m       0.6137        \u001b[31m0.9192\u001b[0m     +  18.1647\n",
      "     31       0.6151        0.9483       0.6249        \u001b[31m0.9164\u001b[0m     +  18.1868\n",
      "     32       \u001b[36m0.6358\u001b[0m        \u001b[32m0.9316\u001b[0m       0.6282        0.9199        18.3610\n",
      "     33       0.6187        0.9499       0.6305        0.9255        18.3852\n",
      "     34       0.6274        \u001b[32m0.9309\u001b[0m       0.6271        0.9253        18.2073\n",
      "     35       0.6316        \u001b[32m0.9185\u001b[0m       0.6293        \u001b[31m0.9115\u001b[0m     +  18.1625\n",
      "     36       0.6307        0.9244       \u001b[35m0.6316\u001b[0m        0.9222        18.2326\n",
      "     37       0.6282        0.9332       0.6260        0.9252        18.1878\n",
      "     38       0.6226        0.9254       0.6159        0.9186        18.2388\n",
      "     39       0.6341        0.9254       0.6293        \u001b[31m0.9038\u001b[0m     +  18.2490\n",
      "     40       0.6349        \u001b[32m0.9114\u001b[0m       0.6249        0.9042        18.1741\n",
      "     41       0.6288        0.9214       0.6193        0.9210        18.1803\n",
      "     42       0.6313        0.9163       0.6316        0.9226        18.2217\n",
      "     43       0.6282        0.9150       0.6237        0.9056        18.1363\n",
      "     44       0.6341        \u001b[32m0.9064\u001b[0m       \u001b[35m0.6405\u001b[0m        \u001b[31m0.9007\u001b[0m     +  18.2240\n",
      "     45       0.6335        \u001b[32m0.9044\u001b[0m       0.6271        0.9015        18.2019\n",
      "     46       0.6321        0.9093       0.6349        0.9057        18.2677\n",
      "     47       \u001b[36m0.6389\u001b[0m        \u001b[32m0.9043\u001b[0m       \u001b[35m0.6428\u001b[0m        0.9032        18.2262\n",
      "     48       0.6383        \u001b[32m0.9027\u001b[0m       0.6316        \u001b[31m0.8936\u001b[0m     +  18.2830\n",
      "     49       \u001b[36m0.6447\u001b[0m        \u001b[32m0.9019\u001b[0m       0.6260        0.9009        18.3515\n",
      "     50       0.6433        0.9043       0.6159        0.9127        18.3316\n",
      "     51       \u001b[36m0.6501\u001b[0m        \u001b[32m0.8987\u001b[0m       0.6271        0.9115        18.3076\n",
      "     52       0.6366        0.9037       0.6361        0.9038        18.3338\n",
      "     53       0.6439        \u001b[32m0.8970\u001b[0m       0.6293        \u001b[31m0.8926\u001b[0m     +  18.3140\n",
      "     54       0.6425        0.9008       0.6349        \u001b[31m0.8854\u001b[0m     +  18.3223\n",
      "     55       \u001b[36m0.6509\u001b[0m        \u001b[32m0.8842\u001b[0m       0.6394        \u001b[31m0.8839\u001b[0m     +  18.4257\n",
      "     56       0.6450        0.8884       \u001b[35m0.6450\u001b[0m        0.8882        18.2178\n",
      "     57       0.6422        0.8849       0.6316        0.8931        18.2737\n",
      "     58       \u001b[36m0.6540\u001b[0m        \u001b[32m0.8788\u001b[0m       0.6338        0.8876        18.3371\n",
      "     59       0.6447        0.8809       0.6338        0.8920        18.2568\n",
      "     60       0.6481        \u001b[32m0.8776\u001b[0m       \u001b[35m0.6495\u001b[0m        0.8874        18.2541\n",
      "     61       0.6456        0.8799       0.6383        \u001b[31m0.8798\u001b[0m     +  18.1707\n",
      "     62       0.6445        0.8820       0.6372        0.9010        18.1675\n",
      "     63       0.6520        \u001b[32m0.8759\u001b[0m       0.6349        0.9045        18.2264\n",
      "     64       0.6487        0.8762       0.6461        0.8906        18.2743\n",
      "     65       0.6534        \u001b[32m0.8692\u001b[0m       0.6327        0.8828        18.3480\n",
      "     66       0.6501        0.8763       0.6461        \u001b[31m0.8784\u001b[0m     +  18.3178\n",
      "     67       0.6517        \u001b[32m0.8645\u001b[0m       0.6473        0.8963        18.3236\n",
      "     68       0.6481        0.8728       0.6428        0.8838        18.2809\n",
      "     69       0.6481        0.8713       0.6450        0.8817        18.2438\n",
      "     70       \u001b[36m0.6585\u001b[0m        0.8645       0.6439        \u001b[31m0.8769\u001b[0m     +  18.6127\n",
      "     71       0.6543        \u001b[32m0.8588\u001b[0m       0.6495        \u001b[31m0.8747\u001b[0m     +  18.2120\n",
      "     72       0.6548        0.8622       0.6428        \u001b[31m0.8690\u001b[0m     +  18.2689\n",
      "     73       0.6487        0.8628       0.6327        0.9057        18.2142\n",
      "     74       0.6559        0.8610       0.6394        0.8928        18.3291\n",
      "     75       0.6579        \u001b[32m0.8556\u001b[0m       0.6383        0.8903        18.2749\n",
      "     76       0.6554        0.8561       0.6461        0.8832        18.2148\n",
      "     77       \u001b[36m0.6599\u001b[0m        \u001b[32m0.8465\u001b[0m       0.6405        0.9040        18.3451\n",
      "     78       \u001b[36m0.6610\u001b[0m        \u001b[32m0.8450\u001b[0m       0.6394        0.8751        18.2400\n",
      "     79       0.6593        0.8451       0.6484        0.8831        18.2101\n",
      "     80       \u001b[36m0.6615\u001b[0m        0.8462       0.6428        0.8693        18.2780\n",
      "     81       \u001b[36m0.6680\u001b[0m        \u001b[32m0.8339\u001b[0m       0.6372        0.8905        18.3261\n",
      "     82       \u001b[36m0.6688\u001b[0m        0.8362       0.6439        0.8776        18.2858\n",
      "     83       0.6599        0.8371       0.6439        \u001b[31m0.8663\u001b[0m     +  18.3960\n",
      "     84       0.6607        0.8393       0.6428        0.8680        18.4556\n",
      "     85       0.6587        0.8408       0.6484        0.8740        18.3222\n",
      "     86       0.6641        \u001b[32m0.8281\u001b[0m       0.6450        0.8891        18.3537\n",
      "     87       0.6641        0.8346       0.6394        0.8763        18.4229\n",
      "     88       0.6685        0.8313       0.6484        0.8715        18.3041\n",
      "     89       0.6663        0.8339       0.6484        \u001b[31m0.8572\u001b[0m     +  18.2736\n",
      "     90       \u001b[36m0.6697\u001b[0m        0.8345       0.6439        0.8661        18.8553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     91       \u001b[36m0.6719\u001b[0m        \u001b[32m0.8233\u001b[0m       \u001b[35m0.6540\u001b[0m        \u001b[31m0.8501\u001b[0m     +  18.3029\n",
      "     92       0.6716        0.8269       0.6484        0.8624        18.3355\n",
      "     93       \u001b[36m0.6727\u001b[0m        \u001b[32m0.8152\u001b[0m       0.6495        \u001b[31m0.8494\u001b[0m     +  18.3730\n",
      "     94       0.6719        0.8175       0.6484        0.8561        18.1030\n",
      "     95       \u001b[36m0.6781\u001b[0m        0.8161       0.6461        0.8898        18.1880\n",
      "     96       0.6688        0.8217       0.6372        0.8578        18.1495\n",
      "     97       \u001b[36m0.6809\u001b[0m        \u001b[32m0.8089\u001b[0m       0.6417        0.8684        18.2003\n",
      "     98       0.6697        0.8109       0.6428        0.8745        18.2144\n",
      "     99       0.6716        0.8192       0.6495        0.8604        18.1024\n",
      "    100       0.6761        \u001b[32m0.7956\u001b[0m       0.6417        0.8670        18.2522\n",
      "    101       0.6795        0.8002       0.6450        0.8695        18.1520\n",
      "    102       \u001b[36m0.6828\u001b[0m        \u001b[32m0.7869\u001b[0m       0.6495        \u001b[31m0.8444\u001b[0m     +  18.2817\n",
      "    103       0.6809        0.8005       0.6461        0.8523        18.3715\n",
      "    104       0.6814        0.7957       0.6529        0.8462        18.1974\n",
      "    105       0.6806        0.8048       0.6495        0.8645        18.2192\n",
      "    106       0.6792        \u001b[32m0.7865\u001b[0m       0.6529        0.8572        18.2703\n",
      "    107       0.6800        0.8022       \u001b[35m0.6618\u001b[0m        0.8447        18.1317\n",
      "    108       \u001b[36m0.6878\u001b[0m        \u001b[32m0.7846\u001b[0m       0.6529        0.8447        18.2051\n",
      "    109       0.6845        0.7872       0.6585        0.8483        18.1666\n",
      "    110       0.6842        \u001b[32m0.7808\u001b[0m       0.6506        0.8556        18.1907\n",
      "    111       \u001b[36m0.6895\u001b[0m        0.7854       0.6573        0.8640        18.1274\n",
      "    112       0.6878        0.7853       0.6573        0.8478        18.1680\n",
      "    113       \u001b[36m0.6968\u001b[0m        \u001b[32m0.7697\u001b[0m       0.6517        \u001b[31m0.8370\u001b[0m     +  18.2480\n",
      "    114       0.6948        0.7706       0.6562        0.8486        18.3243\n",
      "    115       0.6901        0.7837       0.6495        0.8495        18.2038\n",
      "    116       0.6895        \u001b[32m0.7697\u001b[0m       \u001b[35m0.6629\u001b[0m        0.8649        18.4433\n",
      "    117       0.6957        \u001b[32m0.7663\u001b[0m       0.6596        0.8429        18.2996\n",
      "    118       \u001b[36m0.7027\u001b[0m        \u001b[32m0.7617\u001b[0m       0.6484        0.9008        18.2584\n",
      "    119       0.6943        \u001b[32m0.7612\u001b[0m       \u001b[35m0.6674\u001b[0m        \u001b[31m0.8299\u001b[0m     +  18.3994\n",
      "    120       \u001b[36m0.7030\u001b[0m        0.7627       0.6641        0.8349        18.3339\n",
      "    121       0.6946        \u001b[32m0.7541\u001b[0m       0.6641        0.8433        18.3723\n",
      "    122       0.6940        0.7637       0.6641        0.8637        18.3573\n",
      "    123       0.6946        0.7651       0.6573        0.8353        18.3364\n",
      "    124       0.7007        \u001b[32m0.7514\u001b[0m       0.6585        0.8360        18.4011\n",
      "    125       0.6999        \u001b[32m0.7458\u001b[0m       0.6641        0.8447        18.3031\n",
      "    126       \u001b[36m0.7046\u001b[0m        \u001b[32m0.7381\u001b[0m       0.6674        0.8346        18.2803\n",
      "    127       \u001b[36m0.7105\u001b[0m        \u001b[32m0.7340\u001b[0m       \u001b[35m0.6730\u001b[0m        0.8342        18.2794\n",
      "    128       0.7086        \u001b[32m0.7262\u001b[0m       0.6562        0.8698        18.1620\n",
      "    129       0.7049        0.7407       0.6585        0.8550        18.3856\n",
      "    130       0.7072        0.7320       0.6562        0.8444        18.2737\n",
      "    131       0.7013        0.7426       0.6618        0.8520        18.3197\n",
      "    132       \u001b[36m0.7164\u001b[0m        \u001b[32m0.7221\u001b[0m       0.6697        0.8337        18.4995\n",
      "    133       0.7111        0.7340       0.6629        0.8339        18.3951\n",
      "    134       0.7142        \u001b[32m0.7166\u001b[0m       0.6641        0.8395        18.3019\n",
      "    135       \u001b[36m0.7178\u001b[0m        0.7208       0.6562        0.8576        18.4224\n",
      "    136       \u001b[36m0.7186\u001b[0m        \u001b[32m0.7143\u001b[0m       0.6641        0.8538        18.4386\n",
      "    137       \u001b[36m0.7214\u001b[0m        0.7238       0.6719        \u001b[31m0.8288\u001b[0m     +  18.3732\n",
      "    138       0.7114        0.7220       0.6585        0.8430        18.4389\n",
      "    139       \u001b[36m0.7242\u001b[0m        \u001b[32m0.7036\u001b[0m       \u001b[35m0.6741\u001b[0m        \u001b[31m0.8195\u001b[0m     +  18.4130\n",
      "    140       0.7186        \u001b[32m0.6997\u001b[0m       0.6708        0.8391        18.3304\n",
      "    141       0.7228        0.7079       0.6629        0.8523        18.2342\n",
      "    142       \u001b[36m0.7248\u001b[0m        0.7048       0.6685        0.8315        18.3345\n",
      "    143       \u001b[36m0.7256\u001b[0m        \u001b[32m0.6956\u001b[0m       0.6529        0.8458        18.3093\n",
      "    144       \u001b[36m0.7296\u001b[0m        \u001b[32m0.6922\u001b[0m       0.6618        0.8469        18.3718\n",
      "    145       0.7198        0.7009       0.6641        0.8430        18.4341\n",
      "    146       \u001b[36m0.7340\u001b[0m        \u001b[32m0.6858\u001b[0m       0.6585        \u001b[31m0.8165\u001b[0m     +  18.3223\n",
      "    147       \u001b[36m0.7368\u001b[0m        \u001b[32m0.6857\u001b[0m       0.6730        0.8359        18.3176\n",
      "    148       0.7352        \u001b[32m0.6752\u001b[0m       0.6529        0.8538        18.3534\n",
      "    149       0.7293        0.6904       \u001b[35m0.6797\u001b[0m        0.8287        18.2316\n",
      "    150       \u001b[36m0.7377\u001b[0m        \u001b[32m0.6660\u001b[0m       0.6652        0.8521        18.2436\n",
      "    151       0.7335        0.6756       0.6674        \u001b[31m0.8146\u001b[0m     +  18.3437\n",
      "    152       0.7287        0.6844       0.6596        0.8424        18.3080\n",
      "    153       0.7354        0.6805       0.6417        0.8519        18.2236\n",
      "    154       0.7360        0.6686       0.6674        0.8236        18.1975\n",
      "    155       0.7270        0.6838       0.6506        0.8468        18.3510\n",
      "    156       \u001b[36m0.7419\u001b[0m        0.6683       0.6786        0.8228        18.2179\n",
      "    157       \u001b[36m0.7424\u001b[0m        \u001b[32m0.6603\u001b[0m       0.6652        0.8337        18.2285\n",
      "    158       \u001b[36m0.7433\u001b[0m        \u001b[32m0.6498\u001b[0m       0.6753        0.8542        18.3037\n",
      "    159       0.7419        0.6604       0.6697        0.8491        18.2760\n",
      "    160       0.7427        0.6501       0.6753        \u001b[31m0.8109\u001b[0m     +  18.2663\n",
      "    161       \u001b[36m0.7520\u001b[0m        \u001b[32m0.6386\u001b[0m       0.6741        0.8674        18.3308\n",
      "    162       0.7436        0.6523       \u001b[35m0.6842\u001b[0m        0.8174        18.2390\n",
      "    163       0.7494        0.6485       0.6562        0.8231        18.3680\n",
      "    164       0.7508        \u001b[32m0.6347\u001b[0m       0.6730        0.8297        18.4418\n",
      "    165       0.7427        0.6474       0.6764        0.8441        18.4017\n",
      "    166       0.7508        \u001b[32m0.6272\u001b[0m       0.6820        0.8327        18.3302\n",
      "    167       0.7480        0.6474       0.6652        0.8330        18.3528\n",
      "    168       0.7514        0.6280       0.6573        0.8698        18.2633\n",
      "    169       \u001b[36m0.7542\u001b[0m        \u001b[32m0.6260\u001b[0m       \u001b[35m0.6865\u001b[0m        0.8544        18.2677\n",
      "    170       \u001b[36m0.7604\u001b[0m        \u001b[32m0.6217\u001b[0m       0.6652        0.8364        18.3212\n",
      "    171       \u001b[36m0.7657\u001b[0m        \u001b[32m0.6094\u001b[0m       0.6573        0.8479        18.3785\n",
      "    172       0.7632        \u001b[32m0.6077\u001b[0m       0.6786        0.8659        18.4136\n",
      "    173       0.7562        0.6247       0.6439        0.8624        18.4702\n",
      "    174       0.7520        0.6237       0.6797        0.8519        18.3889\n",
      "    175       0.7615        0.6144       0.6708        0.8371        18.3589\n",
      "    176       0.7542        0.6237       0.6562        0.8713        18.2134\n",
      "    177       0.7623        \u001b[32m0.6076\u001b[0m       0.6618        0.8365        18.2644\n",
      "    178       \u001b[36m0.7676\u001b[0m        \u001b[32m0.6020\u001b[0m       0.6719        0.8324        18.2733\n",
      "    179       0.7668        \u001b[32m0.5918\u001b[0m       0.6674        0.8413        18.3122\n",
      "    180       0.7620        0.6025       0.6484        0.8474        18.3415\n",
      "    181       0.7676        0.5949       0.6439        0.8604        18.4235\n",
      "    182       \u001b[36m0.7724\u001b[0m        \u001b[32m0.5764\u001b[0m       0.6775        0.8355        18.2415\n",
      "    183       0.7716        0.5806       0.6629        0.8399        18.4156\n",
      "    184       0.7693        0.5842       0.6629        0.8594        18.3454\n",
      "    185       0.7702        0.5768       0.6674        0.8541        18.2669\n",
      "    186       0.7671        0.5830       0.6719        0.8509        18.2783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    187       \u001b[36m0.7788\u001b[0m        \u001b[32m0.5664\u001b[0m       0.6473        0.8608        18.4243\n",
      "    188       0.7735        0.5835       0.6607        0.8648        18.1953\n",
      "    189       \u001b[36m0.7830\u001b[0m        \u001b[32m0.5494\u001b[0m       0.6484        0.8884        18.2986\n",
      "Stopping since valid_loss has not improved in the last 30 epochs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=AlexNetEgemaps1792(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): ReLU(inplace=True)\n",
       "      (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): ReLU(inplace=True)\n",
       "      (10): Conv2d(256, 256, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(5, 5))\n",
       "    (spec_fc): Sequential(\n",
       "      (0): Dropout(p=0.5, inplace=False)\n",
       "      (1): Linear(in_features=6400, out_features=1792, bias=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (egemaps_fc): Sequential(\n",
       "      (0): Linear(in_features=88, out_features=256, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (joint_fc): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (1): Dropout(p=0.75, inplace=False)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (classifier): Linear(in_features=512, out_features=4, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(train_ds, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
